{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f936debf-85b7-4b90-935c-6e4e73227b65",
   "metadata": {
    "id": "f936debf-85b7-4b90-935c-6e4e73227b65"
   },
   "source": [
    "# Rate-Distortion Guided Knowledge Graph Refinement with LLM-Assisted Operations\n",
    "\n",
    "## This notebook contains the functions that produce the following actions:\n",
    "- Read in lecture notes JSON into leaf elements\n",
    "- Build fused distance on the lecture side: chronology + logic + semantics\n",
    "- Build fused distance on the KG side: structure + semantics\n",
    "- Compute Fused Gromov–Wasserstein (FGW) coupling if POT is installed\n",
    "  (optional; exact). Otherwise, compute a proxy coupling via entropic OT\n",
    "  over a feature+structure cost surrogate.\n",
    "- Execute greedy refinement ops that minimize $L = r + \\beta * d$:\n",
    "  - add concepts from under-represented content\n",
    "  - split/merge concepts from coupling patterns\n",
    "  - add/rewire/remove relationships\n",
    "- Refine the KG by LLM based on domain knowledge\n",
    "- Output:\n",
    "  - Refined KG JSON (same schema as input)\n",
    "  - Refinement report JSON (iterations, objective, rates, distortions)\n",
    "  - Coupling analysis JSON (top-k matches)\n",
    "  - History of refinements\n",
    "  - Rate–Distortion curve PNG\n",
    "\n",
    "\n",
    "## Libraries:\n",
    "- Required: numpy, networkx, scipy (for distance + kmeans), scikit-learn\n",
    "- Optional: sentence-transformers (for embeddings; else falls back to TF-IDF)\n",
    "- Optional: POT (Python Optimal Transport, `pip install POT`) for exact FGW\n",
    "\n",
    "## Notes:\n",
    "- The script is deterministic up to random seeds for kmeans; you can set --seed.\n",
    "- If POT is present, exact FGW is used. If not, we compute an entropic OT coupling\n",
    "  with a proxy cost that mixes feature distance and a local-structure penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efd94d-f8d0-4f86-8d65-4e67e1235cc8",
   "metadata": {
    "id": "b4efd94d-f8d0-4f86-8d65-4e67e1235cc8"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QwaweHLIZfVm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14240,
     "status": "ok",
     "timestamp": 1758489187546,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "QwaweHLIZfVm",
    "outputId": "1d6c57b3-d286-4bca-8541-128edd8d30b1"
   },
   "outputs": [],
   "source": [
    "#!pip install -q POT\n",
    "#!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf2dc0a-5dc9-4f5d-9168-addcfdc94bcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 30306,
     "status": "ok",
     "timestamp": 1758492419310,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4cf2dc0a-5dc9-4f5d-9168-addcfdc94bcf"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import networkx as nx\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires 'networkx'. Please install it: pip install networkx\") from e\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.cluster import KMeans\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires scikit-learn. Please install it: pip install scikit-learn\") from e\n",
    "\n",
    "try:\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from scipy.special import rel_entr\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires scipy. Please install it: pip install scipy\") from e\n",
    "\n",
    "# Optional embeddings\n",
    "_SENTENCE_TF = None\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SENTENCE_TF = SentenceTransformer\n",
    "except Exception:\n",
    "    _SENTENCE_TF = None\n",
    "\n",
    "# Optional POT for exact FGW\n",
    "_HAS_POT = False\n",
    "try:\n",
    "    import ot\n",
    "    from ot.gromov import fused_gromov_wasserstein\n",
    "    _HAS_POT = True\n",
    "except Exception:\n",
    "    _HAS_POT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54fbc2db-968b-4d87-9b7d-f28a56cb509a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758492419324,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "54fbc2db-968b-4d87-9b7d-f28a56cb509a",
    "outputId": "17fcd82b-f715-4397-f556-d29cdab9a0d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentence_transformers.SentenceTransformer.SentenceTransformer, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_SENTENCE_TF, _HAS_POT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f194ce-726e-40db-a17b-72c3bdf8f217",
   "metadata": {
    "id": "01f194ce-726e-40db-a17b-72c3bdf8f217"
   },
   "source": [
    "## Utilitiy and Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f413d1-cdc3-40e2-aabc-7bd36951ea99",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492422426,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "24f413d1-cdc3-40e2-aabc-7bd36951ea99"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utility & Data Structures\n",
    "# -----------------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    alpha_chron: float = 0.2\n",
    "    alpha_logic: float = 0.3\n",
    "    alpha_sem: float = 0.5\n",
    "    gamma_struct: float = 0.4\n",
    "    gamma_sem: float = 0.6\n",
    "    lambda_feat: float = 0.6     # feature balance inside FGW objective\n",
    "    beta: float = 20.0           # rate–distortion trade-off\n",
    "    theta_add: float = 0.06\n",
    "    theta_split: float = 0.35\n",
    "    theta_merge: float = 0.12\n",
    "    theta_relate: float = 0.25\n",
    "    max_iterations: int = 10\n",
    "    convergence_threshold: float = 0.01\n",
    "    sinkhorn_eps: float = 0.05\n",
    "    sinkhorn_iter: int = 300\n",
    "\n",
    "@dataclass\n",
    "class LectureElement:\n",
    "    idx: int\n",
    "    id_path: str\n",
    "    section_path: List[str]\n",
    "    text: str\n",
    "    typ: str\n",
    "\n",
    "@dataclass\n",
    "class KGNode:\n",
    "    id: str\n",
    "    label: str\n",
    "    type: str\n",
    "    definition: str\n",
    "    aliases: List[str]\n",
    "    provenance: List[Dict[str, Any]]\n",
    "    attributes: Dict[str, Any]\n",
    "    confidence: float\n",
    "    rationale: str\n",
    "\n",
    "@dataclass\n",
    "class KGEdge:\n",
    "    id: str\n",
    "    source: str\n",
    "    target: str\n",
    "    relation: str\n",
    "    definition: str\n",
    "    provenance: List[Dict[str, Any]]\n",
    "    confidence: float\n",
    "    rationale: str\n",
    "\n",
    "@dataclass\n",
    "class RefinementOutcome:\n",
    "    iterations: int\n",
    "    final_objective: float\n",
    "    rate: float\n",
    "    distortion: float\n",
    "    fgw_distance: float\n",
    "    history: List[Dict[str, Any]]\n",
    "    operations: Dict[str, int]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f2ef2-759f-475c-8396-ad4d2168af0f",
   "metadata": {
    "id": "120f2ef2-759f-475c-8396-ad4d2168af0f"
   },
   "source": [
    "## I/O Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfee7a6-27b6-4c6d-b956-6ebc8da4e31c",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758492425723,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "6dfee7a6-27b6-4c6d-b956-6ebc8da4e31c"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# IO Helpers\n",
    "# -----------------------------\n",
    "\n",
    "def read_json(path: str) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(obj: Any, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031250b-5c93-4515-9b06-a4b7198dd493",
   "metadata": {},
   "source": [
    "## Read KG with Edge Text Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743c1398-5361-49fa-bc2b-95154ea91449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return definition text for an edge\n",
    "def get_edge_definition(kg, source_id, target_id, relation):\n",
    "    \n",
    "    slabel = source_id\n",
    "    tlabel = target_id\n",
    "    \n",
    "    for anode in kg['nodes']:\n",
    "        \n",
    "        if anode['id'].lower() == source_id.lower():\n",
    "            slabel = anode['label']\n",
    "                \n",
    "        if anode['id'].lower() == target_id.lower():\n",
    "            tlabel = anode['label']\n",
    "\n",
    "        \n",
    "    return slabel + \" \" + relation + \" \" + tlabel\n",
    "\n",
    "# add relations where coupled lecture elements are near\n",
    "def get_edge_rationale(kg, source_id, target_id, relation):\n",
    "\n",
    "    slabel = source_id\n",
    "    tlabel = target_id\n",
    "    \n",
    "    for anode in kg['nodes']:\n",
    "        \n",
    "        if anode['id'].lower() == source_id.lower():\n",
    "            slabel = anode['label']\n",
    "                \n",
    "        if anode['id'].lower() == target_id.lower():\n",
    "            tlabel = anode['label']\n",
    "\n",
    "    retionale = f'{slabel} {relation} {tlabel} because their coupled lecture elements are near.'\n",
    "    \n",
    "    return rationale\n",
    "    \n",
    "# read KG with definition edges\n",
    "def read_kg(path: str) -> Any:\n",
    "    kg_json = read_json(path)\n",
    "\n",
    "    for edge in kg_json['edges']:\n",
    "        if ('definition' not in edge) or (not edge['definition']):\n",
    "            sid = edge['source']\n",
    "            tid = edge['target']\n",
    "            e_def = get_edge_definition(kg_json, sid, tid, edge['relation'])\n",
    "\n",
    "            edge['definition'] = e_def\n",
    "\n",
    "    return kg_json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26e179-4f40-47c3-97a5-4d1eab60bdcc",
   "metadata": {
    "id": "2b26e179-4f40-47c3-97a5-4d1eab60bdcc"
   },
   "source": [
    "## Lecture JSON Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dfdbb1-9fe6-40c2-90e8-3fa0274aa5cd",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1758492427361,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "42dfdbb1-9fe6-40c2-90e8-3fa0274aa5cd"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Lecture JSON Parsing\n",
    "# -----------------------------\n",
    "\n",
    "def _flatten_lecture_json(node: Dict[str, Any],\n",
    "                          section_stack: Optional[List[str]] = None,\n",
    "                          out: Optional[List[LectureElement]] = None,\n",
    "                          idx_offset: int = 0) -> List[LectureElement]:\n",
    "    \"\"\"\n",
    "    Generic flattener for lecture JSONs with fields:\n",
    "      - id, level, type, title, content, children (list)\n",
    "    or where leaf 'elements' already exist (preferred).\n",
    "    \"\"\"\n",
    "    if out is None:\n",
    "        out = []\n",
    "    if section_stack is None:\n",
    "        section_stack = []\n",
    "\n",
    "    title = node.get(\"title\") or node.get(\"label\") or node.get(\"name\") or \"\"\n",
    "    typ = node.get(\"type\") or \"section\"\n",
    "    id_str = str(node.get(\"id\", \"\"))\n",
    "    content = node.get(\"content\", \"\")\n",
    "\n",
    "    # If explicit elements exist (pre-cleaned file), use them\n",
    "    elements = node.get(\"elements\") or []\n",
    "    if elements and isinstance(elements, list):\n",
    "        for e in elements:\n",
    "            text = e.get(\"text\", \"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            etype = e.get(\"type\", \"text\")\n",
    "            out.append(LectureElement(\n",
    "                idx=idx_offset + len(out),\n",
    "                id_path=id_str,\n",
    "                section_path=section_stack + [title] if title else section_stack[:],\n",
    "                text=text,\n",
    "                typ=etype\n",
    "            ))\n",
    "    else:\n",
    "        # Fallback: split content into crude sentence-ish lines\n",
    "        #\n",
    "        # This part repeats \"Break Down to Individual Elements\"\n",
    "        # in \"RD_FGW_source_markdown_json.ipynb\".\n",
    "        # Assume HERE that :code and :markdwon suffixes have been removed\n",
    "        #\n",
    "        lines = [ln.strip() for ln in str(content).splitlines() if ln.strip()]\n",
    "        for ln in lines:\n",
    "            out.append(LectureElement(\n",
    "                idx=idx_offset + len(out),\n",
    "                id_path=id_str,\n",
    "                section_path=section_stack + [title] if title else section_stack[:],\n",
    "                text=ln,\n",
    "                typ=\"text\"\n",
    "            ))\n",
    "\n",
    "    # Recurse into children\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        _flatten_lecture_json(ch, section_stack + ([title] if title else []),\n",
    "                              out, idx_offset)\n",
    "\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from copy import copy\n",
    "\n",
    "def clean_up_elements(elements):\n",
    "    out = []\n",
    "    for e in elements:\n",
    "        etext = (e.get('text') if isinstance(e, dict) else getattr(e, 'text', '')) or ''\n",
    "        if re.fullmatch(r\"\\s*(?:`{3,}|~{3,}|_{3,}|-{3,})\\s*\", etext):\n",
    "            continue\n",
    "        t = re.sub(r\"\\b(?:\\:code|\\:markdown|markdown|exercise|example|agenda|summary)\\b\", \"\", etext, flags=re.I)\n",
    "        t = re.sub(r\"^\\s*[#>*+\\-`~\\u2013\\u2014\\_]*\\s*(?:\\d+[.)]\\s*)?\", \"\", t)\n",
    "        t = re.sub(r\"(?:`{3,}|~{3,}|_{3,}|-{3,})\", \" \", t)\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
    "        if t and not re.fullmatch(r\"[\\W_]+\", t):\n",
    "            if isinstance(e, dict):\n",
    "                ne = dict(e); ne['text'] = t\n",
    "            else:\n",
    "                ne = copy(e); setattr(ne, 'text', t)\n",
    "            out.append(ne)\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_lecture_elements(lecture_json_path: str) -> List[LectureElement]:\n",
    "    data = read_json(lecture_json_path)\n",
    "    # If already a list of sections with elements, flatten all\n",
    "    if isinstance(data, dict) and data.get(\"id\") is not None:\n",
    "        elements = _flatten_lecture_json(data)\n",
    "    elif isinstance(data, list):\n",
    "        elements = []\n",
    "        for x in data:\n",
    "            elements.extend(_flatten_lecture_json(x))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported lecture JSON format.\")\n",
    "\n",
    "    # Attach sequential chronological index\n",
    "    for i, el in enumerate(elements):\n",
    "        el.idx = i\n",
    "\n",
    "    # clean up the elements' text\n",
    "    cleaned_elements = clean_up_elements(elements)\n",
    "\n",
    "    return cleaned_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dad462-7e68-4a5f-a76b-639512705884",
   "metadata": {
    "id": "45dad462-7e68-4a5f-a76b-639512705884"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeeeaad1-db06-4e0c-bde6-af35e6d6e67e",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492433660,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "eeeeaad1-db06-4e0c-bde6-af35e6d6e67e"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", use_sentence_tf: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.use_sentence_tf = use_sentence_tf and (_SENTENCE_TF is not None)\n",
    "        self.model = None\n",
    "        self.tfidf = None\n",
    "\n",
    "        if self.use_sentence_tf:\n",
    "            try:\n",
    "                self.model = _SENTENCE_TF(self.model_name)\n",
    "            except Exception as e:\n",
    "                logging.warning(\"SentenceTransformer init failed; falling back to TF-IDF: %s\", e)\n",
    "                self.use_sentence_tf = False\n",
    "\n",
    "        if not self.use_sentence_tf:\n",
    "            self.tfidf = TfidfVectorizer(max_features=4096)\n",
    "\n",
    "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.use_sentence_tf:\n",
    "            print(\"Use SentenceTransformer.\")\n",
    "            return np.asarray(self.model.encode(texts, normalize_embeddings=True))\n",
    "        else:\n",
    "            print(\"Use TFIDF to embed -> Check.\")\n",
    "            X = self.tfidf.fit_transform(texts)\n",
    "            # Normalize L2\n",
    "            X = X.astype(np.float32)\n",
    "            norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12\n",
    "            X = X.multiply(1/norms[:, None])\n",
    "            return X.toarray()\n",
    "\n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.use_sentence_tf:\n",
    "            print(\"Use SentenceTransformer.\")\n",
    "            return np.asarray(self.model.encode(texts, normalize_embeddings=True))\n",
    "        else:\n",
    "            print(\"Use TFIDF to embed -> check.\")\n",
    "            X = self.tfidf.transform(texts)\n",
    "            # Normalize L2\n",
    "            X = X.astype(np.float32)\n",
    "            norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12\n",
    "            X = X.multiply(1/norms[:, None])\n",
    "            return X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d77d52-82e8-447b-aa8f-5c5109a16948",
   "metadata": {
    "id": "b8d77d52-82e8-447b-aa8f-5c5109a16948"
   },
   "source": [
    "## Building Distance Matrix for Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7791ae2-cbde-4dd0-ac84-ec4b59b9793a",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758492436381,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "e7791ae2-cbde-4dd0-ac84-ec4b59b9793a"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Distance Utilities for Lecture Notes\n",
    "# --------------------------------------\n",
    "\n",
    "def lcp_length(a: List[str], b: List[str]) -> int:\n",
    "    n = min(len(a), len(b))\n",
    "    i = 0\n",
    "    while i < n and a[i] == b[i]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def build_lecture_distance(elements: List[LectureElement],\n",
    "                           embeddings: np.ndarray,\n",
    "                           alpha_chron: float,\n",
    "                           alpha_logic: float,\n",
    "                           alpha_sem: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      D_L: fused NxN distance\n",
    "      D_chron, D_logic, D_sem: component distances\n",
    "    \"\"\"\n",
    "    N = len(elements)\n",
    "    idxs = np.array([el.idx for el in elements], dtype=float)\n",
    "    max_idx = max(1, int(np.max(idxs)))\n",
    "    # Chronological: normalized absolute index difference\n",
    "    D_chron = np.abs(idxs[:, None] - idxs[None, :]) / max(1.0, float(max_idx))\n",
    "\n",
    "    # Logic: 1 - normalized LCP length\n",
    "    # Precompute section paths\n",
    "    paths = [el.section_path for el in elements]\n",
    "    max_depth = max((len(p) for p in paths), default=1)\n",
    "    D_logic = np.zeros((N, N), dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            l = lcp_length(paths[i], paths[j])\n",
    "            d = 1.0 - (l / max(1.0, float(max_depth)))\n",
    "            D_logic[i, j] = D_logic[j, i] = d\n",
    "\n",
    "    # Semantic: 1 - cosine similarity (clipped)\n",
    "    S = cosine_similarity(embeddings)\n",
    "    D_sem = np.clip(1.0 - S, 0.0, 2.0)\n",
    "\n",
    "    D_L = alpha_chron * D_chron + alpha_logic * D_logic + alpha_sem * D_sem\n",
    "    # Normalize to [0,1]\n",
    "    D_L = (D_L - D_L.min()) / (D_L.max() - D_L.min() + 1e-12)\n",
    "    return D_L, D_chron, D_logic, D_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc738d4-bef8-4d4b-acd2-599b6ec2e752",
   "metadata": {
    "id": "ecc738d4-bef8-4d4b-acd2-599b6ec2e752"
   },
   "source": [
    "## Building Distance Matrix for Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e9352e-3bec-41a7-813e-0562ff5302e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1758492447641,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "78e9352e-3bec-41a7-813e-0562ff5302e4"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Distance Utilities for Knowledge Graph\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "def build_kg_graph(kg: Dict[str, Any]) -> nx.Graph:\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for n in kg.get(\"nodes\", []):\n",
    "        G.add_node(n[\"id\"], **n)\n",
    "    for e in kg.get(\"edges\", []):\n",
    "        # Treat as undirected for structure metrics; keep relation on edge data\n",
    "        G.add_edge(e[\"source\"], e[\"target\"], **e)\n",
    "    return G\n",
    "\n",
    "def node_text_for_embedding(node: Dict[str, Any]) -> str:\n",
    "    parts = [node.get(\"label\", \"\"), node.get(\"definition\", \"\")]\n",
    "    aliases = node.get(\"aliases\") or []\n",
    "    if isinstance(aliases, list):\n",
    "        parts.extend(aliases[:3])\n",
    "    return \". \".join([p for p in parts if p]).strip()\n",
    "\n",
    "def build_kg_distance(kg: Dict[str, Any],\n",
    "                      embedder: Embedder,\n",
    "                      gamma_struct: float,\n",
    "                      gamma_sem: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    nodes = kg.get(\"nodes\", [])\n",
    "    node_ids = [n[\"id\"] for n in nodes]\n",
    "    texts = [node_text_for_embedding(n) or n.get(\"label\", n[\"id\"]) for n in nodes]\n",
    "    X = embedder.transform(texts) if getattr(embedder, \"tfidf\", None) else embedder.fit_transform(texts)\n",
    "\n",
    "    # Semantic distance\n",
    "    S = cosine_similarity(X)\n",
    "    D_sem = np.clip(1.0 - S, 0.0, 2.0)\n",
    "\n",
    "    # Structural distance via shortest paths\n",
    "    G = build_kg_graph(kg)\n",
    "    # Precompute APSP lengths (unweighted)\n",
    "    # For disconnected pairs, set to large value\n",
    "    n = len(node_ids)\n",
    "    D_struct = np.full((n, n), fill_value=np.inf, dtype=float)\n",
    "    sp = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    for i, u in enumerate(node_ids):\n",
    "        D_struct[i, i] = 0.0\n",
    "        for j, v in enumerate(node_ids):\n",
    "            if v in sp.get(u, {}):\n",
    "                D_struct[i, j] = float(sp[u][v])\n",
    "    # Replace inf with max finite + 1\n",
    "    finite = D_struct[np.isfinite(D_struct)]\n",
    "    max_f = float(finite.max()) if finite.size else 1.0\n",
    "    D_struct[~np.isfinite(D_struct)] = max_f + 1.0\n",
    "\n",
    "    # Normalize each component to [0,1]\n",
    "    def _norm(M):\n",
    "        return (M - M.min()) / (M.max() - M.min() + 1e-12)\n",
    "    D_struct = _norm(D_struct)\n",
    "    D_sem = _norm(D_sem)\n",
    "\n",
    "    D_K = gamma_struct * D_struct + gamma_sem * D_sem\n",
    "    D_K = _norm(D_K)\n",
    "    return D_K, D_struct, D_sem, X, node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfe2bc-3dbd-4ed9-b20f-fbf58a14a3d3",
   "metadata": {
    "id": "ecbfe2bc-3dbd-4ed9-b20f-fbf58a14a3d3"
   },
   "source": [
    "## Normalized Measure and Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df13c92-119a-4882-9cc0-3a7d98bd5cb8",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492449943,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0df13c92-119a-4882-9cc0-3a7d98bd5cb8"
   },
   "outputs": [],
   "source": [
    "def normalized_measure(n: int) -> np.ndarray:\n",
    "    return np.full((n,), 1.0 / max(1, n), dtype=float)\n",
    "\n",
    "def degree_centrality_measure(kg: Dict[str, Any], node_ids: List[str]) -> np.ndarray:\n",
    "    G = build_kg_graph(kg)\n",
    "    deg = np.array([G.degree(nid) for nid in node_ids], dtype=float)\n",
    "    if deg.sum() <= 0:\n",
    "        return normalized_measure(len(node_ids))\n",
    "    return deg / deg.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367fbd-5bed-4056-852c-563cd33dc45f",
   "metadata": {
    "id": "fc367fbd-5bed-4056-852c-563cd33dc45f"
   },
   "source": [
    "## Compute FGW Distance and Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4970a77e-98e4-4f98-9f40-213826187988",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1758492451848,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4970a77e-98e4-4f98-9f40-213826187988"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# FGW / Proxy Coupling\n",
    "# -----------------------------\n",
    "\n",
    "def compute_feature_cost(E_L: np.ndarray, E_K: np.ndarray) -> np.ndarray:\n",
    "    # Squared Euclidean between embeddings; both are L2-normalized, so this ~ 2 - 2*cosine\n",
    "    C = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "    # Normalize to [0,1]\n",
    "    C = (C - C.min()) / (C.max() - C.min() + 1e-12)\n",
    "    return C\n",
    "\n",
    "def local_structure_fingerprint(D: np.ndarray, k: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each row i in a distance matrix D (N x N), return a vector of the\n",
    "    sorted distances to its k nearest neighbors (excluding self).\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    fp = np.zeros((N, k), dtype=float)\n",
    "    for i in range(N):\n",
    "        row = D[i].copy()\n",
    "        row[i] = np.inf\n",
    "        idx = np.argsort(row)[:k]\n",
    "        fp[i] = np.sort(row[idx])\n",
    "    # Normalize per-column\n",
    "    fp = (fp - fp.min(axis=0, keepdims=True)) / (fp.max(axis=0, keepdims=True) - fp.min(axis=0, keepdims=True) + 1e-12)\n",
    "    return fp\n",
    "\n",
    "def compute_proxy_cost(D_L: np.ndarray, D_K: np.ndarray,\n",
    "                       E_L: np.ndarray, E_K: np.ndarray,\n",
    "                       lam_feat: float = 0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a surrogate cost for OT that mixes:\n",
    "      - feature cost between embeddings\n",
    "      - local structure fingerprints between spaces\n",
    "    \"\"\"\n",
    "    C_feat = compute_feature_cost(E_L, E_K)\n",
    "\n",
    "    # ensure the k is no greater than the mininum number of elements in\n",
    "    # either lecture notes or the kg\n",
    "    min_k = min(D_L.shape[0]-1, D_K.shape[0]-1)\n",
    "\n",
    "    fp_L = local_structure_fingerprint(D_L, k=min(min_k, max(1, D_L.shape[0]-1)))\n",
    "    fp_K = local_structure_fingerprint(D_K, k=min(min_k, max(1, D_K.shape[0]-1)))\n",
    "    #print(\"fp_L shape: {}\".format(fp_L.shape))\n",
    "    #print(\"fp_K shape: {}\".format(fp_K.shape))\n",
    "\n",
    "    # Structure penalty between local fingerprints\n",
    "    C_struct = cdist(fp_L, fp_K, metric=\"sqeuclidean\")\n",
    "    C_struct = (C_struct - C_struct.min()) / (C_struct.max() - C_struct.min() + 1e-12)\n",
    "    C = lam_feat * C_feat + (1.0 - lam_feat) * C_struct\n",
    "    C = (C - C.min()) / (C.max() - C.min() + 1e-12)\n",
    "    return C\n",
    "\n",
    "def sinkhorn_ot(mu: np.ndarray, nu: np.ndarray, C: np.ndarray, eps: float = 0.05, n_iter: int = 300) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple entropic OT via Sinkhorn-Knopp on kernel K = exp(-C/eps).\n",
    "    Returns coupling matrix P (mu x nu).\n",
    "    \"\"\"\n",
    "    K = np.exp(-C / max(1e-8, eps))\n",
    "    u = np.ones_like(mu)\n",
    "    v = np.ones_like(nu)\n",
    "    for _ in range(n_iter):\n",
    "        u = mu / (K @ v + 1e-12)\n",
    "        v = nu / (K.T @ u + 1e-12)\n",
    "    P = np.diag(u) @ K @ np.diag(v)\n",
    "    return P\n",
    "\n",
    "def fgw_distance_proxy(P: np.ndarray,\n",
    "                       D_L: np.ndarray, D_K: np.ndarray,\n",
    "                       E_L: np.ndarray, E_K: np.ndarray,\n",
    "                       lam_feat: float = 0.6) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute an FGW-like loss using current coupling P:\n",
    "      d = sum |D_L[i,k] - D_K[j,l]|^2 * P[i,j]*P[k,l] + lam_feat * sum ||E_L[i]-E_K[j]||^2 * P[i,j]\n",
    "    Returns:\n",
    "      total_loss, structure_term, feature_term\n",
    "    \"\"\"\n",
    "    # Structure term\n",
    "    DL = D_L\n",
    "    DK = D_K\n",
    "    # Efficient einsum formulation\n",
    "    # A[i,k] = D_L[i,k]^2\n",
    "    # B[j,l] = D_K[j,l]^2\n",
    "    # structure term = sum_{i,k,j,l} (D_L[i,k] - D_K[j,l])^2 * P[i,j] * P[k,l]\n",
    "    # = sum A[i,k]*P[i,:].sum_j P[k,:].sum_l + sum B[j,l]*P[:,j].sum_i P[:,l].sum_k - 2 * sum D_L[i,k] D_K[j,l] P[i,j] P[k,l]\n",
    "    A = DL ** 2\n",
    "    B = DK ** 2\n",
    "    Pi_row = P.sum(axis=1)  # size N\n",
    "    Pi_col = P.sum(axis=0)  # size M\n",
    "\n",
    "    term1 = (A * np.outer(Pi_row, Pi_row)).sum()\n",
    "    term2 = (B * np.outer(Pi_col, Pi_col)).sum()\n",
    "    # Cross term\n",
    "    term3 = 0.0\n",
    "    # Compute M1 = D_L @ P @ D_K.T @ P.T ??\n",
    "    # We need sum_{i,k,j,l} D_L[i,k] * D_K[j,l] * P[i,j] * P[k,l]\n",
    "    # This equals trace(D_L^T (P D_K P^T))\n",
    "    # Compute M = P @ D_K @ P.T  -> size N x N\n",
    "    M = P @ DK @ P.T\n",
    "    term3 = np.sum(DL * M)\n",
    "    structure = term1 + term2 - 2.0 * term3\n",
    "\n",
    "    # Feature term\n",
    "    # ||E_L[i] - E_K[j]||^2 = a + b - 2 <...>, but we can compute directly\n",
    "    C_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "    feature = float((C_feat * P).sum())\n",
    "    total = structure + lam_feat * feature\n",
    "    return total, structure, feature\n",
    "\n",
    "def compute_coupling_and_distance(D_L: np.ndarray, D_K: np.ndarray,\n",
    "                                  E_L: np.ndarray, E_K: np.ndarray,\n",
    "                                  mu: np.ndarray, nu: np.ndarray,\n",
    "                                  lam_feat: float, sink_eps: float, sink_iter: int,\n",
    "                                  use_pot: bool) -> Tuple[np.ndarray, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      P (coupling), total_fgw_like, structure_term, feature_term\n",
    "    \"\"\"\n",
    "    if use_pot and _HAS_POT:\n",
    "        print(\"Use POT.\")\n",
    "        # Exact FGW using POT\n",
    "        # Feature matrices are E_L and E_K; costs are cosine or Euclidean\n",
    "        # POT expects feature cost matrices; we give squared Euclidean\n",
    "        C1 = D_L\n",
    "        C2 = D_K\n",
    "        M_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "        # FGW coupling\n",
    "        #P = fused_gromov_wasserstein(M_feat, C1, C2, mu, nu, alpha=lam_feat,\n",
    "        # verbose=False)\n",
    "        # FGW loss\n",
    "        #total = float(fgw_loss(P, M_feat, C1, C2, lam_feat))\n",
    "\n",
    "        P, log = fused_gromov_wasserstein(M_feat, C1, C2, mu, nu,\n",
    "                                              loss_fun=\"square_loss\",\n",
    "                                               alpha=lam_feat, log=True)\n",
    "        total = float(log.get(\"fgw_dist\", np.nan))\n",
    "        # Decompose approximately (for reporting)\n",
    "        # We recompute proxy structure/feature with same P for interpretability\n",
    "        total_proxy, structure, feature = fgw_distance_proxy(P, D_L, D_K, E_L, E_K, lam_feat)\n",
    "        return P, total, structure, feature\n",
    "    else:\n",
    "        print(\"Use Proxy OT.\")\n",
    "        # Proxy: build surrogate cost and do entropic OT\n",
    "        C = compute_proxy_cost(D_L, D_K, E_L, E_K, lam_feat)\n",
    "        P = sinkhorn_ot(mu, nu, C, eps=sink_eps, n_iter=sink_iter)\n",
    "        total, structure, feature = fgw_distance_proxy(P, D_L, D_K, E_L, E_K, lam_feat)\n",
    "        return P, total, structure, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2128eb-6661-49fe-9f4d-7ea1f73b70ac",
   "metadata": {
    "id": "ce2128eb-6661-49fe-9f4d-7ea1f73b70ac"
   },
   "source": [
    "## LLM Assisted Concept and Relationship Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e556005f-dc0a-4858-a462-89a64169a2a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1758492032686,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "e556005f-dc0a-4858-a462-89a64169a2a2",
    "outputId": "b4682cca-cad8-4970-df6c-8b805e172b35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd7b4-d864-4e6a-9731-ac3e96b7bfaf",
   "metadata": {
    "id": "283fd7b4-d864-4e6a-9731-ac3e96b7bfaf"
   },
   "source": [
    "### LLM Helper OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226e5ed0-9848-4fab-931a-449b698cc4bb",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492462199,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "226e5ed0-9848-4fab-931a-449b698cc4bb"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LLM helper (optional)\n",
    "# ---------------------------\n",
    "def llm_call(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 700, temperature: float = 0.2) -> str:\n",
    "    \"\"\"Call OpenAI chat completion. If no key or pkg, raises RuntimeError.\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    #api_key = userdata.get('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "    try:\n",
    "        import openai  # type: ignore\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAI client not available: {e}\")\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise assistant for ontology construction.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JUBuQPMJ5Fpw",
   "metadata": {
    "id": "JUBuQPMJ5Fpw"
   },
   "source": [
    "## LLM Helper Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ac71ad-0998-44d9-baae-3f020ade8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "#api_key = userdata.get('GOOGLE_API_KEY') # Assuming this is how you access secrets\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GOOGLE_API_KEY not set\")\n",
    "\n",
    "try:\n",
    "    model: str = \"gemini-2.5-flash-lite\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    # For chat-based models, you'd typically use genai.GenerativeModel\n",
    "    # and start a chat session. For simple prompt-response, direct generate_content works.\n",
    "    gemini_model = genai.GenerativeModel(model)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gemini client not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3XirqOqj5J6E",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758491891208,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "3XirqOqj5J6E"
   },
   "outputs": [],
   "source": [
    "def llm_call_gemini(prompt: str, model_instance=gemini_model,\n",
    "    max_output_tokens=1000, temperature=0.2) -> str:\n",
    "    \"\"\"Call Google Gemini chat completion. If no key or pkg, raises RuntimeError.\"\"\"\n",
    "    try:\n",
    "        # Gemini's generate_content takes a prompt directly\n",
    "        resp = model_instance.generate_content(\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [\n",
    "                    \"You are a precise assistant for ontology construction.\",\n",
    "                    prompt\n",
    "                ]}\n",
    "            ]\n",
    "        )\n",
    "        # Accessing the text content from the response\n",
    "        return resp.text.strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493400b7-2e3c-4d39-8345-7c5a93fd38bf",
   "metadata": {
    "id": "493400b7-2e3c-4d39-8345-7c5a93fd38bf"
   },
   "source": [
    "### LLM Concept Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b36ebbf-c8ae-49d5-b7ca-2a1aa2021568",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758492466881,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9b36ebbf-c8ae-49d5-b7ca-2a1aa2021568"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LLM concept naming\n",
    "# ---------------------------\n",
    "def make_concept_label_from_text_LLM(text: str) -> str:\n",
    "\n",
    "    print(\"Use LLM to name new concept.\")\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "        Generate a single, meaningful concept that represents\n",
    "        the main idea of the TEXT.\n",
    "        The concept should be specific\n",
    "        and represents the main phrase and terms in the TEXT.\\n\n",
    "        return ONLY the concept name. \\n\n",
    "        If the TEXT provided is empty and does not contain any content to\n",
    "        derive a concept from, return ''.\\n\n",
    "        TEXT: {}.\\n\\n\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    #name = llm_call(prompt.format(text)).strip()\n",
    "\n",
    "    name = llm_call_gemini(prompt.format(text)).strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "def make_edges_between_text_LLM(kg: Dict[str, Any], new_node,\n",
    "                               allowed_relations: List[str]):\n",
    "\n",
    "    print(\"Use LLM to add new edges for a new concept.\")\n",
    "\n",
    "    \"\"\"\n",
    "    Uses LLM to find and add the most likely relation(s) between a new node\n",
    "    and existing nodes in the KG, based on concept proximity and allowed relations.\n",
    "    This version attempts to make a single LLM call for efficiency.\n",
    "    \"\"\"\n",
    "    new_node_info = f\"New Concept: \\\"{new_node['label']}\\\" (Definition: \\\"{new_node['definition']}\\\")\"\n",
    "    existing_nodes_info = \"\\nExisting Concepts:\\n\" + \"\\n\".join([\n",
    "        f\"- \\\"{n['label']}\\\" (Definition: \\\"{n.get('definition', '')}\\\") [ID: {n['id']}]\"\n",
    "        for n in kg[\"nodes\"] if n[\"id\"] != new_node['id']\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Given the new node and existing nodes below, identify the most likely relationship(s)\n",
    "    from the NEW NODE to each EXISTING NODE from the following list of allowed relations:\n",
    "    {', '.join(allowed_relations)}.\n",
    "    For each relevant relationship, output the original EXISTING NODE's ID, the relation name, and a\n",
    "    probability score between 0 and 1, and rationale about adding this edge,\n",
    "    separated by colons, one relationship per line.\n",
    "    Example output format:\n",
    "    [EXISTING_NODE_ID]: [RELATION_NAME]: [PROBABILITY]: [RATIONALE]\n",
    "    [EXISTING_NODE_ID]: [RELATION_NAME]: [PROBABILITY]: [RATIONALE]\n",
    "    ...\n",
    "\n",
    "    If no suitable relation is found for an existing concept with a high probability,\n",
    "    do not include it in the output.\n",
    "\n",
    "    {new_node_info}\n",
    "    {existing_nodes_info}\n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "\n",
    "    candidate_edges = []\n",
    "    try:\n",
    "        #llm_response = llm_call(prompt, max_tokens=5000).strip() # Increase max_tokens\n",
    "\n",
    "        llm_response = llm_call_gemini(prompt, max_output_tokens=5000).strip() # Increase max_tokens\n",
    "        # for potentially longer output Parse the LLM response\n",
    "        for line in llm_response.splitlines():\n",
    "            parts = line.split(':')\n",
    "            if len(parts) == 4:\n",
    "                target_id = parts[0].strip()\n",
    "                llm_relation = parts[1].strip()\n",
    "                \n",
    "                try:\n",
    "                    llm_confidence = float(parts[2].strip())\n",
    "                except ValueError:\n",
    "                    llm_confidence = 0.5 # Default confidence if parsing fails\n",
    "                \n",
    "                llm_rationale = parts[3].strip()\n",
    "                \n",
    "                # Validate target_id and relation\n",
    "                if any(n[\"id\"].lower() == target_id.lower() for n in kg[\"nodes\"]) and llm_relation in allowed_relations:\n",
    "                    \n",
    "                    # get edge defintion\n",
    "                    e_def = get_edge_definition(kg, new_node['id'], target_id, llm_relation)\n",
    "                    \n",
    "                    candidate_edges.append({\n",
    "                        \"source\": new_node['id'],\n",
    "                        \"target\": target_id,\n",
    "                        \"relation\": llm_relation,\n",
    "                        \"definition\": e_def,\n",
    "                        \"confidence\": llm_confidence,\n",
    "                        \"rationale\": llm_rationale\n",
    "                    })\n",
    "                else:\n",
    "                     print(f\"Warning: Invalid target_id or relation from LLM: {line}.\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"LLM call failed for relation prediction: {e}\")\n",
    "        # If LLM fails, add a default 'relatedTo' edge to all existing nodes with low confidence\n",
    "        # This is a fallback to ensure some connectivity, adjust confidence as needed.\n",
    "        # for existing_node in kg[\"nodes\"]:\n",
    "        #     if existing_node[\"id\"] != new_node['id']:\n",
    "        #         candidate_edges.append({\n",
    "        #             \"source\": new_node['id'],\n",
    "        #             \"target\": existing_node[\"id\"],\n",
    "        #             \"relation\": \"relatedTo\",\n",
    "        #             \"confidence\": 0.2\n",
    "        #         })\n",
    "\n",
    "\n",
    "    return candidate_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047dfca-b76c-4234-bdf6-50c9766ee24d",
   "metadata": {},
   "source": [
    "## Task-Oriented All-Allowed Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f1830b-975b-4b70-81e3-27f69627fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ALLOWED_RELATIONS_DEFAULT = [\n",
    "    \"isA\",\"partOf\",\"prerequisiteOf\",\"dependsOn\",\"relatedTo\",\"synonymOf\",\"antonymOf\",\n",
    "    \"contrastsWith\",\"defines\",\"uses\",\"usedBy\",\"appliesTo\",\"exampleOf\",\"counterexampleOf\",\n",
    "    \"illustratedBy\",\"causes\",\"resultsIn\",\"prevents\",\"assumes\",\"implies\",\"equivalentTo\",\n",
    "    \"parameterOf\",\"hasParameter\",\"propertyOf\",\"hasProperty\",\"measuredBy\",\"unitOf\",\n",
    "    \"representedBy\",\"notationFor\",\"formulaFor\",\"provedBy\",\"theoremOf\",\"algorithmFor\",\n",
    "    \"stepOf\",\"produces\",\"consumes\",\"advantageOf\",\"limitationOf\",\"commonErrorIn\",\n",
    "    \"misconceptionOf\",\"commonlyConfusedWith\",\"assessedBy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a070a9-cded-45b7-bebf-3197de89e956",
   "metadata": {
    "id": "f7a070a9-cded-45b7-bebf-3197de89e956"
   },
   "source": [
    "## TF-IDF Top Terms Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64891394-008f-4012-acf2-8c573044a1c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1758492480176,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0d6d7151-8c33-4b08-8b66-92ed58a5173d"
   },
   "outputs": [],
   "source": [
    "def tfidf_top_terms(texts: List[str], topk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return the top-k terms by mean TF-IDF across documents.\n",
    "    Robust to empty/stopword-only inputs and None/NaN values.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Coerce to strings and strip\n",
    "    cleaned = [(t if isinstance(t, str) else \"\").strip() for t in texts]\n",
    "    if all(not t for t in cleaned):\n",
    "        return []\n",
    "\n",
    "    # First attempt: standard English stopwords\n",
    "    vec = TfidfVectorizer(max_features=4096, stop_words=\"english\")\n",
    "    try:\n",
    "        X = vec.fit_transform(cleaned)\n",
    "    except ValueError:\n",
    "\n",
    "        return []\n",
    "\n",
    "    # Mean TF-IDF per term across docs\n",
    "    scores = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "    # Top-k indices (safe if topk > n_features)\n",
    "    k = min(topk, scores.size)\n",
    "    if k == 0:\n",
    "        return []\n",
    "\n",
    "    # Use argpartition for efficiency, then sort those k by score desc\n",
    "    top_idx = np.argpartition(-scores, range(k))[:k]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    return [feature_names[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17876fc3-957a-46c3-a539-1304ae6c7d50",
   "metadata": {
    "id": "17876fc3-957a-46c3-a539-1304ae6c7d50"
   },
   "source": [
    "## Rate, KL_Divergence, Coupling_Entropy, and Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35303cd9-7e77-4085-a0da-6dcd4098c6cb",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758492482056,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "35303cd9-7e77-4085-a0da-6dcd4098c6cb"
   },
   "outputs": [],
   "source": [
    "def rate_complexity(kg: Dict[str, Any]) -> float:\n",
    "    n = len(kg.get(\"nodes\", []))\n",
    "    m = len(kg.get(\"edges\", []))\n",
    "    return float(n) + 0.5 * float(m)\n",
    "\n",
    "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    # Symmetrized KL\n",
    "    p = p + 1e-12\n",
    "    q = q + 1e-12\n",
    "    kl_pq = float(np.sum(rel_entr(p, q)))\n",
    "    kl_qp = float(np.sum(rel_entr(q, p)))\n",
    "    return 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "def coupling_entropy(col: np.ndarray) -> float:\n",
    "    p = col / (col.sum() + 1e-12)\n",
    "    p = p + 1e-12\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "def compute_objective(kg: Dict[str, Any], beta: float, fgw_total: float) -> float:\n",
    "    return rate_complexity(kg) + beta * fgw_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62168b-c747-4a2b-9770-1b669ddae3ce",
   "metadata": {
    "id": "8a62168b-c747-4a2b-9770-1b669ddae3ce"
   },
   "source": [
    "## KG Refinement Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50196bbf-72a9-4458-a8e0-f9795e1ab174",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1758492485841,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "50196bbf-72a9-4458-a8e0-f9795e1ab174"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# KG Refinement Operations\n",
    "# -----------------------------\n",
    "\n",
    "\"\"\"\n",
    "def make_concept_label_from_text(text: str) -> str:\n",
    "    # Simple heuristic: TF-IDF top terms or fallback to first 5 words\n",
    "    toks = re.findall(r\"[A-Za-z_][A-Za-z0-9_]+\", text)[:20]\n",
    "    if len(toks) >= 3:\n",
    "        return \" \".join(toks[:3]).lower()\n",
    "    return (text[:30] + \"...\").strip()\n",
    "\"\"\"\n",
    "\n",
    "def add_concept_from_element(kg: Dict[str, Any],\n",
    "                             element: LectureElement,\n",
    "                             embedder: Embedder,\n",
    "                             allowed_relations: List[str],\n",
    "                             connect_to: Optional[str] = None,\n",
    "                             relation: str = \"relatedTo\") -> KGNode:\n",
    "    label = make_concept_label_from_text_LLM(element.text)\n",
    "\n",
    "    if label != \"''\" and label != '\"\"' or (not label):\n",
    "        node_id = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", label).strip(\"_\")\n",
    "        if not node_id:\n",
    "            node_id = f\"new_node_{len(kg['nodes'])+1}\"\n",
    "        # Ensure unique\n",
    "        base = node_id\n",
    "        c = 1\n",
    "        existing_ids = {n[\"id\"] for n in kg[\"nodes\"]}\n",
    "        while node_id in existing_ids:\n",
    "            c += 1\n",
    "            node_id = f\"{base}_{c}\"\n",
    "        node = {\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": \"Concept\",\n",
    "            \"definition\": element.text,\n",
    "            \"aliases\": [],\n",
    "            \"attributes\": {},\n",
    "            \"provenance\": [{\n",
    "                \"section_path\": element.section_path,\n",
    "                \"line_start\": element.idx,\n",
    "                \"line_end\": element.idx,\n",
    "                \"text_excerpt\": element.text\n",
    "            }],\n",
    "            \"confidence\": 0.6,\n",
    "            \"rationale\": \"Added to reduce distortion; under-represented lecture content.\"\n",
    "        }\n",
    "        kg[\"nodes\"].append(node)\n",
    "\n",
    "        # Use LLM to make edges for the new node\n",
    "        candidate_edges = make_edges_between_text_LLM(kg, node, allowed_relations)\n",
    "\n",
    "        edges = []\n",
    "\n",
    "        # Rank candidate edges by confidence and add the top one(s)\n",
    "        if candidate_edges:\n",
    "            candidate_edges.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "            top_confidence = candidate_edges[0][\"confidence\"]\n",
    "            edges_to_add = [\n",
    "                # Consider ties within 5%\n",
    "                edge for edge in candidate_edges if edge[\"confidence\"] >= top_confidence * 0.95\n",
    "            ]\n",
    "\n",
    "            for edge_info in edges_to_add:\n",
    "                edge = {\n",
    "                    \"id\": f\"e_{edge_info['source']}_{edge_info['relation']}_{edge_info['target']}\",\n",
    "                    \"source\": edge_info['source'],\n",
    "                    \"target\": edge_info['target'],\n",
    "                    \"relation\": edge_info['relation'],\n",
    "                    \"definition\": edge_info['definition'],\n",
    "                    \"provenance\": [{\n",
    "                        \"section_path\": element.section_path,\n",
    "                        \"line_start\": element.idx,\n",
    "                        \"line_end\": element.idx,\n",
    "                        \"text_excerpt\": element.text\n",
    "                    }],\n",
    "                    \"confidence\": edge_info['confidence'],\n",
    "                    \"rationale\": edge_info['rationale']\n",
    "                }\n",
    "                # Avoid duplicates\n",
    "                edge_sig = (edge[\"source\"].lower(), edge[\"target\"].lower(), edge[\"relation\"].lower())\n",
    "                if not any((e[\"source\"].lower(), e[\"target\"].lower(), e[\"relation\"].lower()) == edge_sig for e in kg[\"edges\"]):\n",
    "                    kg[\"edges\"].append(edge)\n",
    "                    edges.append(edge)\n",
    "        else:\n",
    "            if connect_to and relation in allowed_relations:\n",
    "                # get edge definition\n",
    "                e_def = get_edge_definition(kg, node_id, connect_to, relation)\n",
    "                \n",
    "                edge = {\n",
    "                    \"id\": f\"e_{node_id}_{relation}_{connect_to}\",\n",
    "                    \"source\": node_id,\n",
    "                    \"target\": connect_to,\n",
    "                    \"relation\": relation,\n",
    "                    \"definition\": e_def,\n",
    "                    \"provenance\": [{\n",
    "                        \"section_path\": element.section_path,\n",
    "                        \"line_start\": element.idx,\n",
    "                        \"line_end\": element.idx,\n",
    "                        \"text_excerpt\": element.text\n",
    "                    }],\n",
    "                    \"confidence\": 0.6,\n",
    "                    \"rationale\": \"Heuristic relation based on semantic proximity.\"\n",
    "                }\n",
    "                # Avoid duplicates\n",
    "                edge_sig = (edge[\"source\"].lower(), edge[\"target\"].lower(), edge[\"relation\"].lower())\n",
    "                if not any((e[\"source\"].lower(), e[\"target\"].lower(), e[\"relation\"].lower()) == edge_sig for e in kg[\"edges\"]):\n",
    "                    kg[\"edges\"].append(edge)\n",
    "                    edges.append(edge)\n",
    "\n",
    "        return KGNode(**node), edges\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def kmeans_split_concept(col: np.ndarray,\n",
    "                         E_L: np.ndarray,\n",
    "                         elements: List[LectureElement],\n",
    "                         kg: Dict[str, Any],\n",
    "                         node_idx: int) -> Tuple[Optional[KGNode], Optional[KGNode], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Split a concept into two based on the embedding clusters of its coupled\n",
    "    lecture elements.\n",
    "    Returns new nodes (as dicts) and index sets assigned to each.\n",
    "    \"\"\"\n",
    "    weights = col\n",
    "    sel = np.where(weights > weights.mean())[0]\n",
    "    if len(sel) < 4:\n",
    "        return None, None, [], []\n",
    "    X = E_L[sel]\n",
    "    try:\n",
    "        km = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "        labs = km.fit_predict(X)\n",
    "    except Exception:\n",
    "        return None, None, [], []\n",
    "\n",
    "    node = kg[\"nodes\"][node_idx]\n",
    "    base_id = node[\"id\"]\n",
    "\n",
    "    a_idx = [int(sel[i]) for i in range(len(sel)) if labs[i] == 0]\n",
    "    b_idx = [int(sel[i]) for i in range(len(sel)) if labs[i] == 1]\n",
    "\n",
    "    # Concatenate the text of a_idx and b_idx\n",
    "    text_a = \"\\n\".join([elements[i].text for i in a_idx])\n",
    "    text_b = \"\\n\".join([elements[i].text for i in b_idx])\n",
    "\n",
    "    # Use LLM to make a label\n",
    "    label_a = make_concept_label_from_text_LLM(text_a)\n",
    "    if label_a == \"''\" or label_a == '\"\"' or (not label_a):\n",
    "        nA_txt = tfidf_top_terms([elements[i].text for i in a_idx], topk=3)\n",
    "        label_a = (node[\"label\"] + \" \" + \" \".join(nA_txt)).strip()\n",
    "\n",
    "    label_b = make_concept_label_from_text_LLM(text_b)\n",
    "    if label_b == \"''\" or label_b == '\"\"' or (not label_b):\n",
    "        nB_txt = tfidf_top_terms([elements[i].text for i in b_idx], topk=3)\n",
    "        label_b = (node[\"label\"] + \" \" + \" \".join(nB_txt)).strip()\n",
    "\n",
    "    def _new(name_suffix, label, txt):\n",
    "\n",
    "        new_id = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", label).strip(\"_\")\n",
    "        \n",
    "        if not new_id:\n",
    "            new_id = f\"{base_id}_{name_suffix}\"\n",
    "            \n",
    "        # Ensure unique\n",
    "        base = new_id\n",
    "        c = 1\n",
    "        existing_ids = {n[\"id\"] for n in kg[\"nodes\"]}\n",
    "        while new_id in existing_ids:\n",
    "            c += 1\n",
    "            new_id = f\"{base}_{c}\"\n",
    "        \n",
    "        new_node = dict(node)\n",
    "        new_node[\"id\"] = new_id\n",
    "        new_node[\"label\"] = label\n",
    "        new_node[\"confidence\"] = 0.55\n",
    "        new_node[\"definition\"] = txt\n",
    "        new_node[\"provenance\"][0]['text_excerpt'] = txt\n",
    "        new_node[\"rationale\"] = \"Split concept with diverse coupling.\"\n",
    "        return new_node\n",
    "\n",
    "    A = _new(\"a\", label_a, text_a)\n",
    "    B = _new(\"b\", label_b, text_b)\n",
    "    return KGNode(**A), KGNode(**B), a_idx, b_idx\n",
    "\n",
    "def merge_if_redundant(i: int, j: int,\n",
    "                       P: np.ndarray,\n",
    "                       E_K: np.ndarray,\n",
    "                       sim_thresh: float,\n",
    "                       kl_thresh: float) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if concepts i and j should be merged (high semantic sim & low KL).\n",
    "    \"\"\"\n",
    "    v1 = E_K[i]; v2 = E_K[j]\n",
    "    cos = float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12))\n",
    "    if cos < sim_thresh:\n",
    "        return False\n",
    "    pi = P[:, i]; pj = P[:, j]\n",
    "    kl = kl_divergence(pi, pj)\n",
    "    return kl < kl_thresh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b259b3-845f-4a46-93fa-c434a4d33827",
   "metadata": {},
   "source": [
    "## KG Final Refinement by Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c77fbcd4-db1e-4b4a-9aa1-4de827a96f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_json_list(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return a JSON string representing a list.\n",
    "    Handles raw JSON or fenced code blocks like:\n",
    "    ```json\n",
    "    [ ... ]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Try direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            return json.dumps(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Look for fenced block ```json ... ``` or ``` ...\n",
    "    fence_match = re.search(r\"```(?:json)?\\s*(\\[.*?\\])\\s*```\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if fence_match:\n",
    "        return fence_match.group(1)\n",
    "\n",
    "    # Fallback: capture first bracketed list (greedy but safe-ish)\n",
    "    bracket_match = re.search(r\"(\\[.*\\])\", text, flags=re.DOTALL)\n",
    "    if bracket_match:\n",
    "        return bracket_match.group(1)\n",
    "\n",
    "    raise ValueError(\"No JSON array found in LLM output.\")\n",
    "\n",
    "def apply_domain_knowledge_LLM(kg: Dict[str, Any], allowed_relations: List[str]) -> List[KGEdge]:\n",
    "    \"\"\"\n",
    "    Refine the knowledge graph by adding missing edges based on domain knowledge\n",
    "    encoded in the KG, using an LLM guided by a structured prompt.\n",
    "\n",
    "    Args:\n",
    "        kg: Knowledge graph as a dict with nodes and edges.\n",
    "        allowed_relations: List of allowed relation types.\n",
    "\n",
    "    Returns:\n",
    "        A list of KGEdge objects proposed by the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert Knowledge Graph Refinement Agent.\n",
    "    Your goal is to propose NEW edges that make it easier to generate high-quality\n",
    "    multiple-choice questions (MCQs) that produce\n",
    "    high-quality distractors and har-to-guess answers. \n",
    "\n",
    "    ### Inputs\n",
    "    1. Knowledge Graph (KG): {kg_json}\n",
    "    2. Allowed Relations: {allowed_relations}\n",
    "\n",
    "    ### Task Focus\n",
    "    - Prioritize the edges that help build strong distractors.\n",
    "    - Typical contrastive relations include: contrastsWith, antonymOf, differentFrom, opposes, distinguishesFrom, versus/vs.\n",
    "    - Only use relation types that are in the Allowed Relations list.\n",
    "\n",
    "    ### Evidence Constraints\n",
    "    - Base your proposals STRICTLY on domain knowledge already encoded in the KG\n",
    "      (node labels/definitions, existing relations, hierarchies, examples, contrasts).\n",
    "    - Do NOT invent facts beyond what the KG implies.\n",
    "\n",
    "    ### Quality Bar (MCQ Utility)\n",
    "    For each proposed edge:\n",
    "    - It should enable question writers to craft MCQs where wrong options \n",
    "    are **plausible** yet **incorrect**.\n",
    "    - Prefer pairs that are commonly confused or that share overlapping \n",
    "      properties but differ on key dimensions.\n",
    "    - Avoid trivial or redundant contrasts.\n",
    "    \n",
    "    ### Output\n",
    "    - A list of new edges in the following structure:\n",
    "    e = {{\n",
    "        \"id\": \"<unique_id>\",\n",
    "        \"source\": \"<src_node_id>\",\n",
    "        \"target\": \"<dst_node_id>\",\n",
    "        \"relation\": \"<relation_type>\",\n",
    "        \"definition\": \"<edge_definition>\",\n",
    "        \"provenance\": [],\n",
    "        \"confidence\": <number between 0 and 1>,\n",
    "        \"rationale\": \"<explanation of why this relation was added>\"\n",
    "    }}\n",
    "\n",
    "    Do not repeat existing edges. Only include new, refined edges.\n",
    "    Ensure src_node_id and dst_node_id are existing nodes in the knowledge graph.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Chain-of-Thought Refinement Instructions\n",
    "\n",
    "    1. **Deconstruct the KG**\n",
    "       - Read all nodes and existing edges.\n",
    "       - Note the domain knowledge explicitly encoded in the graph \n",
    "         (hierarchies, prerequisites, usage patterns, examples, contrasts).\n",
    "\n",
    "    2. **Diagnose Gaps**\n",
    "       - Identify pairs of nodes that appear semantically, hierarchically, or logically related but lack an explicit relation.\n",
    "       - Use only domain knowledge present in the KG (structure, relations, node labels) to justify candidate edges.\n",
    "\n",
    "    3. **Develop Candidate Edges**\n",
    "       - For each missing link:\n",
    "         - Choose the most appropriate relation type from the allowed set.\n",
    "         - Ensure the edge does not already exist.\n",
    "\n",
    "    4. **Reason Explicitly**\n",
    "       - For each new edge, provide a rationale:\n",
    "         - Why the two nodes are connected.\n",
    "         - Why this relation type is correct.\n",
    "         - Why the confidence score (0–1) is appropriate.\n",
    "\n",
    "       Example reasoning:\n",
    "       Nodes: \"Linear Regression\" → \"Gradient Descent\"\n",
    "       Evidence: In the KG, Gradient Descent is used to optimize models; Linear Regression requires optimization.\n",
    "       Relation: uses\n",
    "       Confidence: 0.9\n",
    "       Rationale: Domain knowledge in the KG indicates optimization is part of regression training.\n",
    "\n",
    "    5. **Deliver Final List**\n",
    "       - Output only the list of new edge objects in the specified structure.\n",
    "       - Each edge must include id, source_id, target_id, relation, provenance, confidence, rationale.\n",
    "       - Ensure the source_id and target_id are in the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the placeholders\n",
    "    prompt = prompt_template.format(\n",
    "        kg_json=json.dumps(kg, indent=2),\n",
    "        allowed_relations=allowed_relations\n",
    "    )\n",
    "\n",
    "    # ---- Call your LLM here ----\n",
    "    # Replace this with your actual LLM call (OpenAI, Anthropic, etc.)\n",
    "    # For example, if using OpenAI:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    # )\n",
    "    # raw_output = response.choices[0].message[\"content\"]\n",
    "\n",
    "    #raw_output = llm_call(prompt)  # <-- Replace with your LLM wrapper\n",
    "\n",
    "    raw_output = llm_call_gemini(prompt, max_output_tokens=50000)\n",
    "\n",
    "    # Try parsing JSON safely\n",
    "    json_str = _extract_json_list(raw_output)\n",
    "    try:\n",
    "        edges: List[KGEdge] = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"LLM output could not be parsed as JSON:\\n\" + json_str)\n",
    "        return None\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf143c-4ad9-4d3f-97a0-4ff9214c5c02",
   "metadata": {},
   "source": [
    "## Add the Edges Refined by Domain Knowledge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74308fab-12d4-4055-832a-e435ae72fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_refined_domain_edges(kg: Dict[str, Any], edges: List[KGEdge]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - Ensure each edge's source/target node exists by id; if not, create a new node.\n",
    "    - New nodes follow your requested schema and use the edge's rationale.\n",
    "    - Append normalized edges to kg['edges'] (avoid duplicates).\n",
    "    \"\"\"\n",
    "\n",
    "    added_node_counter = 0\n",
    "    added_edge_counter = 0\n",
    "    \n",
    "    kg.setdefault(\"nodes\", [])\n",
    "    kg.setdefault(\"edges\", [])\n",
    "\n",
    "    def node_exists(node_id: str) -> bool:\n",
    "        nid = node_id.lower()\n",
    "        return any(str(n.get(\"id\", \"\")).lower() == nid for n in kg[\"nodes\"])\n",
    "\n",
    "    def add_node_from_edge(node_id: str, edge: KGEdge) -> None:\n",
    "        kg[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": node_id,  # per requirement: use id for label\n",
    "            \"type\": \"Concept\",\n",
    "            \"definition\": edge.get(\"rationale\", \"\") or \"\",\n",
    "            \"aliases\": [],  # you didn't request aliases here\n",
    "            \"provenance\": [{\"text\": edge.get(\"rationale\", \"\") or \"\"}],\n",
    "            \"attributes\": {},\n",
    "            \"confidence\": float(edge.get(\"confidence\", 0.5) or 0.5),\n",
    "            \"rationale\": \"Add unmatched node by domain knowledge from LLM\"\n",
    "        })\n",
    "\n",
    "    def edge_exists(src_id: str, rel: str, dst_id: str) -> bool:\n",
    "        s, r, t = src_id.lower(), rel, dst_id.lower()\n",
    "        return any(\n",
    "            str(e.get(\"source\", \"\")).lower() == s and\n",
    "            str(e.get(\"relation\", \"\")) == r and\n",
    "            str(e.get(\"target\", \"\")).lower() == t\n",
    "            for e in kg[\"edges\"]\n",
    "        )\n",
    "\n",
    "    for e in edges:\n",
    "        rel = str(e.get(\"relation\", \"\")).strip()\n",
    "        if not rel:\n",
    "            continue\n",
    "\n",
    "        src_id = str(e.get(\"source\", \"\")).strip()\n",
    "        dst_id = str(e.get(\"target\", \"\")).strip()\n",
    "        if not src_id or not dst_id:\n",
    "            continue\n",
    "\n",
    "        e_def = str(e.get(\"definition\", \"\")).strip()\n",
    "        if not e_def:\n",
    "            continue\n",
    "\n",
    "        if not node_exists(src_id):\n",
    "            add_node_from_edge(src_id, e)\n",
    "            added_node_counter += 1\n",
    "        if not node_exists(dst_id):\n",
    "            add_node_from_edge(dst_id, e)\n",
    "            added_node_counter += 1\n",
    "\n",
    "        # Normalize provenance to list[dict] with \"text\" if needed\n",
    "        norm_prov = []\n",
    "        norm_prov.append({\"text_excerpt\": e.get(\"rationale\", \"\")})\n",
    "\n",
    "        if not edge_exists(src_id, rel, dst_id):\n",
    "            kg[\"edges\"].append({\n",
    "                \"id\": str(e.get(\"id\", \"\")) or f\"e_{abs(hash(src_id + '::' + rel + '::' + dst_id))}\",\n",
    "                \"source\": src_id,\n",
    "                \"target\": dst_id,\n",
    "                \"relation\": rel,\n",
    "                \"definition\": e_def,\n",
    "                \"provenance\": norm_prov,\n",
    "                \"confidence\": float(e.get(\"confidence\", 0.5) or 0.5),\n",
    "                \"rationale\": str(e.get(\"rationale\", \"\")).strip(),\n",
    "            })\n",
    "\n",
    "            added_edge_counter += 1\n",
    "\n",
    "    return added_node_counter, added_edge_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dacc1-f8d7-4a74-96aa-a1ac4ad2daf4",
   "metadata": {
    "id": "a11dacc1-f8d7-4a74-96aa-a1ac4ad2daf4"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d11821f4-7e9b-4450-bf9e-2dfce2fd03d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1758492492893,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "d11821f4-7e9b-4450-bf9e-2dfce2fd03d3",
    "outputId": "035ca342-e805-4a37-8b5f-0349acf62e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using POT for exact FGW: %s True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "lecture_json_path = \"./data/lecture_notes_8.json\" #type=str, required=True, help=\"Path to lecture notes JSON\")\n",
    "kg_json_path = \"./data/kg8.json\" #type=str, required=True, help=\"Path to initial KG JSON\")\n",
    "out_dir = \"./data/output\" #type=str, required=True, help=\"Output directory\")\n",
    "#beta = 20.0 #type=float, default=20.0)\n",
    "beta = 100.0\n",
    "alpha = \"0.2,0.3,0.5\" #help=\"alpha weights for lecture distance (chron,logic,semantic)\"\n",
    "gamma = \"0.4,0.6\" #help=\"gamma weights for KG distance (struct,semantic)\")\n",
    "lambda_feat = 0.6 #help=\"FGW feature balance (alpha in POT)\")\n",
    "max_iterations = 10\n",
    "convergence_threshold = 0.01\n",
    "#theta_add = 0.06\n",
    "theta_add = 0.02\n",
    "theta_split = 0.35\n",
    "theta_merge = 0.12\n",
    "theta_relate = 0.25\n",
    "sinkhorn_eps = 0.05\n",
    "sinkhorn_iter = 300\n",
    "disable_domain_rules = True\n",
    "no_pot =  False #help=\"Disable POT even if installed (use proxy)\")\n",
    "seed = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    a_chron, a_logic, a_sem = [float(x) for x in alpha.split(\",\")]\n",
    "    g_struct, g_sem = [float(x) for x in gamma.split(\",\")]\n",
    "except Exception:\n",
    "    raise SystemExit(\"Please provide --alpha like '0.2,0.3,0.5' and --gamma like '0.4,0.6'\")\n",
    "\n",
    "hp = HyperParams(\n",
    "    alpha_chron=a_chron,\n",
    "    alpha_logic=a_logic,\n",
    "    alpha_sem=a_sem,\n",
    "    gamma_struct=g_struct,\n",
    "    gamma_sem=g_sem,\n",
    "    lambda_feat=lambda_feat,\n",
    "    beta=beta,\n",
    "    theta_add=theta_add,\n",
    "    theta_split=theta_split,\n",
    "    theta_merge=theta_merge,\n",
    "    theta_relate=theta_relate,\n",
    "    max_iterations=max_iterations,\n",
    "    convergence_threshold=convergence_threshold,\n",
    "    sinkhorn_eps=sinkhorn_eps,\n",
    "    sinkhorn_iter=sinkhorn_iter\n",
    ")\n",
    "\n",
    "use_pot = (not no_pot) and _HAS_POT\n",
    "if use_pot:\n",
    "    print(\"Using POT for exact FGW: %s\", use_pot)\n",
    "else:\n",
    "    print(\"Use POT proxy.\")\n",
    "\n",
    "ensure_dir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "389ade94-15f0-46a8-be2a-08549972c746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1758492495401,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "389ade94-15f0-46a8-be2a-08549972c746",
    "outputId": "a18c4d7e-c980-4b3e-aac0-ad14ffa11a82",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LectureElement(idx=2, id_path='1.1', section_path=['__ROOT__', 'Data Science Programming', 'Week 9: Lecture 1: Time Series Data Analysis'], text='apply techiques to time series data', typ='text'),\n",
       " LectureElement(idx=3, id_path='2', section_path=['__ROOT__', 'Time Series'], text='time series data is an important form of structured data in many different fields, such', typ='text'),\n",
       " LectureElement(idx=4, id_path='2', section_path=['__ROOT__', 'Time Series'], text='as finance, economics, ecology, neuroscience, and physics. anything that is observed', typ='text'),\n",
       " LectureElement(idx=5, id_path='2', section_path=['__ROOT__', 'Time Series'], text='or measured at many points in time forms a time series. many time series are fixed', typ='text'),\n",
       " LectureElement(idx=6, id_path='2', section_path=['__ROOT__', 'Time Series'], text='frequency, which is to say that data points occur at regular intervals according to some', typ='text'),\n",
       " LectureElement(idx=7, id_path='2', section_path=['__ROOT__', 'Time Series'], text='rule, such as every 15 seconds, every 5 minutes, or once per month. time series can', typ='text'),\n",
       " LectureElement(idx=8, id_path='2', section_path=['__ROOT__', 'Time Series'], text='also be irregular without a fixed unit of time or offset between units. how you mark', typ='text'),\n",
       " LectureElement(idx=9, id_path='2', section_path=['__ROOT__', 'Time Series'], text='and refer to time series data depends on the application, and you may have one of the', typ='text'),\n",
       " LectureElement(idx=10, id_path='2', section_path=['__ROOT__', 'Time Series'], text='following:', typ='text'),\n",
       " LectureElement(idx=11, id_path='2', section_path=['__ROOT__', 'Time Series'], text='timestamps, specific instants in time fixed periods, such as the month january 2007 or the full year 2010', typ='text'),\n",
       " LectureElement(idx=12, id_path='2', section_path=['__ROOT__', 'Time Series'], text='intervals of time, indicated by a start and end timestamp. periods can be thought', typ='text'),\n",
       " LectureElement(idx=13, id_path='2', section_path=['__ROOT__', 'Time Series'], text='of as special cases of intervals', typ='text'),\n",
       " LectureElement(idx=14, id_path='2', section_path=['__ROOT__', 'Time Series'], text='experiment or elapsed time; each timestamp is a measure of time relative to a', typ='text'),\n",
       " LectureElement(idx=15, id_path='2', section_path=['__ROOT__', 'Time Series'], text='particular start time (e.g., the diameter of a cookie baking each second since', typ='text'),\n",
       " LectureElement(idx=16, id_path='2', section_path=['__ROOT__', 'Time Series'], text='being placed in the oven)', typ='text'),\n",
       " LectureElement(idx=17, id_path='2', section_path=['__ROOT__', 'Time Series'], text='imports', typ='text'),\n",
       " LectureElement(idx=18, id_path='2', section_path=['__ROOT__', 'Time Series'], text='import pandas as pd', typ='text'),\n",
       " LectureElement(idx=19, id_path='2', section_path=['__ROOT__', 'Time Series'], text='import numpy as np', typ='text'),\n",
       " LectureElement(idx=20, id_path='2', section_path=['__ROOT__', 'Time Series'], text='import matplotlib.pyplot as plt', typ='text'),\n",
       " LectureElement(idx=21, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='the python standard library includes data types for date and time data, as well as', typ='text'),\n",
       " LectureElement(idx=22, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='calendar-related functionality. the datetime, time, and calendar modules are the', typ='text'),\n",
       " LectureElement(idx=23, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='main places to start. the datetime.datetime type, or simply datetime, is widely', typ='text'),\n",
       " LectureElement(idx=24, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='used.', typ='text'),\n",
       " LectureElement(idx=26, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='from datetime import datetime', typ='text'),\n",
       " LectureElement(idx=27, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='now = datetime.now()', typ='text'),\n",
       " LectureElement(idx=28, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='now.year, now.month, now.day', typ='text'),\n",
       " LectureElement(idx=30, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='we can apply arithmatic operations on datetime objects:', typ='text'),\n",
       " LectureElement(idx=32, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='datetime.now() - datetime(2024, 4, 20)', typ='text'),\n",
       " LectureElement(idx=34, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='the result have time related properties:', typ='text'),\n",
       " LectureElement(idx=36, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='delta = datetime(2021, 1, 7) - datetime(2008, 6, 24, 8, 15)', typ='text'),\n",
       " LectureElement(idx=37, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='delta.days', typ='text'),\n",
       " LectureElement(idx=38, id_path='2.1', section_path=['__ROOT__', 'Time Series', 'Date and Time Data Types and Tools'], text='delta.seconds', typ='text'),\n",
       " LectureElement(idx=40, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='working with time zones is generally considered one of the most unpleasant parts of', typ='text'),\n",
       " LectureElement(idx=41, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='time series manipulation. as a result, many time series users choose to work with', typ='text'),\n",
       " LectureElement(idx=42, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='time series in coordinated universal time or utc, which is the successor to greenwich', typ='text'),\n",
       " LectureElement(idx=43, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='mean time and is the current international standard. time zones are expressed as', typ='text'),\n",
       " LectureElement(idx=44, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='offsets from utc; for , new york is four hours behind utc during daylight', typ='text'),\n",
       " LectureElement(idx=45, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='saving time and five hours behind the rest of the year.', typ='text'),\n",
       " LectureElement(idx=46, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='in python, time zone information comes from the third-party pytz library (installable', typ='text'),\n",
       " LectureElement(idx=47, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='with pip or conda), which exposes the olson database, a compilation of world', typ='text'),\n",
       " LectureElement(idx=48, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='time zone information. this is especially important for historical data because the', typ='text'),\n",
       " LectureElement(idx=49, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='daylight saving time (dst) transition dates (and even utc offsets) have been', typ='text'),\n",
       " LectureElement(idx=50, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='changed numerous times depending on the whims of local governments. in the united', typ='text'),\n",
       " LectureElement(idx=51, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='states, the dst transition times have been changed many times since 1900!', typ='text'),\n",
       " LectureElement(idx=53, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='import pytz', typ='text'),\n",
       " LectureElement(idx=54, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text=\"local_tz = pytz.timezone('america/new_york')\", typ='text'),\n",
       " LectureElement(idx=55, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='now = datetime.now()', typ='text'),\n",
       " LectureElement(idx=56, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='now = now.astimezone(local_tz)', typ='text'),\n",
       " LectureElement(idx=57, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='now.year', typ='text'),\n",
       " LectureElement(idx=58, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='now.hour', typ='text'),\n",
       " LectureElement(idx=59, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='now.tzinfo', typ='text'),\n",
       " LectureElement(idx=62, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='for tz in pytz.common_timezones:', typ='text'),\n",
       " LectureElement(idx=63, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text=\"if 'america' in tz:\", typ='text'),\n",
       " LectureElement(idx=64, id_path='2.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling'], text='print(tz)', typ='text'),\n",
       " LectureElement(idx=66, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text='format datetime objects and pandas timestamp objects as strings using str or the strftime method, passing a format specification.', typ='text'),\n",
       " LectureElement(idx=68, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text='stamp = datetime(2021, 1, 3)', typ='text'),\n",
       " LectureElement(idx=69, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text='str(stamp)', typ='text'),\n",
       " LectureElement(idx=70, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text=\"stamp.strftime('%y-%m-%d')\", typ='text'),\n",
       " LectureElement(idx=71, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text=\"s = stamp.strftime('%m/%d/%y')\", typ='text'),\n",
       " LectureElement(idx=73, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text='convert from string to datetime:', typ='text'),\n",
       " LectureElement(idx=75, id_path='2.2.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling', 'Converting Between String and Datetime'], text=\"d = datetime.strptime(s, '%m/%d/%y')\", typ='text'),\n",
       " LectureElement(idx=78, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text=\"value = '2021-01-03'\", typ='text'),\n",
       " LectureElement(idx=79, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text=\"datetime.strptime(value, '%y-%m-%d')\", typ='text'),\n",
       " LectureElement(idx=80, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text=\"datestrs = ['7/6/2021', '8/6/2021']\", typ='text'),\n",
       " LectureElement(idx=81, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text=\"[datetime.strptime(x, '%m/%d/%y') for x in datestrs]\", typ='text'),\n",
       " LectureElement(idx=83, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text='pandas is generally oriented toward working with arrays of dates, whether used as an', typ='text'),\n",
       " LectureElement(idx=84, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text='axis index or a column in a dataframe. the to_datetime method parses many different', typ='text'),\n",
       " LectureElement(idx=85, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text='kinds of date representations. standard date formats like iso 8601 can be', typ='text'),\n",
       " LectureElement(idx=86, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text='parsed very quickly:', typ='text'),\n",
       " LectureElement(idx=88, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text=\"datestrs = ['2021-07-06 12:00:00', '2021-08-06 00:00:00']\", typ='text'),\n",
       " LectureElement(idx=89, id_path='2.3', section_path=['__ROOT__', 'Time Series', 'Exercise:'], text='pd.to_datetime(datestrs)', typ='text'),\n",
       " LectureElement(idx=91, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='a basic kind of time series object in pandas is a series indexed by timestamps, which', typ='text'),\n",
       " LectureElement(idx=92, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='is often represented external to pandas as python strings or datetime objects.', typ='text'),\n",
       " LectureElement(idx=94, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='from datetime import datetime', typ='text'),\n",
       " LectureElement(idx=95, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='dates = [datetime(2021, 1, 2), datetime(2021, 1, 5),', typ='text'),\n",
       " LectureElement(idx=96, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='datetime(2021, 1, 7), datetime(2021, 1, 8),', typ='text'),\n",
       " LectureElement(idx=97, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='datetime(2021, 1, 10), datetime(2021, 1, 12)]', typ='text'),\n",
       " LectureElement(idx=98, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='ts = pd.series(np.random.randn(6), index=dates)', typ='text'),\n",
       " LectureElement(idx=99, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='ts', typ='text'),\n",
       " LectureElement(idx=102, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='ts.index', typ='text'),\n",
       " LectureElement(idx=104, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='like other series, arithmetic operations between differently indexed time series automatically', typ='text'),\n",
       " LectureElement(idx=105, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='align on the dates:', typ='text'),\n",
       " LectureElement(idx=107, id_path='2.4', section_path=['__ROOT__', 'Time Series', 'Time Series Basics'], text='ts + ts[::2]', typ='text'),\n",
       " LectureElement(idx=109, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='time series behaves like any other pandas.series when you are indexing and selecting', typ='text'),\n",
       " LectureElement(idx=110, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='data based on label:', typ='text'),\n",
       " LectureElement(idx=112, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='stamp = ts.index[2]', typ='text'),\n",
       " LectureElement(idx=113, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='ts[stamp]', typ='text'),\n",
       " LectureElement(idx=115, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='for longer time series, a year or only a year and month can be passed to easily select', typ='text'),\n",
       " LectureElement(idx=116, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='slices of data:', typ='text'),\n",
       " LectureElement(idx=118, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='longer_ts = pd.series(np.random.randn(1000),', typ='text'),\n",
       " LectureElement(idx=119, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text=\"index=pd.date_range('1/1/2023', periods=1000))\", typ='text'),\n",
       " LectureElement(idx=120, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text='longer_ts', typ='text'),\n",
       " LectureElement(idx=123, id_path='2.4.1', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Indexing, Selection, Subsetting'], text=\"longer_ts['2023-5']\", typ='text'),\n",
       " LectureElement(idx=125, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='in some applications, there may be multiple data observations falling on a particular', typ='text'),\n",
       " LectureElement(idx=126, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='timestamp. here is an :', typ='text'),\n",
       " LectureElement(idx=128, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text=\"dates = pd.datetimeindex(['1/1/2000', '1/2/2000', '1/2/2000',\", typ='text'),\n",
       " LectureElement(idx=129, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text=\"'1/2/2000', '1/3/2000'])\", typ='text'),\n",
       " LectureElement(idx=130, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='dup_ts = pd.series(np.arange(5), index=dates)', typ='text'),\n",
       " LectureElement(idx=131, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='dup_ts', typ='text'),\n",
       " LectureElement(idx=133, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='suppose you wanted to aggregate the data having non-unique timestamps. one way', typ='text'),\n",
       " LectureElement(idx=134, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='to do this is to use groupby and pass level=0:', typ='text'),\n",
       " LectureElement(idx=136, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='grouped = dup_ts.groupby(level=0)', typ='text'),\n",
       " LectureElement(idx=137, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='grouped.mean()', typ='text'),\n",
       " LectureElement(idx=138, id_path='2.4.2', section_path=['__ROOT__', 'Time Series', 'Time Series Basics', 'Time Series with Duplicate Indices'], text='grouped.count()', typ='text'),\n",
       " LectureElement(idx=140, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='generic time series in pandas are assumed to be irregular; that is, they have no fixed', typ='text'),\n",
       " LectureElement(idx=141, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='frequency. for many applications this is sufficient. however, it’s often desirable to', typ='text'),\n",
       " LectureElement(idx=142, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='work relative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if', typ='text'),\n",
       " LectureElement(idx=143, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='that means introducing missing values into a time series. fortunately pandas has a', typ='text'),\n",
       " LectureElement(idx=144, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='full suite of standard time series frequencies and tools for resampling, inferring frequencies,', typ='text'),\n",
       " LectureElement(idx=145, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='and generating fixed-frequency date ranges. for , you can convert', typ='text'),\n",
       " LectureElement(idx=146, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='the sample time series to be fixed daily frequency by calling resample:', typ='text'),\n",
       " LectureElement(idx=148, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='from datetime import datetime', typ='text'),\n",
       " LectureElement(idx=149, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='dates = [datetime(2021, 1, 2), datetime(2021, 1, 5),', typ='text'),\n",
       " LectureElement(idx=150, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='datetime(2021, 1, 7), datetime(2021, 1, 8),', typ='text'),\n",
       " LectureElement(idx=151, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='datetime(2021, 1, 10), datetime(2021, 1, 12)]', typ='text'),\n",
       " LectureElement(idx=152, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='ts = pd.series(np.random.randn(6), index=dates)', typ='text'),\n",
       " LectureElement(idx=153, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='ts', typ='text'),\n",
       " LectureElement(idx=156, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text=\"ts.resample('d')\", typ='text'),\n",
       " LectureElement(idx=159, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text=\"resampler = ts.resample('d')\", typ='text'),\n",
       " LectureElement(idx=160, id_path='2.5', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting'], text='resampler', typ='text'),\n",
       " LectureElement(idx=162, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text='pandas.date_range is responsible for', typ='text'),\n",
       " LectureElement(idx=163, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text='generating a datetimeindex with an indicated length according to a particular', typ='text'),\n",
       " LectureElement(idx=164, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text='frequency:', typ='text'),\n",
       " LectureElement(idx=166, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"index = pd.date_range('2012-04-01', '2012-06-01')\", typ='text'),\n",
       " LectureElement(idx=167, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text='index', typ='text'),\n",
       " LectureElement(idx=170, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"pd.date_range(start='2012-04-01', periods=20)\", typ='text'),\n",
       " LectureElement(idx=171, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"pd.date_range(end='2012-06-01', periods=20)\", typ='text'),\n",
       " LectureElement(idx=174, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"pd.date_range('2000-01-01', '2000-12-01', freq='bm')\", typ='text'),\n",
       " LectureElement(idx=177, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"pd.date_range('2012-05-02 12:56:31', periods=5)\", typ='text'),\n",
       " LectureElement(idx=180, id_path='2.5.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Generating Date Ranges'], text=\"pd.date_range('2012-05-02 12:56:31', periods=5, normalize=true)\", typ='text'),\n",
       " LectureElement(idx=183, id_path='2.5.2', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets'], text='from pandas.tseries.offsets import hour, minute', typ='text'),\n",
       " LectureElement(idx=184, id_path='2.5.2', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets'], text='hour = hour()', typ='text'),\n",
       " LectureElement(idx=185, id_path='2.5.2', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets'], text='hour', typ='text'),\n",
       " LectureElement(idx=188, id_path='2.5.2', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets'], text='four_hours = hour(4)', typ='text'),\n",
       " LectureElement(idx=189, id_path='2.5.2', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets'], text='four_hours', typ='text'),\n",
       " LectureElement(idx=191, id_path='2.5.2.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets', 'Week of month dates'], text='one useful frequency class is “week of month,” starting with wom. this enables you to', typ='text'),\n",
       " LectureElement(idx=192, id_path='2.5.2.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets', 'Week of month dates'], text='get dates like the third friday of each month:', typ='text'),\n",
       " LectureElement(idx=194, id_path='2.5.2.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets', 'Week of month dates'], text=\"rng = pd.date_range('2012-01-01', '2012-09-01', freq='wom-3fri')\", typ='text'),\n",
       " LectureElement(idx=195, id_path='2.5.2.1', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Frequencies and Date Offsets', 'Week of month dates'], text='list(rng)', typ='text'),\n",
       " LectureElement(idx=197, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='“shifting” refers to moving data backward and forward through time. both series and', typ='text'),\n",
       " LectureElement(idx=198, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='dataframe have a shift method for doing naive shifts forward or backward, leaving', typ='text'),\n",
       " LectureElement(idx=199, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='the index unmodified:', typ='text'),\n",
       " LectureElement(idx=201, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='ts = pd.series(np.random.randn(4),', typ='text'),\n",
       " LectureElement(idx=202, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text=\"index=pd.date_range('1/1/2000', periods=4, freq='m'))\", typ='text'),\n",
       " LectureElement(idx=203, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='ts', typ='text'),\n",
       " LectureElement(idx=204, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='ts.shift(2)', typ='text'),\n",
       " LectureElement(idx=205, id_path='2.5.3', section_path=['__ROOT__', 'Time Series', 'Date Ranges, Frequencies, and Shifting', 'Shifting (Leading and Lagging) Data'], text='ts.shift(-2)', typ='text'),\n",
       " LectureElement(idx=207, id_path='2.6.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Time Zone Localization and Conversion'], text='by default, time series in pandas are time zone naive. for , consider the following', typ='text'),\n",
       " LectureElement(idx=208, id_path='2.6.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Time Zone Localization and Conversion'], text='time series:', typ='text'),\n",
       " LectureElement(idx=210, id_path='2.6.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Time Zone Localization and Conversion'], text=\"rng = pd.date_range('3/9/2012 9:30', periods=6, freq='d')\", typ='text'),\n",
       " LectureElement(idx=211, id_path='2.6.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Time Zone Localization and Conversion'], text='ts = pd.series(np.random.randn(len(rng)), index=rng)', typ='text'),\n",
       " LectureElement(idx=212, id_path='2.6.1', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Time Zone Localization and Conversion'], text='ts', typ='text'),\n",
       " LectureElement(idx=214, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text='similar to time series and date ranges, individual timestamp objects similarly can be', typ='text'),\n",
       " LectureElement(idx=215, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text='localized from naive to time zone–aware and converted from one time zone to', typ='text'),\n",
       " LectureElement(idx=216, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text='another:', typ='text'),\n",
       " LectureElement(idx=218, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text=\"stamp = pd.timestamp('2011-03-12 04:00')\", typ='text'),\n",
       " LectureElement(idx=219, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text=\"stamp_utc = stamp.tz_localize('utc')\", typ='text'),\n",
       " LectureElement(idx=220, id_path='2.6.2', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations with Time Zone−Aware Timestamp Objects'], text=\"stamp_utc.tz_convert('america/new_york')\", typ='text'),\n",
       " LectureElement(idx=222, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='if two time series with different time zones are combined, the result will be utc.', typ='text'),\n",
       " LectureElement(idx=223, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='since the timestamps are stored under the hood in utc, this is a straightforward', typ='text'),\n",
       " LectureElement(idx=224, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='operation and requires no conversion to happen:', typ='text'),\n",
       " LectureElement(idx=226, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text=\"rng = pd.date_range('3/7/2012 9:30', periods=10, freq='b')\", typ='text'),\n",
       " LectureElement(idx=227, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='ts = pd.series(np.random.randn(len(rng)), index=rng)', typ='text'),\n",
       " LectureElement(idx=228, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='ts', typ='text'),\n",
       " LectureElement(idx=229, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text=\"ts1 = ts[:7].tz_localize('europe/london')\", typ='text'),\n",
       " LectureElement(idx=230, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text=\"ts2 = ts1[2:].tz_convert('europe/moscow')\", typ='text'),\n",
       " LectureElement(idx=231, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='result = ts1 + ts2', typ='text'),\n",
       " LectureElement(idx=232, id_path='2.6.3', section_path=['__ROOT__', 'Time Series', 'Time Zone Handling in Pandas', 'Operations Between Different Time Zones'], text='result.index', typ='text'),\n",
       " LectureElement(idx=234, id_path='2.7', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic'], text='periods represent timespans, like days, months, quarters, or years. the period class', typ='text'),\n",
       " LectureElement(idx=235, id_path='2.7', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic'], text='represents this data type, requiring a string or integer and a frequency.', typ='text'),\n",
       " LectureElement(idx=237, id_path='2.7', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic'], text=\"p = pd.period(2007, freq='a-dec')\", typ='text'),\n",
       " LectureElement(idx=238, id_path='2.7', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic'], text='p', typ='text'),\n",
       " LectureElement(idx=241, id_path='2.7.1', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Period Frequency Conversion'], text=\"p = pd.period('2007', freq='a-dec')\", typ='text'),\n",
       " LectureElement(idx=242, id_path='2.7.1', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Period Frequency Conversion'], text='p', typ='text'),\n",
       " LectureElement(idx=243, id_path='2.7.1', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Period Frequency Conversion'], text=\"p.asfreq('m', how='start')\", typ='text'),\n",
       " LectureElement(idx=244, id_path='2.7.1', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Period Frequency Conversion'], text=\"p.asfreq('m', how='end')\", typ='text'),\n",
       " LectureElement(idx=247, id_path='2.7.2', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Quarterly Period Frequencies'], text=\"p = pd.period('2012q4', freq='q-jan')\", typ='text'),\n",
       " LectureElement(idx=248, id_path='2.7.2', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Quarterly Period Frequencies'], text='p', typ='text'),\n",
       " LectureElement(idx=251, id_path='2.7.3', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Converting Timestamps to Periods (and Back)'], text=\"rng = pd.date_range('2000-01-01', periods=3, freq='m')\", typ='text'),\n",
       " LectureElement(idx=252, id_path='2.7.3', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Converting Timestamps to Periods (and Back)'], text='ts = pd.series(np.random.randn(3), index=rng)', typ='text'),\n",
       " LectureElement(idx=253, id_path='2.7.3', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Converting Timestamps to Periods (and Back)'], text='ts', typ='text'),\n",
       " LectureElement(idx=254, id_path='2.7.3', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Converting Timestamps to Periods (and Back)'], text='pts = ts.to_period()', typ='text'),\n",
       " LectureElement(idx=255, id_path='2.7.3', section_path=['__ROOT__', 'Time Series', 'Periods and Period Arithmetic', 'Converting Timestamps to Periods (and Back)'], text='pts', typ='text'),\n",
       " LectureElement(idx=258, id_path='2.8', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion'], text=\"rng = pd.date_range('2000-01-01', periods=100, freq='d')\", typ='text'),\n",
       " LectureElement(idx=259, id_path='2.8', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion'], text='ts = pd.series(np.random.randn(len(rng)), index=rng)', typ='text'),\n",
       " LectureElement(idx=260, id_path='2.8', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion'], text='ts', typ='text'),\n",
       " LectureElement(idx=261, id_path='2.8', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion'], text=\"ts.resample('m').mean()\", typ='text'),\n",
       " LectureElement(idx=262, id_path='2.8', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion'], text=\"ts.resample('m', kind='period').mean()\", typ='text'),\n",
       " LectureElement(idx=265, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text=\"rng = pd.date_range('2000-01-01', periods=12, freq='t')\", typ='text'),\n",
       " LectureElement(idx=266, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text='ts = pd.series(np.arange(12), index=rng)', typ='text'),\n",
       " LectureElement(idx=267, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text='ts', typ='text'),\n",
       " LectureElement(idx=270, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text=\"ts.resample('5min', closed='right').sum()\", typ='text'),\n",
       " LectureElement(idx=273, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text=\"ts.resample('5min', closed='right',\", typ='text'),\n",
       " LectureElement(idx=274, id_path='2.8.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling'], text=\"label='right', loffset='-1s').sum()\", typ='text'),\n",
       " LectureElement(idx=277, id_path='2.8.1.1', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Downsampling', 'Open-High-Low-Close (OHLC) resampling'], text=\"ts.resample('5min').ohlc()\", typ='text'),\n",
       " LectureElement(idx=280, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text='frame = pd.dataframe(np.random.randn(2, 4),', typ='text'),\n",
       " LectureElement(idx=281, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text=\"index=pd.date_range('1/1/2000', periods=2,\", typ='text'),\n",
       " LectureElement(idx=282, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text=\"freq='w-wed'),\", typ='text'),\n",
       " LectureElement(idx=283, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text=\"columns=['colorado', 'texas', 'new york', 'ohio'])\", typ='text'),\n",
       " LectureElement(idx=284, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text='frame', typ='text'),\n",
       " LectureElement(idx=287, id_path='2.8.2', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Upsampling and Interpolation'], text=\"frame.resample('w-thu').ffill()\", typ='text'),\n",
       " LectureElement(idx=290, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text='frame = pd.dataframe(np.random.randn(24, 4),', typ='text'),\n",
       " LectureElement(idx=291, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"index=pd.period_range('1-2000', '12-2001',\", typ='text'),\n",
       " LectureElement(idx=292, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"freq='m'),\", typ='text'),\n",
       " LectureElement(idx=293, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"columns=['colorado', 'texas', 'new york', 'ohio'])\", typ='text'),\n",
       " LectureElement(idx=294, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text='frame[:5]', typ='text'),\n",
       " LectureElement(idx=295, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"annual_frame = frame.resample('a-dec').mean()\", typ='text'),\n",
       " LectureElement(idx=296, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text='annual_frame', typ='text'),\n",
       " LectureElement(idx=299, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text='q-dec: quarterly, year ending in december', typ='text'),\n",
       " LectureElement(idx=300, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"annual_frame.resample('q-dec').ffill()\", typ='text'),\n",
       " LectureElement(idx=301, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"annual_frame.resample('q-dec', convention='end').ffill()\", typ='text'),\n",
       " LectureElement(idx=304, id_path='2.8.3', section_path=['__ROOT__', 'Time Series', 'Resampling and Frequency Conversion', 'Resampling with Periods'], text=\"annual_frame.resample('q-mar').ffill()\", typ='text')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = parse_lecture_elements(lecture_json_path)\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "151dd8a3-6543-4098-8173-5e466fcc263b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1758492502140,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "151dd8a3-6543-4098-8173-5e466fcc263b",
    "outputId": "d201c277-eda5-4cda-e10e-038693c54243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n"
     ]
    }
   ],
   "source": [
    "# Build embeddings (fit once on lecture + KG text to align spaces for TF-IDF fallback)\n",
    "embedder = Embedder()\n",
    "lecture_texts = [e.text for e in elements]\n",
    "E_L = embedder.fit_transform(lecture_texts)\n",
    "D_L, D_chron, D_logic, D_semL = build_lecture_distance(\n",
    "    elements, E_L, hp.alpha_chron, hp.alpha_logic, hp.alpha_sem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31395fee-df6e-4212-8a22-4dc1e27f83a1",
   "metadata": {},
   "source": [
    "## Refinement Starts Here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1d976e-6fa2-4e20-a316-f57aa7c078da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_history = {} # store the individual knowledge graph at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "255877a1-b14c-4eba-897f-0b1f712dc917",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758492509499,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "255877a1-b14c-4eba-897f-0b1f712dc917",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "#kg = read_json(kg_json_path)\n",
    "kg = read_kg(kg_json_path)\n",
    "\n",
    "if \"allowed_relations\" not in kg.get(\"meta\", {}):\n",
    "    if \"meta\" not in kg:\n",
    "        kg[\"meta\"] = {}\n",
    "    kg[\"meta\"][\"allowed_relations\"] = _ALLOWED_RELATIONS_DEFAULT.copy()\n",
    "\n",
    "allowed_relations = _ALLOWED_RELATIONS_DEFAULT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a0317a2-0e1b-4fe9-a884-80222c81036f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1758492511086,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9a0317a2-0e1b-4fe9-a884-80222c81036f",
    "outputId": "220427ff-18e9-4da6-ddfc-ffece399d6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kg[\"nodes\"]), len(kg['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55575bf0-21c3-4853-8c78-1389a6388b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "kg_history[0] = deepcopy(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22ef79fd-b315-4f95-b9d1-cbb165edfbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kg_history[0]['nodes']), len(kg_history[0]['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "288091d6-a4a7-4480-a3bc-21ddeecb1c8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1758492512874,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "288091d6-a4a7-4480-a3bc-21ddeecb1c8e",
    "outputId": "9dec460d-c363-4089-b807-45fd5f98e6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n"
     ]
    }
   ],
   "source": [
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "\n",
    "# Measures\n",
    "mu = normalized_measure(len(elements))\n",
    "nu = degree_centrality_measure(kg, node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11e34a1f-264b-4c51-b36f-15d647301d9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758492513915,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "11e34a1f-264b-4c51-b36f-15d647301d9e",
    "outputId": "bd7ca965-b67b-4335-843e-2db694589ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n"
     ]
    }
   ],
   "source": [
    "# Initial coupling and distance\n",
    "P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7fbfe1f-2053-444c-a6d9-d2388ed881f6",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1758492515233,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "b7fbfe1f-2053-444c-a6d9-d2388ed881f6"
   },
   "outputs": [],
   "source": [
    "best_KG = json.loads(json.dumps(kg))\n",
    "best_P = P.copy()\n",
    "best_fgw = fgw_total\n",
    "best_L = compute_objective(kg, hp.beta, best_fgw)\n",
    "\n",
    "history = [{\n",
    "    \"iter\": 0,\n",
    "    \"rate\": rate_complexity(kg),\n",
    "    \"fgw_total\": best_fgw,\n",
    "    \"fgw_structure\": struct_term,\n",
    "    \"fgw_feature\": feat_term,\n",
    "    \"objective\": best_L,\n",
    "    \"ops\": {\"add\":0,\"split\":0,\"merge\":0,\"edge_add\":0,\"edge_remove\":0}\n",
    "}]\n",
    "\n",
    "ops_counters = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5db80d4e-02ff-489f-afa4-6b2d8fdd555a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1758492518778,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5db80d4e-02ff-489f-afa4-6b2d8fdd555a",
    "outputId": "75db6b07-cc0f-4081-d5a5-bb98c4de40c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'iter': 0,\n",
       "  'rate': 7.0,\n",
       "  'fgw_total': 0.6135204083965918,\n",
       "  'fgw_structure': 0.13721822574533182,\n",
       "  'fgw_feature': 1.3279736823734816,\n",
       "  'objective': 68.35204083965918,\n",
       "  'ops': {'add': 0, 'split': 0, 'merge': 0, 'edge_add': 0, 'edge_remove': 0}}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5701a1ea-9cef-4ddf-8975-d372956cc3a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241408,
     "status": "ok",
     "timestamp": 1758492761466,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5701a1ea-9cef-4ddf-8975-d372956cc3a9",
    "outputId": "bb094cf6-1a81-4944-8601-ad4813ed75e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▌                                                                                                        | 1/10 [00:14<02:11, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 1: nodes:12, edges:16\n",
      "Reassign best values.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.8: The `tz_localize` method is a method associated with datetime objects, and \"UTC timestamp localization\" likely operates on or produces such objects..\n",
      "Warning: Invalid target_id or relation from LLM: Time Zone Handling: isA: 0.9: Localizing a timestamp to UTC is a specific instance of handling time zones, making it a sub-concept or a direct application of time zone handling..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: 10: partOf: 0.7: \"Date Range Generation\" is a component or functionality within \"Pandas time series data manipulation\", specifically dealing with the creation of date ranges..\n",
      "Warning: Invalid target_id or relation from LLM: 11: relatedTo: 0.6: \"Date Range Generation\" is a fundamental operation in \"Pandas Time Series Resampling\" as it's often used to create the index for time series data that will be resampled..\n",
      "Warning: Invalid target_id or relation from LLM: 6: relatedTo: 0.5: The definition of \"Date Range Generation\" involves textual representation of dates, making it related to a \"Textual Statement\"..\n",
      "Warning: Invalid target_id or relation from LLM: 8: relatedTo: 0.5: \"Date Range Generation\" is closely related to \"Timestamp creation\" as it generates a sequence of timestamps..\n",
      "Warning: Invalid target_id or relation from LLM: 9: propertyOf: 0.5: \"Date Range Generation\" can be considered a method or property related to \"Current Date and Time Information\" as it generates a sequence of dates..\n",
      "Warning: Invalid target_id or relation from LLM: 10: relatedTo: 0.4: \"Date Range Generation\" can be used in conjunction with \"TimePeriodConversion\" to convert generated date ranges into period objects..\n",
      "Warning: Invalid target_id or relation from LLM: 7: relatedTo: 0.3: While not a direct relationship, \"Date Range Generation\" could potentially be used in contexts involving \"Patient\" data if the timestamps relate to patient records..\n",
      "Warning: Invalid target_id or relation from LLM: 12: relatedTo: 0.3: \"Date Range Generation\" can be related to \"tzinfo\" as date ranges are often interpreted within specific time zones..\n",
      "Warning: Invalid target_id or relation from LLM: 13: relatedTo: 0.3: \"Date Range Generation\" is related to \"UTC timestamp localization\" if the generated date range needs to be localized to UTC..\n",
      "Warning: Invalid target_id or relation from LLM: 12: relatedTo: 0.2: \"date collection\" is related to \"Date Range Generation\" as it represents a collection of dates, which is what date range generation produces..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▏                                                                                            | 2/10 [00:30<02:01, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 2: nodes:19, edges:33\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: DatetimeIndexGenerationParameters: appliedTo: 0.8: The parameters for generating a datetime index would directly apply to the creation of time series data..\n",
      "Warning: Invalid target_id or relation from LLM: DatetimeIndexGenerationParameters: appliedTo: 0.6: The parameters for generating a datetime index would be applied to the process of creating that index..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: \"Pandas_Time_Series_Resampling\": \"equivalentTo\": 0.9: The new concept \"Daily Resampling of Time Series Data\" with the definition \"ts.resample('d')\" is a specific instance of the broader concept \"Pandas Time Series Resampling\" which includes examples like `resampler = ts.resample('d')`..\n",
      "Warning: Invalid target_id or relation from LLM: \"Pandas_time_series_data_manipulation\": \"uses\": 0.8: The operation \"Daily Resampling of Time Series Data\" (ts.resample('d')) is a method applied to time series data, and thus relies on the broader functionalities provided by \"Pandas time series data manipulation\"..\n",
      "Warning: Invalid target_id or relation from LLM: \"Time Series Data\": \"partOf\": 0.7: \"Daily Resampling of Time Series Data\" is an operation performed on \"Time Series Data\", making the latter a component of the former's context..\n",
      "Warning: Invalid target_id or relation from LLM: \"Datetime\": \"relatedTo\": 0.6: Resampling time series data daily implies operations that involve discrete daily intervals, which are fundamentally related to the concept of \"Datetime\"..\n",
      "Warning: Invalid target_id or relation from LLM: \"Timespan\": \"uses\": 0.6: Daily resampling inherently involves grouping data into daily \"Timespan\"s..\n",
      "Warning: Invalid target_id or relation from LLM: \"Date_Range_Generation\": \"relatedTo\": 0.5: Generating date ranges is often a prerequisite or a related operation to time series resampling, as it defines the intervals over which resampling can occur..\n",
      "Warning: Invalid target_id or relation from LLM: \"Pandas Series of Random Normal Data with Datetime Index\": \"exampleOf\": 0.5: The new concept is a method that would be applied to data structures like a \"Pandas Series of Random Normal Data with Datetime Index\"..\n",
      "Warning: Invalid target_id or relation from LLM: \"Date and Time Operations\": \"relatedTo\": 0.5: Daily resampling involves manipulating and grouping data based on dates, which falls under general \"Date and Time Operations\"..\n",
      "Warning: Invalid target_id or relation from LLM: \"Date_range\": \"relatedTo\": 0.4: The concept of a \"Date range\" is closely associated with time series data and operations like resampling..\n",
      "Warning: Invalid target_id or relation from LLM: \"date_representation\": \"relatedTo\": 0.4: Resampling to a daily frequency implies a specific \"date_representation\" or grouping based on days..\n",
      "Warning: Invalid target_id or relation from LLM: \"date_collection\": \"relatedTo\": 0.4: Daily resampling groups data by day, which relates to the collection of dates..\n",
      "Warning: Invalid target_id or relation from LLM: \"Timespan\": \"isA\": 0.3: While \"Timespan\" can refer to various periods, a \"daily resampling\" can be seen as a specific type of timespan operation..\n",
      "Warning: Invalid target_id or relation from LLM: \"DatetimeIndexGenerationParameters\": \"relatedTo\": 0.3: The frequency parameter ('d' for daily) in resampling is a parameter similar to those used in \"DatetimeIndexGenerationParameters\"..\n",
      "Warning: Invalid target_id or relation from LLM: \"Weekly_Wednesday_Frequency\": \"contrastsWith\": 0.3: \"Daily Resampling\" contrasts with other specific frequencies like \"Weekly Wednesday Frequency\" by defining a different interval..\n",
      "Warning: Invalid target_id or relation from LLM: \"Timestamp creation\": \"relatedTo\": 0.3: While not a direct dependency, the creation of timestamps is fundamental to the time series data that will be resampled..\n",
      "Warning: Invalid target_id or relation from LLM: \"TimePeriodConversion\": \"relatedTo\": 0.3: Resampling can be seen as a form of aggregating or converting data across time periods..\n",
      "Warning: Invalid target_id or relation from LLM: \"UTC timestamp localization\": \"relatedTo\": 0.2: Time zone handling can be relevant if the time series data has time zone information, and resampling needs to be aware of it, although not directly implied by 'd'..\n",
      "Warning: Invalid target_id or relation from LLM: \"Textual Statement\": \"relatedTo\": 0.1: The definition \"ts.resample('d')\" is a textual statement representing the operation, but the relation is weak..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.7: DataFrames, especially those dealing with time series, frequently utilize datetime objects or indices that are composed of datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: appliesTo: 0.8: The creation of a Pandas DataFrame with random data and a specified structure is a fundamental step in preparing and manipulating time series data..\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████▊                                                                                 | 3/10 [00:53<02:11, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KG sizes at iteration 3: nodes:26, edges:48\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:isA: 0.8: The phrase \"Non-Unique Timestamp Data Aggregation\" directly describes a method or technique for handling time series data, suggesting it's a specific type or approach within that broader domain..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:partOf: 0.7: Aggregating data with non-unique timestamps is a specific operation that is likely part of a larger process or toolkit for handling time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:uses: 0.7: The definition \"one way\" implies that this aggregation method uses or relies on underlying data structures or functions to achieve its goal, which would be related to time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.8: The concept of aggregating data with non-unique timestamps is inherently related to the manipulation and analysis of time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.7: The concept of aggregating data with non-unique timestamps is related to the handling of Datetime objects, as timestamps are a form of datetime..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.6: Aggregating data by timestamps, especially when they are not unique, is a fundamental operation in time series analysis, which Pandas is a primary tool for..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.7: Aggregating data with non-unique timestamps is a specific case that would fall under the general umbrella of time series resampling..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:uses: 0.6: The aggregation process itself requires dealing with a collection of dates, which is captured by the concept of \"date collection.\".\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.6: Aggregating data with non-unique timestamps is a specific task that might involve generating or working with date ranges..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.6: Aggregating data with non-unique timestamps is a task that would involve generating a datetime index, which is a core component of time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.6: Aggregating time series data, especially when timestamps are not unique, often involves a resampling operation..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: The creation of Pandas Series with data and a datetime index is a foundational step for any time series manipulation, including aggregation..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: Aggregating data with non-unique timestamps is a specific type of resampling operation performed on time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: Aggregating data with non-unique timestamps is a task related to the creation of Pandas DataFrames, which often contain time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: Aggregating data with non-unique timestamps is a task related to the generation of date ranges, which are fundamental to time series..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: Aggregating data with non-unique timestamps is a task related to the generation of date ranges using Pandas..\n",
      "Warning: Invalid target_id or relation from LLM: Non-Unique Timestamp Data Aggregation:relatedTo: 0.5: Aggregating data with non-unique timestamps might involve understanding or manipulating the hour component of a timestamp..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: [Datetime]: [relatedTo]: 0.8: The new concept \"Timedelta Calculation\" involves subtracting two datetime objects, directly utilizing the functionality of the \"Datetime\" class..\n",
      "Warning: Invalid target_id or relation from LLM: [Date_and_Time_Operations]: [uses]: 0.9: The definition of \"Timedelta Calculation\" (\"datetime.now() - datetime(2024, 4, 20)\") is a direct example of a \"Date and Time Operation.\".\n",
      "Warning: Invalid target_id or relation from LLM: [Timespan]: [partOf]: 0.7: A timedelta calculation, which represents a duration between two points in time, can be considered a type of \"Timespan\" or a component that defines a timespan..\n",
      "Warning: Invalid target_id or relation from LLM: [Pandas_time_series_data_manipulation]: [relatedTo]: 0.6: Timedelta calculations are fundamental operations within time series data manipulation, often used to determine intervals or durations between data points..\n",
      "Warning: Invalid target_id or relation from LLM: [Date_range]: [relatedTo]: 0.6: While not directly creating a date range, timedelta calculations are often used in conjunction with date ranges to determine intervals or future/past dates..\n",
      "Warning: Invalid target_id or relation from LLM: [TimePeriodConversion]: [relatedTo]: 0.7: The calculation of a timedelta often precedes or is related to operations involving the conversion of time periods. The example definition for TimePeriodConversion also shows delta.days and delta.seconds which are attributes of timedeltas..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: isA: 0.7: The `pd.date_range` function generates a sequence of datetime objects. Therefore, \"DateRangeGeneration\" is a way to create a collection of datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.6: \"DateRangeGeneration\" is defined using `pd.date_range`, which internally relies on and manipulates datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: isA: 0.8: The output of `pd.date_range` is typically used as an index for time series data. Thus, generating a date range is a fundamental step in creating or working with time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas Series with Random Normal Data and Datetime Index: uses: 0.7: The definition of \"DateRangeGeneration\" produces a `rng` object, which is then used as an index for creating a pandas Series in the related concept \"Pandas Series with Random Normal Data and Datetime Index\"..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas Series of Random Normal Data with Datetime Index: uses: 0.7: Similar to the above, `pd.date_range` generates the index for this type of pandas Series..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: isA: 0.6: The generated date range represents a sequence of timespans..\n",
      "Warning: Invalid target_id or relation from LLM: Timestamp creation: relatedTo: 0.5: `pd.date_range` generates a sequence of timestamps, so it's related to the concept of creating individual timestamps..\n",
      "Warning: Invalid target_id or relation from LLM: Date collection: relatedTo: 0.5: The output of `pd.date_range` is a collection of dates..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas DataFrame Creation: uses: 0.4: A generated date range can serve as the index for a pandas DataFrame..\n",
      "Warning: Invalid target_id or relation from LLM: Time PeriodConversion: relatedTo: 0.2: Date ranges can be converted to periods..\n",
      "Warning: Invalid target_id or relation from LLM: strftime operation: relatedTo: 0.2: Date ranges can be formatted using `strftime`..\n",
      "Warning: Invalid target_id or relation from LLM: Time Zone Handling: relatedTo: 0.2: Date range generation can be influenced by time zones if the start/end dates or frequencies are time-zone aware, but the provided definition doesn't highlight this..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████▍                                                                     | 4/10 [01:20<02:12, 22.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 4: nodes:33, edges:65\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: appliesTo: 0.85: The 'a-dec' in the resampling operation signifies an annual timespan, and the operation is applied to aggregate data over these annual periods..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.70: The resampling operation inherently deals with time-based data, which is based on datetimes..\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: relatedTo: 0.70: Annual frame resampling is a process applied to time series data to aggregate it over yearly periods..\n",
      "Warning: Invalid target_id or relation from LLM: Date Range Generation_2: relatedTo: 0.50: The concept of resampling with a frequency like 'a-dec' is closely related to generating date ranges with specific frequencies..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Datetime: 0.9: The definition of \"Datetime String Parsing\" explicitly shows a `datetime.strptime` call, which is a method of Python's `datetime` object for parsing strings into datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Date_and_Time_Operations: 0.7: Parsing a datetime string is a fundamental operation related to handling dates and times..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Pandas_time_series_data_manipulation: 0.6: Parsing datetime strings is a common prerequisite for creating or manipulating time series data in libraries like Pandas..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Date_and_Datetime_Manipulation: 0.6: Parsing a datetime string is a way to represent or create date and datetime objects, which are central to date and datetime manipulation..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: DatetimeIndexGenerationParameters: 0.5: The format string used in `strptime` (`'%y-%m-%d'`) is a parameter for specifying how a datetime string should be interpreted, which is a parameter for datetime index generation..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Time Series Data: 0.5: Datetime string parsing is often used to convert string representations of dates and times into a format suitable for time series data analysis..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Timestamp creation: 0.5: Parsing a datetime string is a method to create a timestamp or datetime object..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: datetime_arithmetic_operations: 0.4: While not directly arithmetic, parsing is often the first step before performing arithmetic operations on datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: strftime operation: 0.3: `strptime` is the inverse operation of `strftime`. `strftime` converts datetime objects to strings, while `strptime` converts strings to datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime String Parsing: Textual Statement: 0.2: A datetime string is a type of textual statement..\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████                                                          | 5/10 [01:40<01:46, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 5: nodes:39, edges:83\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.7: Hour and minute offsets are used in conjunction with datetime objects for time series manipulation..\n",
      "Warning: Invalid target_id or relation from LLM: Date and Datetime Manipulation: uses: 0.3: Hour and minute offsets are tools used in manipulating dates and datetimes..\n",
      "Warning: Invalid target_id or relation from LLM: Time Zone Handling: relatedTo: 0.2: Hour and minute are fundamental units within time zone handling..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.2: Hour and minute offsets define specific durations or spans of time..\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: relatedTo: 0.1: Offsets are used to define the temporal aspect of time series data..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: grouped mean: isA: 0.8: The term \"grouped mean\" is a statistical concept that is likely a specific type of mean calculation, which is a fundamental statistical operation..\n",
      "Warning: Invalid target_id or relation from LLM: grouped mean: partOf: 0.6: A grouped mean is a component of a larger dataset or a calculation within a broader statistical analysis..\n",
      "Warning: Invalid target_id or relation from LLM: grouped mean: relatedTo: 0.7: The concept of \"grouped mean\" is closely related to other statistical operations and data aggregation techniques..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 6/10 [02:05<01:30, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 6: nodes:46, edges:113\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: appliesTo: 0.7: The operation of resampling and forward filling is applied to time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: appliesTo: 0.6: Resampling to quarterly intervals implies working with specific timespans..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.5: Forward fill is a method that operates on sequences, implicitly involving datetime objects or indices..\n",
      "Warning: Invalid target_id or relation from LLM: Date Range Generation_2: relatedTo: 0.4: The resampling frequency ('q-mar') is similar to frequencies used in date range generation..\n",
      "Warning: Invalid target_id or relation from LLM: Timestamp creation: relatedTo: 0.3: The process of creating timestamps is fundamental to time series data that would be resampled..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: relatedTo: 0.4: Creating a Pandas DataFrame is a common first step when working with \"Time Series Data\" in Python, especially when the data has temporal ordering..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.3: While not directly used in the provided snippet for \"Pandas DataFrame Creation\", the creation of time series data often involves \"Datetime\" objects for indexing..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.1: \"Timespan\" represents periods in time series data. Creating a DataFrame to hold such data would be related to \"Timespan\"..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: produces: 0.4: While not directly produced by the snippet, the creation of a DataFrame with a datetime index would involve or produce datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: relatedTo: 0.7: Creating a Pandas DataFrame is a fundamental step in preparing and structuring \"Time Series Data\" for analysis..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.4: DataFrames intended for time series data often use \"Datetime\" objects as part of their index..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.1: Creating a DataFrame to hold time series data is related to the concept of \"Timespan\"..\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: relatedTo: 0.6: Creating a Pandas DataFrame is a common and necessary step when working with \"Time Series Data\"..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.5: The creation of a DataFrame often involves using \"Datetime\" objects, particularly when defining its index for time series..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.1: Creating a DataFrame to hold time series data is related to the concept of \"Timespan\"..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: isA: 0.9: The concept \"Time Series Interval\" describes a rule for the frequency of data points in time series, making it a fundamental aspect or characteristic of \"Time Series Data\"..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.7: Time series intervals are fundamentally related to specifying periods and frequencies within datetime objects or ranges..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.8: A \"Time Series Interval\" directly defines the duration or frequency of a \"Timespan\" within time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Date Range Generation: relatedTo: 0.7: Similar to fixed-frequency date range generation, specifying intervals is key to generating date ranges..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: relatedTo: 0.5: \"DeltaSeconds\" is a unit of time that can be relevant to time series data, which is indexed in time order..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas Time Series Resampling: relatedTo: 0.5: Resampling time series data often involves specifying time intervals, and \"DeltaSeconds\" could be a unit used for this..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.5: A timespan can be represented by a duration, which can be broken down into seconds..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.5: \"DeltaSeconds\" is a component of time differences (timedeltas) that are derived from datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: strftime operation: relatedTo: 0.2: strftime formats output, it does not directly represent or use seconds as a concept itself, though seconds might be part of the formatted string..\n",
      "Warning: Invalid target_id or relation from LLM: Oven_placement: unrelated: 0.0: No clear connection between oven placement and seconds..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas_DataFrame_Creation: unrelated: 0.0: DataFrame creation is a general concept..\n",
      "Warning: Invalid target_id or relation from LLM: Pandas_DataFrame_Creation_2: unrelated: 0.0: DataFrame creation is a general concept..\n",
      "Warning: Invalid target_id or relation from LLM: Patient: unrelated: 0.0: No relation to patient data..\n",
      "Warning: Invalid target_id or relation from LLM: UTC_timestamp_localization: unrelated: 0.0: Localization is about assigning a time zone, not measuring time differences in seconds..\n",
      "Warning: Invalid target_id or relation from LLM: Timestamp_object: unrelated: 0.0: While related to time, 'delta.seconds' is a specific component of a timedelta, not the timestamp object itself..\n",
      "Warning: Invalid target_id or relation from LLM: LabelBasedData: unrelated: 0.0: Label-based data does not inherently relate to specific time deltas..\n",
      "Warning: Invalid target_id or relation from LLM: resampling_and_grouping_time_series_data: unrelated: 0.0: While related to time series, resampling and grouping do not directly imply a need for 'delta.seconds' as a concept..\n",
      "Warning: Invalid target_id or relation from LLM: annual_frame_resampling: unrelated: 0.0: Annual resampling is a high-level aggregation..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.7: Time zones are intrinsically linked to datetime objects, as they define the specific point in time a datetime object represents..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.5: While timespans are durations, understanding the context of time zones can influence how those durations are interpreted across different regions..\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 7/10 [02:40<01:20, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 7: nodes:52, edges:139\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: isA: 0.8: The \"Date String Representation\" is a specific way to represent datetime objects, suggesting it is a type of datetime representation..\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: relatedTo: 0.8: Time series data often uses string representations of dates for input or storage..\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: partOf: 0.7: Date string representations are often a component of time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.9: \"Date String Representation\" is a format for representing datetime values..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Date_Alignment_and_Period_Calculation: usedTo: 0.8: The definition involves `pd.period(2007, freq='a-dec')` and resampling operations, all of which are Pandas functionalities. Importing Pandas is a prerequisite..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Datetime: 0.95: The definition of \"datetime string formatting\" explicitly mentions formatting \"datetime objects\", and \"Datetime\" is a core concept related to this..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: strftime operation: 0.9: The definition mentions using the \"strftime method\", which is directly related to a \"strftime operation\"..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Timestamp object: 0.9: The definition mentions formatting \"pandas timestamp objects\", making it a direct relation..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_time_series_data_manipulation: 0.8: \"datetime string formatting\" is a specific operation within the broader context of manipulating time series data with Pandas..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Date_and_Datetime_Manipulation: 0.75: Formatting datetime objects is a form of date and datetime manipulation..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Datetime_String_Parsing: 0.5: While formatting is the inverse of parsing, they are closely related operations on datetime strings..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Date_String_Representation: 0.4: Formatting a datetime object results in a date string representation..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_Series_of_Random_Normal_Data_with_Datetime_Index: 0.3: Such series would likely require datetime formatting if the index needs to be represented as strings..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_Series_with_Random_Normal_Data_and_Datetime_Index: 0.3: Similar to the above, formatting might be needed for string representations..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Date Range Generation: 0.3: Date ranges are often represented as strings, and formatting would be involved..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_Date_Range: 0.3: Similar to Date Range Generation, formatting might be used for representing Pandas date ranges..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: DateRangeGeneration: 0.3: Similar to Date Range Generation, formatting might be used for representing DateRangeGeneration..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_Series_Creation_with_Random_Data_and_Time_Index: 0.3: Similar to other series creations with time indices, formatting could be relevant..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Pandas_DataFrame_Creation: 0.2: If a DataFrame has datetime columns, formatting might be applied..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Time Series Data: 0.2: Time series data often needs to be represented as strings, and formatting is the process for this..\n",
      "Warning: Invalid target_id or relation from LLM: datetime string formatting: Timestamp creation: 0.1: Formatting is a subsequent operation after timestamp creation..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 8/10 [03:07<00:53, 26.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 8: nodes:58, edges:176\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: 11: relatedTo: 0.6: The new node is about resampling time series daily. \"Pandas time series data manipulation\" is a broad category that would encompass such operations..\n",
      "Warning: Invalid target_id or relation from LLM: 22: uses: 0.9: \"Daily Resampling of Time Series\" is a direct application of \"Pandas Time Series Resampling\" as indicated by the term \"resample\" in the definition..\n",
      "Warning: Invalid target_id or relation from LLM: 23: partOf: 0.7: Resampling time series data, especially daily, can involve converting data into periods, and \"TimePeriodConversion\" is related to this..\n",
      "Warning: Invalid target_id or relation from LLM: 24: relatedTo: 0.5: Daily resampling involves collecting dates, so it is related to \"date collection\"..\n",
      "Warning: Invalid target_id or relation from LLM: 25: uses: 0.7: Daily resampling might involve dealing with timezones, and \"UTC timestamp localization\" is a related concept in time series handling..\n",
      "Warning: Invalid target_id or relation from LLM: 29: relatedTo: 0.5: Daily resampling involves a frequency, and \"Weekly Wednesday Frequency\" is an example of a specific frequency..\n",
      "Warning: Invalid target_id or relation from LLM: 31: relatedTo: 0.5: Daily resampling focuses on a daily frequency, which relates to the concept of \"current hour\" as a time unit..\n",
      "Warning: Invalid target_id or relation from LLM: 33: uses: 0.8: \"Daily Resampling of Time Series\" is a process that would utilize parameters for generating date ranges, such as those described in \"DatetimeIndexGenerationParameters\"..\n",
      "Warning: Invalid target_id or relation from LLM: 34: uses: 0.7: \"Daily Resampling of Time Series\" would likely be applied to data structures like a \"Pandas Series of Random Normal Data with Datetime Index\"..\n",
      "Warning: Invalid target_id or relation from LLM: 35: uses: 0.7: Similar to the above, \"Daily Resampling of Time Series\" would operate on data like \"Pandas Series with Random Normal Data and Datetime Index\"..\n",
      "Warning: Invalid target_id or relation from LLM: 38: uses: 0.9: \"Daily Resampling of Time Series\" is a specific instance or application of the broader concept of \"Pandas DataFrame Creation\", where resampling is applied to dataframes..\n",
      "Warning: Invalid target_id or relation from LLM: 39: uses: 0.8: Generating a daily frequency for resampling requires a mechanism for creating date ranges, making \"Date Range Generation\" relevant..\n",
      "Warning: Invalid target_id or relation from LLM: 40: uses: 0.8: \"Daily Resampling of Time Series\" relies on the ability to generate date ranges, which is the core function of \"Pandas Date Range\"..\n",
      "Warning: Invalid target_id or relation from LLM: 44: relatedTo: 0.5: Daily resampling is a form of marking time at regular intervals, making it related to \"Irregular Time Marking\"..\n",
      "Warning: Invalid target_id or relation from LLM: 46: relatedTo: 0.5: The concept of daily resampling, which involves discrete time steps, is related to \"Timedelta Calculation\" as it deals with time differences..\n",
      "Warning: Invalid target_id or relation from LLM: 47: uses: 0.8: \"Daily Resampling of Time Series\" is a process that involves generating date ranges, making \"DateRangeGeneration\" a relevant utility..\n",
      "Warning: Invalid target_id or relation from LLM: 49: uses: 0.7: \"Daily Resampling of Time Series\" would be applied to data that has a date index, like \"pandas Series with random data and date index\"..\n",
      "Warning: Invalid target_id or relation from LLM: 50: uses: 0.8: \"Daily Resampling of Time Series\" is a specific operation that falls under \"Date and Datetime Manipulation\"..\n",
      "Warning: Invalid target_id or relation from LLM: 51: uses: 0.7: \"Daily Resampling of Time Series\" would likely be applied to data structures like \"Pandas Series Creation with Random Data and Time Index\"..\n",
      "Warning: Invalid target_id or relation from LLM: 52: relatedTo: 0.6: \"Daily Resampling of Time Series\" is a form of creating a specific type of time series, hence related to \"Longer Time Series\"..\n",
      "Warning: Invalid target_id or relation from LLM: 53: uses: 0.9: \"Daily Resampling of Time Series\" is a direct implementation of \"Fixed-frequency date range generation\" as it aims to fix the frequency to daily..\n",
      "Warning: Invalid target_id or relation from LLM: 55: relatedTo: 0.5: Daily resampling is a specific form of resampling, and \"Regular interval data occurrence\" describes the characteristic of data that is often a subject of resampling..\n",
      "Warning: Invalid target_id or relation from LLM: 57: relatedTo: 0.5: Daily resampling involves operations on time, which is related to \"datetime arithmetic operations\"..\n",
      "Warning: Invalid target_id or relation from LLM: 58: uses: 0.7: \"Daily Resampling of Time Series\" might involve parsing date strings to create the time series, making \"Datetime String Parsing\" relevant..\n",
      "Warning: Invalid target_id or relation from LLM: 59: relatedTo: 0.6: The \"ts\" variable is a common placeholder for time series data, and daily resampling would operate on such a variable..\n",
      "Warning: Invalid target_id or relation from LLM: 60: relatedTo: 0.6: \"Daily Resampling of Time Series\" deals with specific points in time, which is related to \"Stamp\"..\n",
      "Warning: Invalid target_id or relation from LLM: 61: relatedTo: 0.6: A \"Timestamp object\" is a fundamental building block for time series data, and daily resampling operates on these..\n",
      "Warning: Invalid target_id or relation from LLM: 65: uses: 0.7: \"Daily Resampling of Time Series\" might use specific offsets for time operations, such as those provided by \"Pandas HourMinuteOffset\"..\n",
      "Warning: Invalid target_id or relation from LLM: 67: relatedTo: 0.5: While not a direct relationship, the concept of resampling can involve grouping data before aggregation, making it loosely related to \"grouped mean\"..\n",
      "Warning: Invalid target_id or relation from LLM: 69: relatedTo: 0.5: \"Daily Resampling of Time Series\" is a specific type of operation that falls under the broader category of \"resampling and grouping time series data\"..\n",
      "Warning: Invalid target_id or relation from LLM: 71: relatedTo: 0.5: Daily resampling can be performed on time series data that may have timezone information, making \"Time Zone Handling\" a related concept..\n",
      "Warning: Invalid target_id or relation from LLM: 72: relatedTo: 0.5: If time series data with different time zones are resampled daily, \"UTC Time Series Combination\" becomes relevant..\n",
      "Warning: Invalid target_id or relation from LLM: 73: relatedTo: 0.6: \"Daily Resampling of Time Series\" is a specific method of resampling that may involve filling data, which is related to \"Quarterly Resampling with Forward Fill\" as an example of a fill operation..\n",
      "Warning: Invalid target_id or relation from LLM: 74: relatedTo: 0.7: \"Daily Resampling of Time Series\" is fundamentally about defining and working with a \"Time Series Interval\" of one day..\n",
      "Warning: Invalid target_id or relation from LLM: 76: relatedTo: 0.5: \"Daily Resampling of Time Series\" operates on time intervals, which relates to \"DeltaSeconds\" as a measure of time duration..\n",
      "Warning: Invalid target_id or relation from LLM: 77: relatedTo: 0.5: \"Daily Resampling of Time Series\" involves a specific frequency (daily), and \"CommonTimeZoneList\" can be relevant if timezone-aware resampling is performed..\n",
      "Warning: Invalid target_id or relation from LLM: 80: uses: 0.9: \"Daily Resampling of Time Series\" is a direct application of \"Frequency Conversion of Time Series Data\" where the target frequency is daily..\n",
      "Warning: Invalid target_id or relation from LLM: 81: relatedTo: 0.5: Daily resampling is a process that aligns data to a daily frequency, relating it to \"Date Alignment and Period Calculation\"..\n",
      "Warning: Invalid target_id or relation from LLM: 82: relatedTo: 0.5: \"Daily Resampling of Time Series\" involves specific points in time and their attributes, which is related to \"Current_Date_and_Time_Offset\"..\n",
      "Warning: Invalid target_id or relation from LLM: 83: relatedTo: 0.5: \"Daily Resampling of Time Series\" relies on the underlying date and time representations, making \"Date String Representation\" relevant..\n",
      "Warning: Invalid target_id or relation from LLM: 84: relatedTo: 0.5: \"Daily Resampling of Time Series\" deals with time units, and \"Current Hour\" is a component of time..\n",
      "Warning: Invalid target_id or relation from LLM: 85: uses: 0.8: \"Daily Resampling of Time Series\" involves formatting and manipulating dates as strings, hence using \"datetime string formatting\"..\n",
      "Warning: Invalid target_id or relation from LLM: 86: relatedTo: 0.5: \"Daily Resampling of Time Series\" aims for a daily frequency, and \"q-dec\" (quarterly, year ending in December) is another type of time frequency..\n",
      "Warning: Invalid target_id or relation from LLM: 88: relatedTo: 0.6: \"Daily Resampling of Time Series\" is a specific operation within the broader field of \"time series data manipulation\"..\n",
      "Warning: Invalid target_id or relation from LLM: 89: uses: 0.9: \"Daily Resampling of Time Series\" is a specific task that utilizes \"Pandas Datetime Object Handling\" as it deals with datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: 90: uses: 0.8: \"Daily Resampling of Time Series\" involves the manipulation of dates, making \"Date Formatting\" a relevant operation..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.9: The definition of \"Wednesday Frequency\" (\"freq='w-wed'),\") directly relates to specifying a frequency for time-based operations, which are fundamental to datetime objects..\n",
      "Warning: Invalid target_id or relation from LLM: Time_Series_Data: relatedTo: 0.5: \"Wednesday Frequency\" is a characteristic or property related to how time series data is structured or sampled..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime_object_handling: uses: 0.9: The `datetime` module provides the core `datetime` objects that pandas manipulates..\n",
      "Warning: Invalid target_id or relation from LLM: 2: relatedTo: 0.9: The new node \"datetime module\" is a direct instantiation and representation of the Python standard library's \"datetime\" module, making it a synonym or direct equivalent in purpose..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: relatedTo: 0.8: The new concept \"Timestamp relative to experiment or elapsed time\" directly relates to \"Time Series Data\" as timestamps are fundamental to ordering and indexing time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: relatedTo: 0.7: \"Timestamp relative to experiment or elapsed time\" is a specific type of datetime representation, making it related..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: relatedTo: 0.6: While distinct, elapsed time in a timestamp can define a timespan, hence a relatedness..\n",
      "Warning: Invalid target_id or relation from LLM: Timestamp creation: relatedTo: 0.9: The definition of the new concept implies the creation of timestamps, making it highly related..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Data: isA: 0.9: \"Pandas Period Range\" is a specific type of index used for time series data..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: isA: 0.7: A \"Pandas Period Range\" represents a sequence of timespans..\n",
      "Warning: Invalid target_id or relation from LLM: Date Range Generation: isA: 0.9: \"Pandas Period Range\" is a specific type of date range generation..\n",
      "Warning: Invalid target_id or relation from LLM: Time Series Interval: relatedTo: 0.7: A \"Pandas Period Range\" defines a sequence of time intervals..\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 9/10 [03:35<00:26, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 9: nodes:64, edges:217\n",
      "best values remain.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: Datetime: uses: 0.7: \"Time Series Interval Rule\" likely uses \"Datetime\" objects to define intervals..\n",
      "Warning: Invalid target_id or relation from LLM: Timespan: isA: 0.7: A \"Time Series Interval Rule\" defines or specifies a \"Timespan\"..\n",
      "Warning: Invalid target_id or relation from LLM: Irregular_Time_Marking: definitionally relatedTo: 0.5: While the rule implies regularity, it's a way to *mark* time, contrasting with truly irregular marking..\n",
      "Warning: Invalid target_id or relation from LLM: Date Alignment and Period Calculation: relatedTo: 0.5: The rule can influence how data is aligned by date or period..\n",
      "Use LLM to name new concept.\n",
      "Use LLM to add new edges for a new concept.\n",
      "Warning: Invalid target_id or relation from LLM: 6834532839: relatedTo: 0.5: The example provided in the new node definition `datetime.strptime(value, '%y-%m-%d')` is a direct example of a datetime string parsing operation..\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use LLM to name new concept.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n",
      "Use SentenceTransformer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [04:00<00:00, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use POT.\n",
      "The KG sizes at iteration 10: nodes:69, edges:245\n",
      "best values remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterative refinement\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "for it in tqdm(range(1, hp.max_iterations + 1)):\n",
    "    improved = False\n",
    "    local_ops = collections.Counter()\n",
    "\n",
    "    # --- Operation A: Add new concepts for under-represented content\n",
    "    # mass over lecture elements\n",
    "    row_mass = P.sum(axis=1)  # size N (lecture)\n",
    "    add_candidates = np.where(row_mass < hp.theta_add)[0].tolist()\n",
    "    random.shuffle(add_candidates)\n",
    "    added_in_this_iter = 0\n",
    "    added_edge_in_this_iter = 0\n",
    "    for i in add_candidates[:5]:  # cap additions per iter\n",
    "        # connect to nearest existing concept by semantic similarity\n",
    "        v = E_L[i]\n",
    "        sims = E_K @ v\n",
    "        j = int(np.argmax(sims))\n",
    "        connect_to = node_ids[j]\n",
    "        added_node, added_edges = add_concept_from_element(\n",
    "            kg, elements[i], embedder, allowed_relations,\n",
    "            connect_to=connect_to,\n",
    "            relation=\"relatedTo\"\n",
    "        )\n",
    "\n",
    "        if added_node:\n",
    "            added_in_this_iter += 1\n",
    "        if added_edges:\n",
    "            added_edge_in_this_iter += len(added_edges)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    local_ops[\"add\"] += added_in_this_iter\n",
    "    local_ops[\"edge_add\"] += added_edge_in_this_iter\n",
    "\n",
    "    # Rebuild KG side after additions (embeddings, distances, measures)\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "    # --- Operation B: Split concepts with high coupling entropy\n",
    "    # Recompute coupling to assess splits\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    cols_entropy = [coupling_entropy(P[:, j]) for j in range(P.shape[1])]\n",
    "    split_targets = [j for j, H in enumerate(cols_entropy) if H > hp.theta_split]\n",
    "    random.shuffle(split_targets)\n",
    "    splits_done = 0\n",
    "    for j in split_targets[:2]:  # cap splits per iter\n",
    "        A, B, a_idx, b_idx = kmeans_split_concept(P[:, j], E_L, elements, kg, j)\n",
    "        if A is None:\n",
    "            continue\n",
    "        # Commit split: remove old node j and add A,B\n",
    "        old_id = kg[\"nodes\"][j][\"id\"]\n",
    "        # Add A, B\n",
    "        kg[\"nodes\"].append(dataclasses.asdict(A))\n",
    "        kg[\"nodes\"].append(dataclasses.asdict(B))\n",
    "        # Rewire edges connected to old_id: duplicate to A and B\n",
    "        new_edges = []\n",
    "        for e in kg[\"edges\"]:\n",
    "            if e[\"source\"] == old_id:\n",
    "                for nid in [A.id, B.id]:\n",
    "                    new_e = dict(e); new_e[\"source\"] = nid; new_edges.append(new_e)\n",
    "            elif e[\"target\"] == old_id:\n",
    "                for nid in [A.id, B.id]:\n",
    "                    new_e = dict(e); new_e[\"target\"] = nid; new_edges.append(new_e)\n",
    "            else:\n",
    "                new_edges.append(e)\n",
    "        kg[\"edges\"] = new_edges\n",
    "        # Remove old node\n",
    "        del kg[\"nodes\"][j]\n",
    "        splits_done += 1\n",
    "    local_ops[\"split\"] += splits_done\n",
    "\n",
    "    # --- Operation C: Merge redundant concept pairs\n",
    "    # Rebuild KG matrices after splits\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    to_merge = []\n",
    "    m = len(node_ids)\n",
    "    for i in range(m):\n",
    "        for j in range(i+1, m):\n",
    "            if merge_if_redundant(i, j, P, E_K, sim_thresh=0.92, kl_thresh=hp.theta_merge):\n",
    "                to_merge.append((i, j))\n",
    "    # Commit merges (greedy, no transitive handling beyond sequential)\n",
    "    merges_done = 0\n",
    "    merged_ids = set()\n",
    "    for (i, j) in to_merge[:2]:  # cap merges per iter\n",
    "        if i in merged_ids or j in merged_ids:\n",
    "            continue\n",
    "        id_i = kg[\"nodes\"][i][\"id\"]; id_j = kg[\"nodes\"][j][\"id\"]\n",
    "        # Merge j into i: keep i's label/def; absorb aliases\n",
    "        ni = kg[\"nodes\"][i]; nj = kg[\"nodes\"][j]\n",
    "        ali = set(ni.get(\"aliases\", []) + [nj.get(\"label\", \"\")] + nj.get(\"aliases\", []))\n",
    "        ni[\"aliases\"] = sorted([a for a in ali if a])\n",
    "        # Rewire edges from j to i\n",
    "        for e in kg[\"edges\"]:\n",
    "            if e[\"source\"] == id_j: e[\"source\"] = id_i\n",
    "            if e[\"target\"] == id_j: e[\"target\"] = id_i\n",
    "        # Remove node j\n",
    "        del kg[\"nodes\"][j]\n",
    "        merges_done += 1\n",
    "        merged_ids.add(i); merged_ids.add(j)\n",
    "    local_ops[\"merge\"] += merges_done\n",
    "\n",
    "    # --- Operation D: Relationship updates (add/rewire/remove)\n",
    "    # Add edges based on coupling + D_L proximity\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "\n",
    "    # Add relations where coupled lecture elements are near\n",
    "    edge_adds = 0\n",
    "    N, M = P.shape\n",
    "    # For each concept j, find top lecture elements and propose relations with \n",
    "    # other concepts k\n",
    "    for j in range(M):\n",
    "        top_i = np.argsort(-P[:, j])[:5]\n",
    "        for k in range(M):\n",
    "            if j == k:\n",
    "                continue\n",
    "            # If top elements for j are close to top elements for k in lecture space, \n",
    "            # relate\n",
    "            top_k = np.argsort(-P[:, k])[:5]\n",
    "            # Compute average cross-distance\n",
    "            pairs = list(itertools.product(top_i, top_k))\n",
    "            if not pairs:\n",
    "                continue\n",
    "            avg_d = float(np.mean([D_L[i1, i2] for i1, i2 in pairs]))\n",
    "            if avg_d < hp.theta_relate:\n",
    "                a = kg[\"nodes\"][j][\"id\"]; b = kg[\"nodes\"][k][\"id\"]\n",
    "                sig = (a, b, \"relatedTo\")\n",
    "                if not any((e[\"source\"], e[\"target\"], e[\"relation\"]) == sig for e in kg[\"edges\"]):\n",
    "                    # get edge definition\n",
    "                    e_def = get_edge_definition(kg, a, b, \"relatedTo\")\n",
    "\n",
    "                    e_rationale = get_edge_rationale(kg, a, b, \"relatedTo\")\n",
    "                    \n",
    "                    kg[\"edges\"].append({\n",
    "                        \"id\": f\"rel_{a}_{b}\",\n",
    "                        \"source\": a,\n",
    "                        \"target\": b,\n",
    "                        \"relation\": \"relatedTo\",\n",
    "                        \"definition\": e_def,\n",
    "                        \"provenance\": [],\n",
    "                        \"confidence\": 0.6,\n",
    "                        \"rationale\": e_rationale\n",
    "                    })\n",
    "                    edge_adds += 1\n",
    "    local_ops[\"edge_add\"] += edge_adds\n",
    "\n",
    "    # Remove weak edges not supported by coupling\n",
    "    edge_removes = 0\n",
    "    keep_edges = []\n",
    "    for e in kg[\"edges\"]:\n",
    "        try:\n",
    "            a = node_ids.index(e[\"source\"])\n",
    "            b = node_ids.index(e[\"target\"])\n",
    "        except ValueError:\n",
    "            # Node removed; drop edge\n",
    "            edge_removes += 1\n",
    "            continue\n",
    "        # Support measure ~ product of marginals around a,b\n",
    "        supp = float(P[:, a].sum() * P[:, b].sum())\n",
    "        if supp < 1e-4:\n",
    "            edge_removes += 1\n",
    "            continue\n",
    "        keep_edges.append(e)\n",
    "    kg[\"edges\"] = keep_edges\n",
    "    local_ops[\"edge_remove\"] += edge_removes\n",
    "\n",
    "    # Domain rules (optional)\n",
    "    #if not disable_domain_rules:\n",
    "    #    added = apply_domain_rules(kg, allowed_relations)\n",
    "    #    local_ops[\"edge_add\"] += len(added)\n",
    "\n",
    "    # Evaluate objective\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    r = rate_complexity(kg)\n",
    "    L = r + hp.beta * fgw_total\n",
    "\n",
    "    history.append({\n",
    "        \"iter\": it,\n",
    "        \"rate\": r,\n",
    "        \"fgw_total\": fgw_total,\n",
    "        \"fgw_structure\": struct_term,\n",
    "        \"fgw_feature\": feat_term,\n",
    "        \"objective\": L,\n",
    "        \"ops\": dict(local_ops)\n",
    "    })\n",
    "\n",
    "    kg_history[it] = deepcopy(kg)\n",
    "    print(\"The KG sizes at iteration {}: nodes:{}, edges:{}\".format(it, \n",
    "                                                                    len(kg_history[it]['nodes']),\n",
    "                                                                    len(kg_history[it]['edges'])))\n",
    "\n",
    "    if L + 1e-9 < best_L:\n",
    "        print(\"Reassign best values.\")\n",
    "        best_L = L\n",
    "        best_KG = json.loads(json.dumps(kg))\n",
    "        best_P = P.copy()\n",
    "        best_fgw = fgw_total\n",
    "        improved = True\n",
    "        for k, v in local_ops.items():\n",
    "            ops_counters[k] += v\n",
    "    else:\n",
    "        print(\"best values remain.\")\n",
    "        for k, v in local_ops.items():\n",
    "            ops_counters[k] += v\n",
    "\n",
    "    if not improved:\n",
    "        # If no improvement, consider stopping early\n",
    "        if abs(history[-1][\"objective\"] - history[-2][\"objective\"]) < hp.convergence_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5009af3b-2b95-4b5e-a6ce-e856e3ea4854",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758492765344,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5009af3b-2b95-4b5e-a6ce-e856e3ea4854",
    "outputId": "260c49d7-4f9a-460f-e8c5-9faa2bc76bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'edge_add': 114,\n",
       "         'add': 50,\n",
       "         'edge_remove': 27,\n",
       "         'split': 19,\n",
       "         'merge': 5})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53099753-390d-4f8c-afcc-8b4603626e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 245)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kg['nodes']), len(kg['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf130615-76b3-4fea-b26c-105eedefc047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++\n",
    "# Domain knowledge graph refinement\n",
    "#\n",
    "#+++++++++++++++++\n",
    "added_edges_domain_refinement = apply_domain_knowledge_LLM(kg, allowed_relations)\n",
    "\n",
    "added_node_counter, added_edge_counter = add_refined_domain_edges(kg, added_edges_domain_refinement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90716d5e-179e-4dc2-a7ee-0375751d4ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 35)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_node_counter, added_edge_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22d3b96d-009e-4376-849d-d1a1022cbc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    }
   ],
   "source": [
    "#+++++++++++++++++\n",
    "# Domain knowledge graph refinement\n",
    "# Update history\n",
    "#+++++++++++++++++\n",
    "\n",
    "it = it + 1\n",
    "\n",
    "local_ops = collections.Counter()\n",
    "local_ops['add'] += added_node_counter\n",
    "local_ops['edge_add'] += added_edge_counter\n",
    "\n",
    "# Evaluate objective\n",
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")\n",
    "\n",
    "r = rate_complexity(kg)\n",
    "L = r + hp.beta * fgw_total\n",
    "\n",
    "history.append({\n",
    "    \"iter\": it,\n",
    "    \"rate\": r,\n",
    "    \"fgw_total\": fgw_total,\n",
    "    \"fgw_structure\": struct_term,\n",
    "    \"fgw_feature\": feat_term,\n",
    "    \"objective\": L,\n",
    "    \"ops\": dict(local_ops)\n",
    "})\n",
    "\n",
    "for k, v in local_ops.items():\n",
    "    ops_counters[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a86b406-6cee-404f-b8f1-c18ca0c65fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'edge_add': 149,\n",
       "         'add': 50,\n",
       "         'edge_remove': 27,\n",
       "         'split': 19,\n",
       "         'merge': 5})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cb5f0d1-c464-415e-b8a7-3926f9fadac1",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758492771936,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "6cb5f0d1-c464-415e-b8a7-3926f9fadac1"
   },
   "outputs": [],
   "source": [
    "# Build outcome\n",
    "outcome = RefinementOutcome(\n",
    "    iterations=len(history)-1,\n",
    "    final_objective=float(best_L),\n",
    "    rate=float(rate_complexity(best_KG)),\n",
    "    distortion=float(best_fgw),   # report main FGW term as 'distortion'\n",
    "    fgw_distance=float(best_fgw),\n",
    "    history=history,\n",
    "    operations={\n",
    "        \"concepts_added\": int(ops_counters.get(\"add\", 0)),\n",
    "        \"merged\": int(ops_counters.get(\"merge\", 0)),\n",
    "        \"concepts_split\": int(ops_counters.get(\"split\", 0)),\n",
    "        \"relationships_added\": int(ops_counters.get(\"edge_add\", 0)),\n",
    "        \"relationships_removed\": int(ops_counters.get(\"edge_remove\", 0)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5399316-c8cb-41e3-ad62-409c96ec9a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1758492774376,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "c5399316-c8cb-41e3-ad62-409c96ec9a23",
    "outputId": "7cb5226c-139d-4ae3-bbc9-1a5c1ded1441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concepts_added': 50,\n",
       " 'merged': 5,\n",
       " 'concepts_split': 19,\n",
       " 'relationships_added': 149,\n",
       " 'relationships_removed': 27}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57903a4e-06ec-4389-ae12-45e907c7a111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1758492776808,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "57903a4e-06ec-4389-ae12-45e907c7a111",
    "outputId": "97c9a7fc-bbc8-4a6c-9c50-f45edc044912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    }
   ],
   "source": [
    "# Coupling analysis (top-k)\n",
    "# For interpretability, recompute matrices for best_KG\n",
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(best_KG, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "nu = degree_centrality_measure(best_KG, node_ids)\n",
    "P, best_fgw, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")\n",
    "coupling_report = []\n",
    "\n",
    "topk_align = 12\n",
    "\n",
    "N, M = P.shape\n",
    "for i in range(N):\n",
    "    topj = np.argsort(-P[i, :])[:min(topk_align, M)]\n",
    "    row = []\n",
    "    for j in topj:\n",
    "        row.append({\n",
    "            \"lecture_idx\": int(i),\n",
    "            \"lecture_text\": elements[i].text[:240],\n",
    "            \"concept_id\": node_ids[j],\n",
    "            \"concept_label\": next((n[\"label\"] for n in best_KG[\"nodes\"] if n[\"id\"] == node_ids[j]), node_ids[j]),\n",
    "            \"weight\": float(P[i, j])\n",
    "        })\n",
    "    coupling_report.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81a45df6-e769-428e-a61f-b04abfc75f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/lecture_notes_8_objective_rate_distortion_curve.png'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save artifacts\n",
    "file_name = \"objective_rate_distortion_curve.png\"\n",
    "rd_curve_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{file_name}\"))\n",
    "rd_curve_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9974bd86-691c-44fd-96ee-2231d2e018ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1758492778791,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9974bd86-691c-44fd-96ee-2231d2e018ca"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    # One chart, no styles/colors\n",
    "    xs = [h[\"iter\"] for h in history]\n",
    "    rates = [h[\"rate\"] for h in history]\n",
    "    dists = [h[\"fgw_total\"] for h in history]\n",
    "    objs = [h[\"objective\"] for h in history]\n",
    "    plt.figure()\n",
    "    plt.plot(xs, objs, label=\"Objective L\")\n",
    "    plt.plot(xs, rates, label=\"Rate r\")\n",
    "    plt.plot(xs, dists, label=\"Distortion d (FGW)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(rd_curve_path, dpi=160)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    logging.warning(\"Could not generate RD curve plot: %s\", e)\n",
    "    rd_curve_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24bebd2b-ec3c-4e35-912d-fbb8d8246d09",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492781063,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "24bebd2b-ec3c-4e35-912d-fbb8d8246d09"
   },
   "outputs": [],
   "source": [
    "# Package refined KG meta\n",
    "if \"meta\" not in best_KG:\n",
    "    best_KG[\"meta\"] = {}\n",
    "best_KG[\"meta\"].setdefault(\"refinement\", {})\n",
    "best_KG[\"meta\"][\"refinement\"].update({\n",
    "    \"iterations\": outcome.iterations,\n",
    "    \"final_objective\": outcome.final_objective,\n",
    "    \"rate\": outcome.rate,\n",
    "    \"distortion\": outcome.distortion,\n",
    "    \"fgw_distance\": outcome.fgw_distance,\n",
    "    \"used_pot_fgw\": bool(use_pot and _HAS_POT),\n",
    "    \"alpha\": {\n",
    "        \"chronological\": hp.alpha_chron,\n",
    "        \"logical\": hp.alpha_logic,\n",
    "        \"semantic\": hp.alpha_sem\n",
    "    },\n",
    "    \"gamma\": {\n",
    "        \"structural\": hp.gamma_struct,\n",
    "        \"semantic\": hp.gamma_sem\n",
    "    },\n",
    "    \"lambda_feature_balance\": hp.lambda_feat,\n",
    "    \"beta_rate_distortion\": hp.beta,\n",
    "    \"thresholds\": {\n",
    "        \"theta_add\": hp.theta_add,\n",
    "        \"theta_split\": hp.theta_split,\n",
    "        \"theta_merge\": hp.theta_merge,\n",
    "        \"theta_relate\": hp.theta_relate\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db84d6b-ae2d-4425-b72c-b336fc1e37d5",
   "metadata": {
    "id": "1db84d6b-ae2d-4425-b72c-b336fc1e37d5"
   },
   "source": [
    "# Plot\n",
    "- Computes rate R = nodes + 0.5 * edges.\n",
    "- Computes objective L = R + beta * D for a user-chosen beta (default 20).\n",
    "- Finds the geometric knee (max perpendicular distance to the line between the\n",
    "  first and last points in (D, R) space).\n",
    "- Writes a CSV of the iteration history and saves a PNG plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbefbd45-42eb-4e03-9062-9ea7a6d12465",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1758492917145,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "cbefbd45-42eb-4e03-9062-9ea7a6d12465"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RDHistory:\n",
    "    iters: np.ndarray       # iteration indices\n",
    "    D: np.ndarray           # distortion per iteration\n",
    "    R: np.ndarray\n",
    "\n",
    "    #nodes: np.ndarray       # node count per iteration\n",
    "    #edges: np.ndarray       # edge count per iteration\n",
    "\n",
    "\n",
    "def objective_L(R: np.ndarray, D: np.ndarray, beta: float) -> np.ndarray:\n",
    "    return R + beta * D\n",
    "\n",
    "\n",
    "def knee_index_max_distance(D: np.ndarray, R: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Knee = point of maximum perpendicular distance to the chord from the\n",
    "    first to last point in (D, R) space.\n",
    "    \"\"\"\n",
    "    A = np.array([D[0], R[0]], dtype=float)\n",
    "    B = np.array([D[-1], R[-1]], dtype=float)\n",
    "    AB = B - A\n",
    "    ab_norm = np.linalg.norm(AB)\n",
    "    if ab_norm == 0:\n",
    "        return 0\n",
    "\n",
    "    def dist_to_line(p: np.ndarray) -> float:\n",
    "        # In 2D, || (B - A) x (P - A) || / ||B - A||\n",
    "        return np.abs(np.cross(AB, p - A)) / ab_norm\n",
    "\n",
    "    distances = np.array([dist_to_line(np.array([D[i], R[i]], dtype=float)) for i in range(len(D))])\n",
    "    return int(np.argmax(distances))\n",
    "\n",
    "\n",
    "def beta_threshold_vs_seed(R: np.ndarray, D: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each iteration i, solve R_i + beta*D_i = R_0 + beta*D_0 for beta.\n",
    "    beta_i = (R0 - Ri) / (Di - D0) = (Ri - R0) / (D0 - Di)\n",
    "    \"\"\"\n",
    "    R0, D0 = R[0], D[0]\n",
    "    betas = []\n",
    "    for i in range(len(D)):\n",
    "        if i == 0:\n",
    "            betas.append(np.nan)\n",
    "            continue\n",
    "        denom = (D0 - D[i])\n",
    "        betas.append((R[i] - R0) / denom if denom != 0 else np.inf)\n",
    "    return np.array(betas, dtype=float)\n",
    "\n",
    "\n",
    "def build_dataframe(hist: RDHistory, beta: float) -> pd.DataFrame:\n",
    "    R = hist.R\n",
    "    L = objective_L(R, hist.D, beta)\n",
    "    beta_tie = beta_threshold_vs_seed(R, hist.D)\n",
    "    df = pd.DataFrame({\n",
    "        \"iter\": hist.iters,\n",
    "        \"distortion_D\": hist.D,\n",
    "        \"rate_R\": R,\n",
    "        #\"nodes\": hist.nodes,\n",
    "        #\"edges\": hist.edges,\n",
    "        f\"objective_L(beta={beta:g})\": L,\n",
    "        \"beta_threshold_vs_seed\": beta_tie\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_rd_curve(\n",
    "    hist: RDHistory,\n",
    "    beta: float,\n",
    "    savepath: str,\n",
    "    show_iso_L: bool = False,\n",
    "    iso_L_count: int = 4\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Plot R vs D, annotate iterations, knee, and min-L point (for given beta).\n",
    "    Optionally overlay iso-L lines.\n",
    "    \"\"\"\n",
    "    D, R = hist.D, hist.R\n",
    "    L = objective_L(R, D, beta)\n",
    "    knee_idx = knee_index_max_distance(D, R)\n",
    "    lopt_idx = int(np.argmin(L))\n",
    "\n",
    "    plt.figure(figsize=(6.0, 5.0))\n",
    "    # R–D curve\n",
    "    plt.plot(D, R, marker='o')  # default styling\n",
    "\n",
    "    # annotate points t0..tn\n",
    "    for i in range(len(D)):\n",
    "        plt.annotate(f\"t{i}\", (D[i], R[i]), xytext=(5, 5),\n",
    "                     textcoords='offset points', fontsize=8)\n",
    "\n",
    "    # highlight knee and L-optimum\n",
    "    plt.scatter([D[knee_idx]], [R[knee_idx]], marker='s', s=90, label=f\"Knee ~ t{knee_idx}\")\n",
    "    plt.scatter([D[lopt_idx]], [R[lopt_idx]], marker='*', s=160, label=f\"Min L (β={beta:g}) ~ t{lopt_idx}\")\n",
    "\n",
    "    # optional: iso-L lines through a few L values\n",
    "    if show_iso_L:\n",
    "        # pick a span of L values centered around median\n",
    "        Lmin, Lmax = float(np.min(L)), float(np.max(L))\n",
    "        Lvals = np.linspace(Lmin, Lmax, num=iso_L_count)\n",
    "        x_span = np.linspace(float(np.min(D)) - 0.02, float(np.max(D)) + 0.02, 100)\n",
    "        for Lv in Lvals:\n",
    "            # R = L - beta * D\n",
    "            y_line = Lv - beta * x_span\n",
    "            plt.plot(x_span, y_line, linestyle='--', linewidth=0.8)\n",
    "\n",
    "    plt.xlabel(\"Distortion D\")\n",
    "    plt.ylabel(\"Rate R\")\n",
    "    plt.title(\"Rate–Distortion Curve (D on x, R on y)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=150)\n",
    "    plt.close()\n",
    "    return knee_idx, lopt_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4437e891-ede0-458d-8b2f-fb5f993cd252",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492922180,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4437e891-ede0-458d-8b2f-fb5f993cd252"
   },
   "outputs": [],
   "source": [
    "plot_file_name = \"rate_distortion_plot_curve.png\" #help=\"Base filename (without extension) for outputs\")\n",
    "iso = True # help=\"Overlay iso-L lines on the plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd0cb073-8724-4e1e-9ab7-e12fa3f47dc3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758492924108,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "dd0cb073-8724-4e1e-9ab7-e12fa3f47dc3"
   },
   "outputs": [],
   "source": [
    "iters = []\n",
    "D = []\n",
    "R = []\n",
    "for h in history:\n",
    "    iters.append(h['iter'])\n",
    "    R.append(h['rate'])\n",
    "    D.append(h['fgw_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cc122fe-e91d-4007-a27a-3145b2982027",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758492926166,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0cc122fe-e91d-4007-a27a-3145b2982027"
   },
   "outputs": [],
   "source": [
    "hist = RDHistory(np.array(iters), np.array(D), np.array(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37e31110-04ee-46a4-8196-7924ec899f4d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758492926889,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "37e31110-04ee-46a4-8196-7924ec899f4d"
   },
   "outputs": [],
   "source": [
    "df = build_dataframe(hist, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d73ee278-0ffb-4ae0-b45e-fa021440c925",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1758492927853,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "d73ee278-0ffb-4ae0-b45e-fa021440c925",
    "outputId": "26888e3b-d7ca-404a-8e88-3588f6ee4899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>distortion_D</th>\n",
       "      <th>rate_R</th>\n",
       "      <th>objective_L(beta=100)</th>\n",
       "      <th>beta_threshold_vs_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.613520</td>\n",
       "      <td>7.0</td>\n",
       "      <td>68.352041</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.474801</td>\n",
       "      <td>20.0</td>\n",
       "      <td>67.480114</td>\n",
       "      <td>93.714448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.432905</td>\n",
       "      <td>35.5</td>\n",
       "      <td>78.790532</td>\n",
       "      <td>157.794124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.410908</td>\n",
       "      <td>50.0</td>\n",
       "      <td>91.090846</td>\n",
       "      <td>212.228348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.396742</td>\n",
       "      <td>65.5</td>\n",
       "      <td>105.174217</td>\n",
       "      <td>269.861030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.372485</td>\n",
       "      <td>80.5</td>\n",
       "      <td>117.748481</td>\n",
       "      <td>304.934217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.377586</td>\n",
       "      <td>102.5</td>\n",
       "      <td>140.258605</td>\n",
       "      <td>404.773604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.365558</td>\n",
       "      <td>121.5</td>\n",
       "      <td>158.055805</td>\n",
       "      <td>461.763638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.358528</td>\n",
       "      <td>146.0</td>\n",
       "      <td>181.852827</td>\n",
       "      <td>545.114835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.347876</td>\n",
       "      <td>172.5</td>\n",
       "      <td>207.287637</td>\n",
       "      <td>623.014166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.361719</td>\n",
       "      <td>191.5</td>\n",
       "      <td>227.671885</td>\n",
       "      <td>732.719835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>209.0</td>\n",
       "      <td>245.292673</td>\n",
       "      <td>806.085790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  distortion_D  rate_R  objective_L(beta=100)  beta_threshold_vs_seed\n",
       "0      0      0.613520     7.0              68.352041                     NaN\n",
       "1      1      0.474801    20.0              67.480114               93.714448\n",
       "2      2      0.432905    35.5              78.790532              157.794124\n",
       "3      3      0.410908    50.0              91.090846              212.228348\n",
       "4      4      0.396742    65.5             105.174217              269.861030\n",
       "5      5      0.372485    80.5             117.748481              304.934217\n",
       "6      6      0.377586   102.5             140.258605              404.773604\n",
       "7      7      0.365558   121.5             158.055805              461.763638\n",
       "8      8      0.358528   146.0             181.852827              545.114835\n",
       "9      9      0.347876   172.5             207.287637              623.014166\n",
       "10    10      0.361719   191.5             227.671885              732.719835\n",
       "11    11      0.362927   209.0             245.292673              806.085790"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47452a56-f4e1-40ab-916e-49ff1f5586f2",
   "metadata": {},
   "source": [
    "### Save Rate_Distortion History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "990d1d0e-cd02-429b-ac1e-9ddb1407a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/lecture_notes_8_rate_distortion_history.csv'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_name = \"rate_distortion_history.csv\"\n",
    "history_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{history_name}\"))\n",
    "history_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "368b9171-9753-4af9-80fb-d676aa82ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2079772-0914-4cc2-8ea3-0c2182993597",
   "metadata": {},
   "source": [
    "### Save KG History and KG_Knee to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89fde175-89d4-4fd6-98d7-2407adbd7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{plot_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce8a3b97-f565-4e6a-bf35-90981544d9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/lecture_notes_8_rate_distortion_plot_curve.png'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "png_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60666231-73e1-4a00-a513-ac7583c9bae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1758492931676,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "60666231-73e1-4a00-a513-ac7583c9bae4",
    "outputId": "eb5483c0-75a8-47a4-95bb-ac8dfe3dca60"
   },
   "outputs": [],
   "source": [
    "knee_idx, lopt_idx = plot_rd_curve(hist, beta, png_path, show_iso_L=iso, iso_L_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9934dc10-399c-4138-84bc-abb85459360c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knee_idx, lopt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dc3c48f-7793-441a-a56b-f08828787af8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758492934812,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "2dc3c48f-7793-441a-a56b-f08828787af8",
    "outputId": "b5b72f33-79ed-498c-a170-5a918964df1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rate–Distortion Summary ===\n",
      "beta (β): 100\n",
      "Knee index (max distance): t5  [D=0.372, R=80.500]\n",
      "Min L index:               t1  [D=0.475, R=20.000, L=67.480]\n",
      "\n",
      "β thresholds to tie seed (t0) in L (higher → favors higher-fidelity iterations):\n",
      "  t0: —\n",
      "  t1: β ≈ 93.71\n",
      "  t2: β ≈ 157.79\n",
      "  t3: β ≈ 212.23\n",
      "  t4: β ≈ 269.86\n",
      "  t5: β ≈ 304.93\n",
      "  t6: β ≈ 404.77\n",
      "  t7: β ≈ 461.76\n",
      "  t8: β ≈ 545.11\n",
      "  t9: β ≈ 623.01\n",
      "  t10: β ≈ 732.72\n",
      "  t11: β ≈ 806.09\n"
     ]
    }
   ],
   "source": [
    "# Console summary\n",
    "R, D = hist.R, hist.D\n",
    "L = objective_L(R, D, beta)\n",
    "print(\"\\n=== Rate–Distortion Summary ===\")\n",
    "print(f\"beta (β): {beta:g}\")\n",
    "print(f\"Knee index (max distance): t{knee_idx}  [D={D[knee_idx]:.3f}, R={R[knee_idx]:.3f}]\")\n",
    "print(f\"Min L index:               t{lopt_idx}  [D={D[lopt_idx]:.3f}, R={R[lopt_idx]:.3f}, L={L[lopt_idx]:.3f}]\")\n",
    "\n",
    "# Optional quick thresholds report\n",
    "beta_tie = beta_threshold_vs_seed(R, D)\n",
    "print(\"\\nβ thresholds to tie seed (t0) in L (higher → favors higher-fidelity iterations):\")\n",
    "for i, b in enumerate(beta_tie):\n",
    "    if i == 0:\n",
    "        print(f\"  t{i}: —\")\n",
    "    else:\n",
    "        if math.isfinite(b):\n",
    "            print(f\"  t{i}: β ≈ {b:.2f}\")\n",
    "        else:\n",
    "            print(f\"  t{i}: β = ∞ (no finite tie)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4902c4e-892b-43ff-a6ce-508ac55adbae",
   "metadata": {
    "id": "f4902c4e-892b-43ff-a6ce-508ac55adbae"
   },
   "outputs": [],
   "source": [
    "# The KG at knee point\n",
    "kg_knee = kg_history[knee_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0b5b556-326a-4865-9650-6de4aaa7e139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 83)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kg_knee['nodes']), len(kg_knee['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3296831-0b35-4841-8461-88ae93e76d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_file_name = \"kg_history.json\"\n",
    "kg_history_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{history_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d180658e-8d7e-4007-97e5-7cf1dd10cd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/lecture_notes_8_kg_history.json'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_history_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f49badfc-6e07-4169-aca6-628a1ace69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(kg_history_path, \"w\") as f:\n",
    "    json.dump(kg_history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "456550a1-45eb-43f3-ab6a-978dc6e63c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_file_name = \"kg_knee.json\"\n",
    "kg_knee_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{knee_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f64a4a89-e1d6-42c1-9b41-0be1d5ab55f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/lecture_notes_8_kg_knee.json'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_knee_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "492c0c26-e183-4ffd-94d5-0774ab57abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(kg_knee_path, \"w\") as f:\n",
    "    json.dump(kg_knee, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b610e50-a67e-4e79-af51-f32d2bfa5a54",
   "metadata": {},
   "source": [
    "## Checking Coverage of KG vs. Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "898303dc-d09f-4a61-9b11-3e3988120f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given lecture elements and KG, evaluate the KG's coverage\n",
    "# within tolerated feature distortion\n",
    "def kg_coverage_with_feature_distortion(elements, kg, embedder, hp, us_pot, thresh):\n",
    "    # build lecture metric-measure space \n",
    "    lecture_texts = [e.text for e in elements]\n",
    "    E_L = embedder.fit_transform(lecture_texts)\n",
    "    D_L, D_chron, D_logic, D_semL = build_lecture_distance(\n",
    "        elements, E_L, hp.alpha_chron, hp.alpha_logic, hp.alpha_sem\n",
    "    )\n",
    "\n",
    "    # build KG metri-measure space\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, \n",
    "                                                             embedder, \n",
    "                                                             hp.gamma_struct, hp.gamma_sem)\n",
    "\n",
    "    # build lecture elements and kg nodes distributions\n",
    "    mu = normalized_measure(len(elements))\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "    # compute coupling\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "\n",
    "    # compute feature distortion\n",
    "    M_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "\n",
    "    # compute feature distortion tolerance\n",
    "    tau = 0\n",
    "    feat_ravel = np.asarray(M_feat, dtype=float).ravel()\n",
    "    feat_ravel = feat_ravel[np.isfinite(feat_ravel)]\n",
    "    if feat_ravel.size != 0:\n",
    "        tau = float(np.quantile(feat_ravel, thresh))\n",
    "\n",
    "    # compute coverage\n",
    "    good = (M_feat <= tau).astype(float)\n",
    "\n",
    "    per_pair = P * good\n",
    "    return per_pair.sum()  # already in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2bbd4c36-d906-44fd-af50-4c0a6a947846",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_json_path = \"./data/lecture_notes_8.json\"\n",
    "kg_json_path = \"./data/kg8.json\"\n",
    "kg_knee_path = \"./data/output/lecture_notes_8_kg_knee.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "358eec85-0b09-4445-a3d5-cc4bc9dededd",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = parse_lecture_elements(lecture_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "055f9a95-ef24-4639-b4c5-0c0162b7fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = read_kg(kg_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb4c7963-ca40-4bf1-b3c9-7e73a3b599ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5306603773584907"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_cov = kg_coverage_with_feature_distortion(elements, kg, embedder, hp, True, 0.3)\n",
    "kg_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fded01e-c934-4a18-af8b-54eb9a6fc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_knee_file = read_json(kg_knee_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9ff06ab-74d7-48aa-9a34-35d006389652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use SentenceTransformer.\n",
      "Use SentenceTransformer.\n",
      "Use POT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.896112752898386"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_knee_cov = kg_coverage_with_feature_distortion(elements, kg_knee_file, embedder, hp, True, 0.3)\n",
    "kg_knee_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "51921395-d42e-4be2-83f9-5d2df6853e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_list = []\n",
    "kg_cov_list  = []\n",
    "kg_knee_cov_list = []\n",
    "tau_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a718e5f0-40a1-4f2f-8d76-ef8a40403af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_list.append(\"lecture_notes_8\")\n",
    "kg_cov_list.append(kg_cov)\n",
    "kg_knee_cov_list.append(kg_knee_cov)\n",
    "tau_list.append(\"30-th quantile of feature distortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7642ac8-0d80-43b6-8978-9a2429fd83d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lecture</th>\n",
       "      <th>initial_kg_coverage</th>\n",
       "      <th>knee_kg_coverage</th>\n",
       "      <th>feature_distortion_tolerance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lecture_notes_8</td>\n",
       "      <td>0.53066</td>\n",
       "      <td>0.896113</td>\n",
       "      <td>30-th quantile of feature distortion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lecture  initial_kg_coverage  knee_kg_coverage  \\\n",
       "0  lecture_notes_8              0.53066          0.896113   \n",
       "\n",
       "           feature_distortion_tolerance  \n",
       "0  30-th quantile of feature distortion  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_pd = pd.DataFrame({\"lecture\":lecture_list, \"initial_kg_coverage\":kg_cov_list,\n",
    "              \"knee_kg_coverage\":kg_knee_cov_list, \"feature_distortion_tolerance\":tau_list})\n",
    "coverage_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b994564-0f9d-40b6-a410-8e33bd173c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_pd.to_csv(\"./data/output/kg_coverages.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "drtutor",
   "language": "python",
   "name": "drtutor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
