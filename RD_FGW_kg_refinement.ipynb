{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f936debf-85b7-4b90-935c-6e4e73227b65",
   "metadata": {
    "id": "f936debf-85b7-4b90-935c-6e4e73227b65"
   },
   "source": [
    "# Rate-Distortion Guided Knowledge Graph Refinement with LLM-Assisted Operations\n",
    "\n",
    "## This notebook contains the functions that produce the following actions:\n",
    "- Read in lecture notes JSON into leaf elements\n",
    "- Build fused distance on the lecture side: chronology + logic + semantics\n",
    "- Build fused distance on the KG side: structure + semantics\n",
    "- Compute Fused Gromov–Wasserstein (FGW) coupling if POT is installed\n",
    "  (optional; exact). Otherwise, compute a proxy coupling via entropic OT\n",
    "  over a feature+structure cost surrogate.\n",
    "- Execute greedy refinement ops that minimize $L = r + \\beta * d$:\n",
    "  - add concepts from under-represented content\n",
    "  - split/merge concepts from coupling patterns\n",
    "  - add/rewire/remove relationships\n",
    "- Refine the KG by LLM based on domain knowledge\n",
    "- Output:\n",
    "  - Refined KG JSON (same schema as input)\n",
    "  - Refinement report JSON (iterations, objective, rates, distortions)\n",
    "  - Coupling analysis JSON (top-k matches)\n",
    "  - History of refinements\n",
    "  - Rate–Distortion curve PNG\n",
    "\n",
    "\n",
    "## Libraries:\n",
    "- Required: numpy, networkx, scipy (for distance + kmeans), scikit-learn\n",
    "- Optional: sentence-transformers (for embeddings; else falls back to TF-IDF)\n",
    "- Optional: POT (Python Optimal Transport, `pip install POT`) for exact FGW\n",
    "\n",
    "## Notes:\n",
    "- The script is deterministic up to random seeds for kmeans; you can set --seed.\n",
    "- If POT is present, exact FGW is used. If not, we compute an entropic OT coupling\n",
    "  with a proxy cost that mixes feature distance and a local-structure penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efd94d-f8d0-4f86-8d65-4e67e1235cc8",
   "metadata": {
    "id": "b4efd94d-f8d0-4f86-8d65-4e67e1235cc8"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QwaweHLIZfVm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14240,
     "status": "ok",
     "timestamp": 1758489187546,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "QwaweHLIZfVm",
    "outputId": "1d6c57b3-d286-4bca-8541-128edd8d30b1"
   },
   "outputs": [],
   "source": [
    "#!pip install -q POT\n",
    "#!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2dc0a-5dc9-4f5d-9168-addcfdc94bcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 30306,
     "status": "ok",
     "timestamp": 1758492419310,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4cf2dc0a-5dc9-4f5d-9168-addcfdc94bcf"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import networkx as nx\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires 'networkx'. Please install it: pip install networkx\") from e\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.cluster import KMeans\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires scikit-learn. Please install it: pip install scikit-learn\") from e\n",
    "\n",
    "try:\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from scipy.special import rel_entr\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"This script requires scipy. Please install it: pip install scipy\") from e\n",
    "\n",
    "# Optional embeddings\n",
    "_SENTENCE_TF = None\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SENTENCE_TF = SentenceTransformer\n",
    "except Exception:\n",
    "    _SENTENCE_TF = None\n",
    "\n",
    "# Optional POT for exact FGW\n",
    "_HAS_POT = False\n",
    "try:\n",
    "    import ot\n",
    "    from ot.gromov import fused_gromov_wasserstein\n",
    "    _HAS_POT = True\n",
    "except Exception:\n",
    "    _HAS_POT = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbc2db-968b-4d87-9b7d-f28a56cb509a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758492419324,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "54fbc2db-968b-4d87-9b7d-f28a56cb509a",
    "outputId": "17fcd82b-f715-4397-f556-d29cdab9a0d1"
   },
   "outputs": [],
   "source": [
    "_SENTENCE_TF, _HAS_POT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f194ce-726e-40db-a17b-72c3bdf8f217",
   "metadata": {
    "id": "01f194ce-726e-40db-a17b-72c3bdf8f217"
   },
   "source": [
    "## Utilitiy and Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f413d1-cdc3-40e2-aabc-7bd36951ea99",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492422426,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "24f413d1-cdc3-40e2-aabc-7bd36951ea99"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utility & Data Structures\n",
    "# -----------------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    alpha_chron: float = 0.2\n",
    "    alpha_logic: float = 0.3\n",
    "    alpha_sem: float = 0.5\n",
    "    gamma_struct: float = 0.4\n",
    "    gamma_sem: float = 0.6\n",
    "    lambda_feat: float = 0.6     # feature balance inside FGW objective\n",
    "    beta: float = 20.0           # rate–distortion trade-off\n",
    "    theta_add: float = 0.06\n",
    "    theta_split: float = 0.35\n",
    "    theta_merge: float = 0.12\n",
    "    theta_relate: float = 0.25\n",
    "    max_iterations: int = 10\n",
    "    convergence_threshold: float = 0.01\n",
    "    sinkhorn_eps: float = 0.05\n",
    "    sinkhorn_iter: int = 300\n",
    "\n",
    "@dataclass\n",
    "class LectureElement:\n",
    "    idx: int\n",
    "    id_path: str\n",
    "    section_path: List[str]\n",
    "    text: str\n",
    "    typ: str\n",
    "\n",
    "@dataclass\n",
    "class KGNode:\n",
    "    id: str\n",
    "    label: str\n",
    "    type: str\n",
    "    definition: str\n",
    "    aliases: List[str]\n",
    "    provenance: List[Dict[str, Any]]\n",
    "    attributes: Dict[str, Any]\n",
    "    confidence: float\n",
    "    rationale: str\n",
    "\n",
    "@dataclass\n",
    "class KGEdge:\n",
    "    id: str\n",
    "    source: str\n",
    "    target: str\n",
    "    relation: str\n",
    "    definition: str\n",
    "    provenance: List[Dict[str, Any]]\n",
    "    confidence: float\n",
    "    rationale: str\n",
    "\n",
    "@dataclass\n",
    "class RefinementOutcome:\n",
    "    iterations: int\n",
    "    final_objective: float\n",
    "    rate: float\n",
    "    distortion: float\n",
    "    fgw_distance: float\n",
    "    history: List[Dict[str, Any]]\n",
    "    operations: Dict[str, int]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f2ef2-759f-475c-8396-ad4d2168af0f",
   "metadata": {
    "id": "120f2ef2-759f-475c-8396-ad4d2168af0f"
   },
   "source": [
    "## I/O Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfee7a6-27b6-4c6d-b956-6ebc8da4e31c",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758492425723,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "6dfee7a6-27b6-4c6d-b956-6ebc8da4e31c"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# IO Helpers\n",
    "# -----------------------------\n",
    "\n",
    "def read_json(path: str) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(obj: Any, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031250b-5c93-4515-9b06-a4b7198dd493",
   "metadata": {},
   "source": [
    "## Read KG with Edge Text Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c1398-5361-49fa-bc2b-95154ea91449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return definition text for an edge\n",
    "def get_edge_definition(kg, source_id, target_id, relation):\n",
    "    \n",
    "    slabel = source_id\n",
    "    tlabel = target_id\n",
    "    \n",
    "    for anode in kg['nodes']:\n",
    "        \n",
    "        if anode['id'].lower() == source_id.lower():\n",
    "            slabel = anode['label']\n",
    "                \n",
    "        if anode['id'].lower() == target_id.lower():\n",
    "            tlabel = anode['label']\n",
    "\n",
    "        \n",
    "    return slabel + \" \" + relation + \" \" + tlabel\n",
    "\n",
    "# add relations where coupled lecture elements are near\n",
    "def get_edge_rationale(kg, source_id, target_id, relation):\n",
    "\n",
    "    slabel = source_id\n",
    "    tlabel = target_id\n",
    "    \n",
    "    for anode in kg['nodes']:\n",
    "        \n",
    "        if anode['id'].lower() == source_id.lower():\n",
    "            slabel = anode['label']\n",
    "                \n",
    "        if anode['id'].lower() == target_id.lower():\n",
    "            tlabel = anode['label']\n",
    "\n",
    "    retionale = f'{slabel} {relation} {tlabel} because their coupled lecture elements are near.'\n",
    "    \n",
    "    return rationale\n",
    "    \n",
    "# read KG with definition edges\n",
    "def read_kg(path: str) -> Any:\n",
    "    kg_json = read_json(path)\n",
    "\n",
    "    for edge in kg_json['edges']:\n",
    "        if ('definition' not in edge) or (not edge['definition']):\n",
    "            sid = edge['source']\n",
    "            tid = edge['target']\n",
    "            e_def = get_edge_definition(kg_json, sid, tid, edge['relation'])\n",
    "\n",
    "            edge['definition'] = e_def\n",
    "\n",
    "    return kg_json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26e179-4f40-47c3-97a5-4d1eab60bdcc",
   "metadata": {
    "id": "2b26e179-4f40-47c3-97a5-4d1eab60bdcc"
   },
   "source": [
    "## Lecture JSON Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dfdbb1-9fe6-40c2-90e8-3fa0274aa5cd",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1758492427361,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "42dfdbb1-9fe6-40c2-90e8-3fa0274aa5cd"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Lecture JSON Parsing\n",
    "# -----------------------------\n",
    "\n",
    "def _flatten_lecture_json(node: Dict[str, Any],\n",
    "                          section_stack: Optional[List[str]] = None,\n",
    "                          out: Optional[List[LectureElement]] = None,\n",
    "                          idx_offset: int = 0) -> List[LectureElement]:\n",
    "    \"\"\"\n",
    "    Generic flattener for lecture JSONs with fields:\n",
    "      - id, level, type, title, content, children (list)\n",
    "    or where leaf 'elements' already exist (preferred).\n",
    "    \"\"\"\n",
    "    if out is None:\n",
    "        out = []\n",
    "    if section_stack is None:\n",
    "        section_stack = []\n",
    "\n",
    "    title = node.get(\"title\") or node.get(\"label\") or node.get(\"name\") or \"\"\n",
    "    typ = node.get(\"type\") or \"section\"\n",
    "    id_str = str(node.get(\"id\", \"\"))\n",
    "    content = node.get(\"content\", \"\")\n",
    "\n",
    "    # If explicit elements exist (pre-cleaned file), use them\n",
    "    elements = node.get(\"elements\") or []\n",
    "    if elements and isinstance(elements, list):\n",
    "        for e in elements:\n",
    "            text = e.get(\"text\", \"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            etype = e.get(\"type\", \"text\")\n",
    "            out.append(LectureElement(\n",
    "                idx=idx_offset + len(out),\n",
    "                id_path=id_str,\n",
    "                section_path=section_stack + [title] if title else section_stack[:],\n",
    "                text=text,\n",
    "                typ=etype\n",
    "            ))\n",
    "    else:\n",
    "        # Fallback: split content into crude sentence-ish lines\n",
    "        #\n",
    "        # This part repeats \"Break Down to Individual Elements\"\n",
    "        # in \"RD_FGW_source_markdown_json.ipynb\".\n",
    "        # Assume HERE that :code and :markdwon suffixes have been removed\n",
    "        #\n",
    "        lines = [ln.strip() for ln in str(content).splitlines() if ln.strip()]\n",
    "        for ln in lines:\n",
    "            out.append(LectureElement(\n",
    "                idx=idx_offset + len(out),\n",
    "                id_path=id_str,\n",
    "                section_path=section_stack + [title] if title else section_stack[:],\n",
    "                text=ln,\n",
    "                typ=\"text\"\n",
    "            ))\n",
    "\n",
    "    # Recurse into children\n",
    "    for ch in node.get(\"children\", []) or []:\n",
    "        _flatten_lecture_json(ch, section_stack + ([title] if title else []),\n",
    "                              out, idx_offset)\n",
    "\n",
    "    return out\n",
    "\n",
    "import re\n",
    "from copy import copy\n",
    "\n",
    "def clean_up_elements(elements):\n",
    "    out = []\n",
    "    for e in elements:\n",
    "        etext = (e.get('text') if isinstance(e, dict) else getattr(e, 'text', '')) or ''\n",
    "        if re.fullmatch(r\"\\s*(?:`{3,}|~{3,}|_{3,}|-{3,})\\s*\", etext):\n",
    "            continue\n",
    "        t = re.sub(r\"\\b(?:\\:code|\\:markdown|markdown|exercise|example|agenda|summary)\\b\", \"\", etext, flags=re.I)\n",
    "        t = re.sub(r\"^\\s*[#>*+\\-`~\\u2013\\u2014\\_]*\\s*(?:\\d+[.)]\\s*)?\", \"\", t)\n",
    "        t = re.sub(r\"(?:`{3,}|~{3,}|_{3,}|-{3,})\", \" \", t)\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
    "        if t and not re.fullmatch(r\"[\\W_]+\", t):\n",
    "            if isinstance(e, dict):\n",
    "                ne = dict(e); ne['text'] = t\n",
    "            else:\n",
    "                ne = copy(e); setattr(ne, 'text', t)\n",
    "            out.append(ne)\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_lecture_elements(lecture_json_path: str) -> List[LectureElement]:\n",
    "    data = read_json(lecture_json_path)\n",
    "    # If already a list of sections with elements, flatten all\n",
    "    if isinstance(data, dict) and data.get(\"id\") is not None:\n",
    "        elements = _flatten_lecture_json(data)\n",
    "    elif isinstance(data, list):\n",
    "        elements = []\n",
    "        for x in data:\n",
    "            elements.extend(_flatten_lecture_json(x))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported lecture JSON format.\")\n",
    "\n",
    "    # Attach sequential chronological index\n",
    "    for i, el in enumerate(elements):\n",
    "        el.idx = i\n",
    "\n",
    "    # clean up the elements' text\n",
    "    cleaned_elements = clean_up_elements(elements)\n",
    "\n",
    "    return cleaned_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dad462-7e68-4a5f-a76b-639512705884",
   "metadata": {
    "id": "45dad462-7e68-4a5f-a76b-639512705884"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeeaad1-db06-4e0c-bde6-af35e6d6e67e",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492433660,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "eeeeaad1-db06-4e0c-bde6-af35e6d6e67e"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", use_sentence_tf: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.use_sentence_tf = use_sentence_tf and (_SENTENCE_TF is not None)\n",
    "        self.model = None\n",
    "        self.tfidf = None\n",
    "\n",
    "        if self.use_sentence_tf:\n",
    "            try:\n",
    "                self.model = _SENTENCE_TF(self.model_name)\n",
    "            except Exception as e:\n",
    "                logging.warning(\"SentenceTransformer init failed; falling back to TF-IDF: %s\", e)\n",
    "                self.use_sentence_tf = False\n",
    "\n",
    "        if not self.use_sentence_tf:\n",
    "            self.tfidf = TfidfVectorizer(max_features=4096)\n",
    "\n",
    "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.use_sentence_tf:\n",
    "            print(\"Use SentenceTransformer.\")\n",
    "            return np.asarray(self.model.encode(texts, normalize_embeddings=True))\n",
    "        else:\n",
    "            print(\"Use TFIDF to embed -> Check.\")\n",
    "            X = self.tfidf.fit_transform(texts)\n",
    "            # Normalize L2\n",
    "            X = X.astype(np.float32)\n",
    "            norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12\n",
    "            X = X.multiply(1/norms[:, None])\n",
    "            return X.toarray()\n",
    "\n",
    "    def transform(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.use_sentence_tf:\n",
    "            print(\"Use SentenceTransformer.\")\n",
    "            return np.asarray(self.model.encode(texts, normalize_embeddings=True))\n",
    "        else:\n",
    "            print(\"Use TFIDF to embed -> check.\")\n",
    "            X = self.tfidf.transform(texts)\n",
    "            # Normalize L2\n",
    "            X = X.astype(np.float32)\n",
    "            norms = np.sqrt((X.power(2)).sum(axis=1)).A1 + 1e-12\n",
    "            X = X.multiply(1/norms[:, None])\n",
    "            return X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d77d52-82e8-447b-aa8f-5c5109a16948",
   "metadata": {
    "id": "b8d77d52-82e8-447b-aa8f-5c5109a16948"
   },
   "source": [
    "## Building Distance Matrix for Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7791ae2-cbde-4dd0-ac84-ec4b59b9793a",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758492436381,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "e7791ae2-cbde-4dd0-ac84-ec4b59b9793a"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Distance Utilities for Lecture Notes\n",
    "# --------------------------------------\n",
    "\n",
    "def lcp_length(a: List[str], b: List[str]) -> int:\n",
    "    n = min(len(a), len(b))\n",
    "    i = 0\n",
    "    while i < n and a[i] == b[i]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def build_lecture_distance(elements: List[LectureElement],\n",
    "                           embeddings: np.ndarray,\n",
    "                           alpha_chron: float,\n",
    "                           alpha_logic: float,\n",
    "                           alpha_sem: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      D_L: fused NxN distance\n",
    "      D_chron, D_logic, D_sem: component distances\n",
    "    \"\"\"\n",
    "    N = len(elements)\n",
    "    idxs = np.array([el.idx for el in elements], dtype=float)\n",
    "    max_idx = max(1, int(np.max(idxs)))\n",
    "    # Chronological: normalized absolute index difference\n",
    "    D_chron = np.abs(idxs[:, None] - idxs[None, :]) / max(1.0, float(max_idx))\n",
    "\n",
    "    # Logic: 1 - normalized LCP length\n",
    "    # Precompute section paths\n",
    "    paths = [el.section_path for el in elements]\n",
    "    max_depth = max((len(p) for p in paths), default=1)\n",
    "    D_logic = np.zeros((N, N), dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            l = lcp_length(paths[i], paths[j])\n",
    "            d = 1.0 - (l / max(1.0, float(max_depth)))\n",
    "            D_logic[i, j] = D_logic[j, i] = d\n",
    "\n",
    "    # Semantic: 1 - cosine similarity (clipped)\n",
    "    S = cosine_similarity(embeddings)\n",
    "    D_sem = np.clip(1.0 - S, 0.0, 2.0)\n",
    "\n",
    "    D_L = alpha_chron * D_chron + alpha_logic * D_logic + alpha_sem * D_sem\n",
    "    # Normalize to [0,1]\n",
    "    D_L = (D_L - D_L.min()) / (D_L.max() - D_L.min() + 1e-12)\n",
    "    return D_L, D_chron, D_logic, D_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc738d4-bef8-4d4b-acd2-599b6ec2e752",
   "metadata": {
    "id": "ecc738d4-bef8-4d4b-acd2-599b6ec2e752"
   },
   "source": [
    "## Building Distance Matrix for Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9352e-3bec-41a7-813e-0562ff5302e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1758492447641,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "78e9352e-3bec-41a7-813e-0562ff5302e4"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Distance Utilities for Knowledge Graph\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "def build_kg_graph(kg: Dict[str, Any]) -> nx.Graph:\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for n in kg.get(\"nodes\", []):\n",
    "        G.add_node(n[\"id\"], **n)\n",
    "    for e in kg.get(\"edges\", []):\n",
    "        # Treat as undirected for structure metrics; keep relation on edge data\n",
    "        G.add_edge(e[\"source\"], e[\"target\"], **e)\n",
    "    return G\n",
    "\n",
    "def node_text_for_embedding(node: Dict[str, Any]) -> str:\n",
    "    parts = [node.get(\"label\", \"\"), node.get(\"definition\", \"\")]\n",
    "    aliases = node.get(\"aliases\") or []\n",
    "    if isinstance(aliases, list):\n",
    "        parts.extend(aliases[:3])\n",
    "    return \". \".join([p for p in parts if p]).strip()\n",
    "\n",
    "def build_kg_distance(kg: Dict[str, Any],\n",
    "                      embedder: Embedder,\n",
    "                      gamma_struct: float,\n",
    "                      gamma_sem: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:\n",
    "    nodes = kg.get(\"nodes\", [])\n",
    "    node_ids = [n[\"id\"] for n in nodes]\n",
    "    texts = [node_text_for_embedding(n) or n.get(\"label\", n[\"id\"]) for n in nodes]\n",
    "    X = embedder.transform(texts) if getattr(embedder, \"tfidf\", None) else embedder.fit_transform(texts)\n",
    "\n",
    "    # Semantic distance\n",
    "    S = cosine_similarity(X)\n",
    "    D_sem = np.clip(1.0 - S, 0.0, 2.0)\n",
    "\n",
    "    # Structural distance via shortest paths\n",
    "    G = build_kg_graph(kg)\n",
    "    # Precompute APSP lengths (unweighted)\n",
    "    # For disconnected pairs, set to large value\n",
    "    n = len(node_ids)\n",
    "    D_struct = np.full((n, n), fill_value=np.inf, dtype=float)\n",
    "    sp = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    for i, u in enumerate(node_ids):\n",
    "        D_struct[i, i] = 0.0\n",
    "        for j, v in enumerate(node_ids):\n",
    "            if v in sp.get(u, {}):\n",
    "                D_struct[i, j] = float(sp[u][v])\n",
    "    # Replace inf with max finite + 1\n",
    "    finite = D_struct[np.isfinite(D_struct)]\n",
    "    max_f = float(finite.max()) if finite.size else 1.0\n",
    "    D_struct[~np.isfinite(D_struct)] = max_f + 1.0\n",
    "\n",
    "    # Normalize each component to [0,1]\n",
    "    def _norm(M):\n",
    "        return (M - M.min()) / (M.max() - M.min() + 1e-12)\n",
    "    D_struct = _norm(D_struct)\n",
    "    D_sem = _norm(D_sem)\n",
    "\n",
    "    D_K = gamma_struct * D_struct + gamma_sem * D_sem\n",
    "    D_K = _norm(D_K)\n",
    "    return D_K, D_struct, D_sem, X, node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfe2bc-3dbd-4ed9-b20f-fbf58a14a3d3",
   "metadata": {
    "id": "ecbfe2bc-3dbd-4ed9-b20f-fbf58a14a3d3"
   },
   "source": [
    "## Normalized Measure and Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df13c92-119a-4882-9cc0-3a7d98bd5cb8",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492449943,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0df13c92-119a-4882-9cc0-3a7d98bd5cb8"
   },
   "outputs": [],
   "source": [
    "def normalized_measure(n: int) -> np.ndarray:\n",
    "    return np.full((n,), 1.0 / max(1, n), dtype=float)\n",
    "\n",
    "def degree_centrality_measure(kg: Dict[str, Any], node_ids: List[str]) -> np.ndarray:\n",
    "    G = build_kg_graph(kg)\n",
    "    deg = np.array([G.degree(nid) for nid in node_ids], dtype=float)\n",
    "    if deg.sum() <= 0:\n",
    "        return normalized_measure(len(node_ids))\n",
    "    return deg / deg.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367fbd-5bed-4056-852c-563cd33dc45f",
   "metadata": {
    "id": "fc367fbd-5bed-4056-852c-563cd33dc45f"
   },
   "source": [
    "## Compute FGW Distance and Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970a77e-98e4-4f98-9f40-213826187988",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1758492451848,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4970a77e-98e4-4f98-9f40-213826187988"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# FGW / Proxy Coupling\n",
    "# -----------------------------\n",
    "\n",
    "def compute_feature_cost(E_L: np.ndarray, E_K: np.ndarray) -> np.ndarray:\n",
    "    # Squared Euclidean between embeddings; both are L2-normalized, so this ~ 2 - 2*cosine\n",
    "    C = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "    # Normalize to [0,1]\n",
    "    C = (C - C.min()) / (C.max() - C.min() + 1e-12)\n",
    "    return C\n",
    "\n",
    "def local_structure_fingerprint(D: np.ndarray, k: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each row i in a distance matrix D (N x N), return a vector of the\n",
    "    sorted distances to its k nearest neighbors (excluding self).\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    fp = np.zeros((N, k), dtype=float)\n",
    "    for i in range(N):\n",
    "        row = D[i].copy()\n",
    "        row[i] = np.inf\n",
    "        idx = np.argsort(row)[:k]\n",
    "        fp[i] = np.sort(row[idx])\n",
    "    # Normalize per-column\n",
    "    fp = (fp - fp.min(axis=0, keepdims=True)) / (fp.max(axis=0, keepdims=True) - fp.min(axis=0, keepdims=True) + 1e-12)\n",
    "    return fp\n",
    "\n",
    "def compute_proxy_cost(D_L: np.ndarray, D_K: np.ndarray,\n",
    "                       E_L: np.ndarray, E_K: np.ndarray,\n",
    "                       lam_feat: float = 0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a surrogate cost for OT that mixes:\n",
    "      - feature cost between embeddings\n",
    "      - local structure fingerprints between spaces\n",
    "    \"\"\"\n",
    "    C_feat = compute_feature_cost(E_L, E_K)\n",
    "\n",
    "    # ensure the k is no greater than the mininum number of elements in\n",
    "    # either lecture notes or the kg\n",
    "    min_k = min(D_L.shape[0]-1, D_K.shape[0]-1)\n",
    "\n",
    "    fp_L = local_structure_fingerprint(D_L, k=min(min_k, max(1, D_L.shape[0]-1)))\n",
    "    fp_K = local_structure_fingerprint(D_K, k=min(min_k, max(1, D_K.shape[0]-1)))\n",
    "    #print(\"fp_L shape: {}\".format(fp_L.shape))\n",
    "    #print(\"fp_K shape: {}\".format(fp_K.shape))\n",
    "\n",
    "    # Structure penalty between local fingerprints\n",
    "    C_struct = cdist(fp_L, fp_K, metric=\"sqeuclidean\")\n",
    "    C_struct = (C_struct - C_struct.min()) / (C_struct.max() - C_struct.min() + 1e-12)\n",
    "    C = lam_feat * C_feat + (1.0 - lam_feat) * C_struct\n",
    "    C = (C - C.min()) / (C.max() - C.min() + 1e-12)\n",
    "    return C\n",
    "\n",
    "def sinkhorn_ot(mu: np.ndarray, nu: np.ndarray, C: np.ndarray, eps: float = 0.05, n_iter: int = 300) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple entropic OT via Sinkhorn-Knopp on kernel K = exp(-C/eps).\n",
    "    Returns coupling matrix P (mu x nu).\n",
    "    \"\"\"\n",
    "    K = np.exp(-C / max(1e-8, eps))\n",
    "    u = np.ones_like(mu)\n",
    "    v = np.ones_like(nu)\n",
    "    for _ in range(n_iter):\n",
    "        u = mu / (K @ v + 1e-12)\n",
    "        v = nu / (K.T @ u + 1e-12)\n",
    "    P = np.diag(u) @ K @ np.diag(v)\n",
    "    return P\n",
    "\n",
    "def fgw_distance_proxy(P: np.ndarray,\n",
    "                       D_L: np.ndarray, D_K: np.ndarray,\n",
    "                       E_L: np.ndarray, E_K: np.ndarray,\n",
    "                       lam_feat: float = 0.6) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute an FGW-like loss using current coupling P:\n",
    "      d = sum |D_L[i,k] - D_K[j,l]|^2 * P[i,j]*P[k,l] + lam_feat * sum ||E_L[i]-E_K[j]||^2 * P[i,j]\n",
    "    Returns:\n",
    "      total_loss, structure_term, feature_term\n",
    "    \"\"\"\n",
    "    # Structure term\n",
    "    DL = D_L\n",
    "    DK = D_K\n",
    "    # Efficient einsum formulation\n",
    "    # A[i,k] = D_L[i,k]^2\n",
    "    # B[j,l] = D_K[j,l]^2\n",
    "    # structure term = sum_{i,k,j,l} (D_L[i,k] - D_K[j,l])^2 * P[i,j] * P[k,l]\n",
    "    # = sum A[i,k]*P[i,:].sum_j P[k,:].sum_l + sum B[j,l]*P[:,j].sum_i P[:,l].sum_k - 2 * sum D_L[i,k] D_K[j,l] P[i,j] P[k,l]\n",
    "    A = DL ** 2\n",
    "    B = DK ** 2\n",
    "    Pi_row = P.sum(axis=1)  # size N\n",
    "    Pi_col = P.sum(axis=0)  # size M\n",
    "\n",
    "    term1 = (A * np.outer(Pi_row, Pi_row)).sum()\n",
    "    term2 = (B * np.outer(Pi_col, Pi_col)).sum()\n",
    "    # Cross term\n",
    "    term3 = 0.0\n",
    "    # Compute M1 = D_L @ P @ D_K.T @ P.T ??\n",
    "    # We need sum_{i,k,j,l} D_L[i,k] * D_K[j,l] * P[i,j] * P[k,l]\n",
    "    # This equals trace(D_L^T (P D_K P^T))\n",
    "    # Compute M = P @ D_K @ P.T  -> size N x N\n",
    "    M = P @ DK @ P.T\n",
    "    term3 = np.sum(DL * M)\n",
    "    structure = term1 + term2 - 2.0 * term3\n",
    "\n",
    "    # Feature term\n",
    "    # ||E_L[i] - E_K[j]||^2 = a + b - 2 <...>, but we can compute directly\n",
    "    C_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "    feature = float((C_feat * P).sum())\n",
    "    total = structure + lam_feat * feature\n",
    "    return total, structure, feature\n",
    "\n",
    "def compute_coupling_and_distance(D_L: np.ndarray, D_K: np.ndarray,\n",
    "                                  E_L: np.ndarray, E_K: np.ndarray,\n",
    "                                  mu: np.ndarray, nu: np.ndarray,\n",
    "                                  lam_feat: float, sink_eps: float, sink_iter: int,\n",
    "                                  use_pot: bool) -> Tuple[np.ndarray, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      P (coupling), total_fgw_like, structure_term, feature_term\n",
    "    \"\"\"\n",
    "    if use_pot and _HAS_POT:\n",
    "        print(\"Use POT.\")\n",
    "        # Exact FGW using POT\n",
    "        # Feature matrices are E_L and E_K; costs are cosine or Euclidean\n",
    "        # POT expects feature cost matrices; we give squared Euclidean\n",
    "        C1 = D_L\n",
    "        C2 = D_K\n",
    "        M_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "        # FGW coupling\n",
    "        #P = fused_gromov_wasserstein(M_feat, C1, C2, mu, nu, alpha=lam_feat,\n",
    "        # verbose=False)\n",
    "        # FGW loss\n",
    "        #total = float(fgw_loss(P, M_feat, C1, C2, lam_feat))\n",
    "\n",
    "        P, log = fused_gromov_wasserstein(M_feat, C1, C2, mu, nu,\n",
    "                                              loss_fun=\"square_loss\",\n",
    "                                               alpha=lam_feat, log=True)\n",
    "        total = float(log.get(\"fgw_dist\", np.nan))\n",
    "        # Decompose approximately (for reporting)\n",
    "        # We recompute proxy structure/feature with same P for interpretability\n",
    "        total_proxy, structure, feature = fgw_distance_proxy(P, D_L, D_K, E_L, E_K, lam_feat)\n",
    "        return P, total, structure, feature\n",
    "    else:\n",
    "        print(\"Use Proxy OT.\")\n",
    "        # Proxy: build surrogate cost and do entropic OT\n",
    "        C = compute_proxy_cost(D_L, D_K, E_L, E_K, lam_feat)\n",
    "        P = sinkhorn_ot(mu, nu, C, eps=sink_eps, n_iter=sink_iter)\n",
    "        total, structure, feature = fgw_distance_proxy(P, D_L, D_K, E_L, E_K, lam_feat)\n",
    "        return P, total, structure, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2128eb-6661-49fe-9f4d-7ea1f73b70ac",
   "metadata": {
    "id": "ce2128eb-6661-49fe-9f4d-7ea1f73b70ac"
   },
   "source": [
    "## LLM Assisted Concept and Relationship Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556005f-dc0a-4858-a462-89a64169a2a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1758492032686,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "e556005f-dc0a-4858-a462-89a64169a2a2",
    "outputId": "b4682cca-cad8-4970-df6c-8b805e172b35"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd7b4-d864-4e6a-9731-ac3e96b7bfaf",
   "metadata": {
    "id": "283fd7b4-d864-4e6a-9731-ac3e96b7bfaf"
   },
   "source": [
    "### LLM Helper OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e5ed0-9848-4fab-931a-449b698cc4bb",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492462199,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "226e5ed0-9848-4fab-931a-449b698cc4bb"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LLM helper (optional)\n",
    "# ---------------------------\n",
    "def llm_call(prompt: str, model: str = \"gpt-4o-mini\", max_tokens: int = 700, temperature: float = 0.2) -> str:\n",
    "    \"\"\"Call OpenAI chat completion. If no key or pkg, raises RuntimeError.\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    #api_key = userdata.get('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "    try:\n",
    "        import openai  # type: ignore\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAI client not available: {e}\")\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise assistant for ontology construction.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JUBuQPMJ5Fpw",
   "metadata": {
    "id": "JUBuQPMJ5Fpw"
   },
   "source": [
    "## LLM Helper Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac71ad-0998-44d9-baae-3f020ade8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "#api_key = userdata.get('GOOGLE_API_KEY') # Assuming this is how you access secrets\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GOOGLE_API_KEY not set\")\n",
    "\n",
    "try:\n",
    "    model: str = \"gemini-2.5-flash-lite\"\n",
    "    genai.configure(api_key=api_key)\n",
    "    # For chat-based models, you'd typically use genai.GenerativeModel\n",
    "    # and start a chat session. For simple prompt-response, direct generate_content works.\n",
    "    gemini_model = genai.GenerativeModel(model)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gemini client not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3XirqOqj5J6E",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758491891208,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "3XirqOqj5J6E"
   },
   "outputs": [],
   "source": [
    "def llm_call_gemini(prompt: str, model_instance=gemini_model,\n",
    "    max_output_tokens=1000, temperature=0.2) -> str:\n",
    "    \"\"\"Call Google Gemini chat completion. If no key or pkg, raises RuntimeError.\"\"\"\n",
    "    try:\n",
    "        # Gemini's generate_content takes a prompt directly\n",
    "        resp = model_instance.generate_content(\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [\n",
    "                    \"You are a precise assistant for ontology construction.\",\n",
    "                    prompt\n",
    "                ]}\n",
    "            ]\n",
    "        )\n",
    "        # Accessing the text content from the response\n",
    "        return resp.text.strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493400b7-2e3c-4d39-8345-7c5a93fd38bf",
   "metadata": {
    "id": "493400b7-2e3c-4d39-8345-7c5a93fd38bf"
   },
   "source": [
    "### LLM Concept Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36ebbf-c8ae-49d5-b7ca-2a1aa2021568",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758492466881,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9b36ebbf-c8ae-49d5-b7ca-2a1aa2021568"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# LLM concept naming\n",
    "# ---------------------------\n",
    "def make_concept_label_from_text_LLM(text: str) -> str:\n",
    "\n",
    "    print(\"Use LLM to name new concept.\")\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "        Generate a single, meaningful concept that represents\n",
    "        the main idea of the TEXT.\n",
    "        The concept should be specific\n",
    "        and represents the main phrase and terms in the TEXT.\\n\n",
    "        return ONLY the concept name. \\n\n",
    "        If the TEXT provided is empty and does not contain any content to\n",
    "        derive a concept from, return ''.\\n\n",
    "        TEXT: {}.\\n\\n\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    #name = llm_call(prompt.format(text)).strip()\n",
    "\n",
    "    name = llm_call_gemini(prompt.format(text)).strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "def make_edges_between_text_LLM(kg: Dict[str, Any], new_node,\n",
    "                               allowed_relations: List[str]):\n",
    "\n",
    "    print(\"Use LLM to add new edges for a new concept.\")\n",
    "\n",
    "    \"\"\"\n",
    "    Uses LLM to find and add the most likely relation(s) between a new node\n",
    "    and existing nodes in the KG, based on concept proximity and allowed relations.\n",
    "    This version attempts to make a single LLM call for efficiency.\n",
    "    \"\"\"\n",
    "    new_node_info = f\"New Concept: \\\"{new_node['label']}\\\" (Definition: \\\"{new_node['definition']}\\\")\"\n",
    "    existing_nodes_info = \"\\nExisting Concepts:\\n\" + \"\\n\".join([\n",
    "        f\"- \\\"{n['label']}\\\" (Definition: \\\"{n.get('definition', '')}\\\") [ID: {n['id']}]\"\n",
    "        for n in kg[\"nodes\"] if n[\"id\"] != new_node['id']\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Given the new node and existing nodes below, identify the most likely relationship(s)\n",
    "    from the NEW NODE to each EXISTING NODE from the following list of allowed relations:\n",
    "    {', '.join(allowed_relations)}.\n",
    "    For each relevant relationship, output the original EXISTING NODE's ID, the relation name, and a\n",
    "    probability score between 0 and 1, and rationale about adding this edge,\n",
    "    separated by colons, one relationship per line.\n",
    "    Example output format:\n",
    "    [EXISTING_NODE_ID]: [RELATION_NAME]: [PROBABILITY]: [RATIONALE]\n",
    "    [EXISTING_NODE_ID]: [RELATION_NAME]: [PROBABILITY]: [RATIONALE]\n",
    "    ...\n",
    "\n",
    "    If no suitable relation is found for an existing concept with a high probability,\n",
    "    do not include it in the output.\n",
    "\n",
    "    {new_node_info}\n",
    "    {existing_nodes_info}\n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "\n",
    "    candidate_edges = []\n",
    "    try:\n",
    "        #llm_response = llm_call(prompt, max_tokens=5000).strip() # Increase max_tokens\n",
    "\n",
    "        llm_response = llm_call_gemini(prompt, max_output_tokens=5000).strip() # Increase max_tokens\n",
    "        # for potentially longer output Parse the LLM response\n",
    "        for line in llm_response.splitlines():\n",
    "            parts = line.split(':')\n",
    "            if len(parts) == 4:\n",
    "                target_id = parts[0].strip()\n",
    "                llm_relation = parts[1].strip()\n",
    "                \n",
    "                try:\n",
    "                    llm_confidence = float(parts[2].strip())\n",
    "                except ValueError:\n",
    "                    llm_confidence = 0.5 # Default confidence if parsing fails\n",
    "                \n",
    "                llm_rationale = parts[3].strip()\n",
    "                \n",
    "                # Validate target_id and relation\n",
    "                if any(n[\"id\"].lower() == target_id.lower() for n in kg[\"nodes\"]) and llm_relation in allowed_relations:\n",
    "                    \n",
    "                    # get edge defintion\n",
    "                    e_def = get_edge_definition(kg, new_node['id'], target_id, llm_relation)\n",
    "                    \n",
    "                    candidate_edges.append({\n",
    "                        \"source\": new_node['id'],\n",
    "                        \"target\": target_id,\n",
    "                        \"relation\": llm_relation,\n",
    "                        \"definition\": e_def,\n",
    "                        \"confidence\": llm_confidence,\n",
    "                        \"rationale\": llm_rationale\n",
    "                    })\n",
    "                else:\n",
    "                     print(f\"Warning: Invalid target_id or relation from LLM: {line}.\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"LLM call failed for relation prediction: {e}\")\n",
    "        # If LLM fails, add a default 'relatedTo' edge to all existing nodes with low confidence\n",
    "        # This is a fallback to ensure some connectivity, adjust confidence as needed.\n",
    "        # for existing_node in kg[\"nodes\"]:\n",
    "        #     if existing_node[\"id\"] != new_node['id']:\n",
    "        #         candidate_edges.append({\n",
    "        #             \"source\": new_node['id'],\n",
    "        #             \"target\": existing_node[\"id\"],\n",
    "        #             \"relation\": \"relatedTo\",\n",
    "        #             \"confidence\": 0.2\n",
    "        #         })\n",
    "\n",
    "\n",
    "    return candidate_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047dfca-b76c-4234-bdf6-50c9766ee24d",
   "metadata": {},
   "source": [
    "## Task-Oriented All-Allowed Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1830b-975b-4b70-81e3-27f69627fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ALLOWED_RELATIONS_DEFAULT = [\n",
    "    \"isA\",\"partOf\",\"prerequisiteOf\",\"dependsOn\",\"relatedTo\",\"synonymOf\",\"antonymOf\",\n",
    "    \"contrastsWith\",\"defines\",\"uses\",\"usedBy\",\"appliesTo\",\"exampleOf\",\"counterexampleOf\",\n",
    "    \"illustratedBy\",\"causes\",\"resultsIn\",\"prevents\",\"assumes\",\"implies\",\"equivalentTo\",\n",
    "    \"parameterOf\",\"hasParameter\",\"propertyOf\",\"hasProperty\",\"measuredBy\",\"unitOf\",\n",
    "    \"representedBy\",\"notationFor\",\"formulaFor\",\"provedBy\",\"theoremOf\",\"algorithmFor\",\n",
    "    \"stepOf\",\"produces\",\"consumes\",\"advantageOf\",\"limitationOf\",\"commonErrorIn\",\n",
    "    \"misconceptionOf\",\"commonlyConfusedWith\",\"assessedBy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a070a9-cded-45b7-bebf-3197de89e956",
   "metadata": {
    "id": "f7a070a9-cded-45b7-bebf-3197de89e956"
   },
   "source": [
    "## TF-IDF Top Terms Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64891394-008f-4012-acf2-8c573044a1c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1758492480176,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0d6d7151-8c33-4b08-8b66-92ed58a5173d"
   },
   "outputs": [],
   "source": [
    "def tfidf_top_terms(texts: List[str], topk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return the top-k terms by mean TF-IDF across documents.\n",
    "    Robust to empty/stopword-only inputs and None/NaN values.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Coerce to strings and strip\n",
    "    cleaned = [(t if isinstance(t, str) else \"\").strip() for t in texts]\n",
    "    if all(not t for t in cleaned):\n",
    "        return []\n",
    "\n",
    "    # First attempt: standard English stopwords\n",
    "    vec = TfidfVectorizer(max_features=4096, stop_words=\"english\")\n",
    "    try:\n",
    "        X = vec.fit_transform(cleaned)\n",
    "    except ValueError:\n",
    "\n",
    "        return []\n",
    "\n",
    "    # Mean TF-IDF per term across docs\n",
    "    scores = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "    # Top-k indices (safe if topk > n_features)\n",
    "    k = min(topk, scores.size)\n",
    "    if k == 0:\n",
    "        return []\n",
    "\n",
    "    # Use argpartition for efficiency, then sort those k by score desc\n",
    "    top_idx = np.argpartition(-scores, range(k))[:k]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    return [feature_names[i] for i in top_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17876fc3-957a-46c3-a539-1304ae6c7d50",
   "metadata": {
    "id": "17876fc3-957a-46c3-a539-1304ae6c7d50"
   },
   "source": [
    "## Rate, KL_Divergence, Coupling_Entropy, and Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35303cd9-7e77-4085-a0da-6dcd4098c6cb",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758492482056,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "35303cd9-7e77-4085-a0da-6dcd4098c6cb"
   },
   "outputs": [],
   "source": [
    "def rate_complexity(kg: Dict[str, Any]) -> float:\n",
    "    n = len(kg.get(\"nodes\", []))\n",
    "    m = len(kg.get(\"edges\", []))\n",
    "    return float(n) + 0.5 * float(m)\n",
    "\n",
    "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    # Symmetrized KL\n",
    "    p = p + 1e-12\n",
    "    q = q + 1e-12\n",
    "    kl_pq = float(np.sum(rel_entr(p, q)))\n",
    "    kl_qp = float(np.sum(rel_entr(q, p)))\n",
    "    return 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "def coupling_entropy(col: np.ndarray) -> float:\n",
    "    p = col / (col.sum() + 1e-12)\n",
    "    p = p + 1e-12\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "def compute_objective(kg: Dict[str, Any], beta: float, fgw_total: float) -> float:\n",
    "    return rate_complexity(kg) + beta * fgw_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62168b-c747-4a2b-9770-1b669ddae3ce",
   "metadata": {
    "id": "8a62168b-c747-4a2b-9770-1b669ddae3ce"
   },
   "source": [
    "## KG Refinement Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50196bbf-72a9-4458-a8e0-f9795e1ab174",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1758492485841,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "50196bbf-72a9-4458-a8e0-f9795e1ab174"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# KG Refinement Operations\n",
    "# -----------------------------\n",
    "\n",
    "\"\"\"\n",
    "def make_concept_label_from_text(text: str) -> str:\n",
    "    # Simple heuristic: TF-IDF top terms or fallback to first 5 words\n",
    "    toks = re.findall(r\"[A-Za-z_][A-Za-z0-9_]+\", text)[:20]\n",
    "    if len(toks) >= 3:\n",
    "        return \" \".join(toks[:3]).lower()\n",
    "    return (text[:30] + \"...\").strip()\n",
    "\"\"\"\n",
    "\n",
    "def add_concept_from_element(kg: Dict[str, Any],\n",
    "                             element: LectureElement,\n",
    "                             embedder: Embedder,\n",
    "                             allowed_relations: List[str],\n",
    "                             connect_to: Optional[str] = None,\n",
    "                             relation: str = \"relatedTo\") -> KGNode:\n",
    "    label = make_concept_label_from_text_LLM(element.text)\n",
    "\n",
    "    if label != \"''\" and label != '\"\"' or (not label):\n",
    "        node_id = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", label).strip(\"_\")\n",
    "        if not node_id:\n",
    "            node_id = f\"new_node_{len(kg['nodes'])+1}\"\n",
    "        # Ensure unique\n",
    "        base = node_id\n",
    "        c = 1\n",
    "        existing_ids = {n[\"id\"] for n in kg[\"nodes\"]}\n",
    "        while node_id in existing_ids:\n",
    "            c += 1\n",
    "            node_id = f\"{base}_{c}\"\n",
    "        node = {\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": \"Concept\",\n",
    "            \"definition\": element.text,\n",
    "            \"aliases\": [],\n",
    "            \"attributes\": {},\n",
    "            \"provenance\": [{\n",
    "                \"section_path\": element.section_path,\n",
    "                \"line_start\": element.idx,\n",
    "                \"line_end\": element.idx,\n",
    "                \"text_excerpt\": element.text\n",
    "            }],\n",
    "            \"confidence\": 0.6,\n",
    "            \"rationale\": \"Added to reduce distortion; under-represented lecture content.\"\n",
    "        }\n",
    "        kg[\"nodes\"].append(node)\n",
    "\n",
    "        # Use LLM to make edges for the new node\n",
    "        candidate_edges = make_edges_between_text_LLM(kg, node, allowed_relations)\n",
    "\n",
    "        edges = []\n",
    "\n",
    "        # Rank candidate edges by confidence and add the top one(s)\n",
    "        if candidate_edges:\n",
    "            candidate_edges.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "            top_confidence = candidate_edges[0][\"confidence\"]\n",
    "            edges_to_add = [\n",
    "                # Consider ties within 5%\n",
    "                edge for edge in candidate_edges if edge[\"confidence\"] >= top_confidence * 0.95\n",
    "            ]\n",
    "\n",
    "            for edge_info in edges_to_add:\n",
    "                edge = {\n",
    "                    \"id\": f\"e_{edge_info['source']}_{edge_info['relation']}_{edge_info['target']}\",\n",
    "                    \"source\": edge_info['source'],\n",
    "                    \"target\": edge_info['target'],\n",
    "                    \"relation\": edge_info['relation'],\n",
    "                    \"definition\": edge_info['definition'],\n",
    "                    \"provenance\": [{\n",
    "                        \"section_path\": element.section_path,\n",
    "                        \"line_start\": element.idx,\n",
    "                        \"line_end\": element.idx,\n",
    "                        \"text_excerpt\": element.text\n",
    "                    }],\n",
    "                    \"confidence\": edge_info['confidence'],\n",
    "                    \"rationale\": edge_info['rationale']\n",
    "                }\n",
    "                # Avoid duplicates\n",
    "                edge_sig = (edge[\"source\"].lower(), edge[\"target\"].lower(), edge[\"relation\"].lower())\n",
    "                if not any((e[\"source\"].lower(), e[\"target\"].lower(), e[\"relation\"].lower()) == edge_sig for e in kg[\"edges\"]):\n",
    "                    kg[\"edges\"].append(edge)\n",
    "                    edges.append(edge)\n",
    "        else:\n",
    "            if connect_to and relation in allowed_relations:\n",
    "                # get edge definition\n",
    "                e_def = get_edge_definition(kg, node_id, connect_to, relation)\n",
    "                \n",
    "                edge = {\n",
    "                    \"id\": f\"e_{node_id}_{relation}_{connect_to}\",\n",
    "                    \"source\": node_id,\n",
    "                    \"target\": connect_to,\n",
    "                    \"relation\": relation,\n",
    "                    \"definition\": e_def,\n",
    "                    \"provenance\": [{\n",
    "                        \"section_path\": element.section_path,\n",
    "                        \"line_start\": element.idx,\n",
    "                        \"line_end\": element.idx,\n",
    "                        \"text_excerpt\": element.text\n",
    "                    }],\n",
    "                    \"confidence\": 0.6,\n",
    "                    \"rationale\": \"Heuristic relation based on semantic proximity.\"\n",
    "                }\n",
    "                # Avoid duplicates\n",
    "                edge_sig = (edge[\"source\"].lower(), edge[\"target\"].lower(), edge[\"relation\"].lower())\n",
    "                if not any((e[\"source\"].lower(), e[\"target\"].lower(), e[\"relation\"].lower()) == edge_sig for e in kg[\"edges\"]):\n",
    "                    kg[\"edges\"].append(edge)\n",
    "                    edges.append(edge)\n",
    "\n",
    "        return KGNode(**node), edges\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def kmeans_split_concept(col: np.ndarray,\n",
    "                         E_L: np.ndarray,\n",
    "                         elements: List[LectureElement],\n",
    "                         kg: Dict[str, Any],\n",
    "                         node_idx: int) -> Tuple[Optional[KGNode], Optional[KGNode], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Split a concept into two based on the embedding clusters of its coupled\n",
    "    lecture elements.\n",
    "    Returns new nodes (as dicts) and index sets assigned to each.\n",
    "    \"\"\"\n",
    "    weights = col\n",
    "    sel = np.where(weights > weights.mean())[0]\n",
    "    if len(sel) < 4:\n",
    "        return None, None, [], []\n",
    "    X = E_L[sel]\n",
    "    try:\n",
    "        km = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "        labs = km.fit_predict(X)\n",
    "    except Exception:\n",
    "        return None, None, [], []\n",
    "\n",
    "    node = kg[\"nodes\"][node_idx]\n",
    "    base_id = node[\"id\"]\n",
    "\n",
    "    a_idx = [int(sel[i]) for i in range(len(sel)) if labs[i] == 0]\n",
    "    b_idx = [int(sel[i]) for i in range(len(sel)) if labs[i] == 1]\n",
    "\n",
    "    # Concatenate the text of a_idx and b_idx\n",
    "    text_a = \"\\n\".join([elements[i].text for i in a_idx])\n",
    "    text_b = \"\\n\".join([elements[i].text for i in b_idx])\n",
    "\n",
    "    # Use LLM to make a label\n",
    "    label_a = make_concept_label_from_text_LLM(text_a)\n",
    "    if label_a == \"''\" or label_a == '\"\"' or (not label_a):\n",
    "        nA_txt = tfidf_top_terms([elements[i].text for i in a_idx], topk=3)\n",
    "        label_a = (node[\"label\"] + \" \" + \" \".join(nA_txt)).strip()\n",
    "\n",
    "    label_b = make_concept_label_from_text_LLM(text_b)\n",
    "    if label_b == \"''\" or label_b == '\"\"' or (not label_b):\n",
    "        nB_txt = tfidf_top_terms([elements[i].text for i in b_idx], topk=3)\n",
    "        label_b = (node[\"label\"] + \" \" + \" \".join(nB_txt)).strip()\n",
    "\n",
    "    def _new(name_suffix, label, txt):\n",
    "\n",
    "        new_id = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", label).strip(\"_\")\n",
    "        \n",
    "        if not new_id:\n",
    "            new_id = f\"{base_id}_{name_suffix}\"\n",
    "            \n",
    "        # Ensure unique\n",
    "        base = new_id\n",
    "        c = 1\n",
    "        existing_ids = {n[\"id\"] for n in kg[\"nodes\"]}\n",
    "        while new_id in existing_ids:\n",
    "            c += 1\n",
    "            new_id = f\"{base}_{c}\"\n",
    "        \n",
    "        new_node = dict(node)\n",
    "        new_node[\"id\"] = new_id\n",
    "        new_node[\"label\"] = label\n",
    "        new_node[\"confidence\"] = 0.55\n",
    "        new_node[\"definition\"] = txt\n",
    "        new_node[\"provenance\"][0]['text_excerpt'] = txt\n",
    "        new_node[\"rationale\"] = \"Split concept with diverse coupling.\"\n",
    "        return new_node\n",
    "\n",
    "    A = _new(\"a\", label_a, text_a)\n",
    "    B = _new(\"b\", label_b, text_b)\n",
    "    return KGNode(**A), KGNode(**B), a_idx, b_idx\n",
    "\n",
    "def merge_if_redundant(i: int, j: int,\n",
    "                       P: np.ndarray,\n",
    "                       E_K: np.ndarray,\n",
    "                       sim_thresh: float,\n",
    "                       kl_thresh: float) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if concepts i and j should be merged (high semantic sim & low KL).\n",
    "    \"\"\"\n",
    "    v1 = E_K[i]; v2 = E_K[j]\n",
    "    cos = float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12))\n",
    "    if cos < sim_thresh:\n",
    "        return False\n",
    "    pi = P[:, i]; pj = P[:, j]\n",
    "    kl = kl_divergence(pi, pj)\n",
    "    return kl < kl_thresh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b259b3-845f-4a46-93fa-c434a4d33827",
   "metadata": {},
   "source": [
    "## KG Final Refinement by Domain Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fbcd4-db1e-4b4a-9aa1-4de827a96f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_json_list(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return a JSON string representing a list.\n",
    "    Handles raw JSON or fenced code blocks like:\n",
    "    ```json\n",
    "    [ ... ]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Try direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, list):\n",
    "            return json.dumps(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Look for fenced block ```json ... ``` or ``` ...\n",
    "    fence_match = re.search(r\"```(?:json)?\\s*(\\[.*?\\])\\s*```\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if fence_match:\n",
    "        return fence_match.group(1)\n",
    "\n",
    "    # Fallback: capture first bracketed list (greedy but safe-ish)\n",
    "    bracket_match = re.search(r\"(\\[.*\\])\", text, flags=re.DOTALL)\n",
    "    if bracket_match:\n",
    "        return bracket_match.group(1)\n",
    "\n",
    "    raise ValueError(\"No JSON array found in LLM output.\")\n",
    "\n",
    "def apply_domain_knowledge_LLM(kg: Dict[str, Any], allowed_relations: List[str]) -> List[KGEdge]:\n",
    "    \"\"\"\n",
    "    Refine the knowledge graph by adding missing edges based on domain knowledge\n",
    "    encoded in the KG, using an LLM guided by a structured prompt.\n",
    "\n",
    "    Args:\n",
    "        kg: Knowledge graph as a dict with nodes and edges.\n",
    "        allowed_relations: List of allowed relation types.\n",
    "\n",
    "    Returns:\n",
    "        A list of KGEdge objects proposed by the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert Knowledge Graph Refinement Agent.\n",
    "    Your goal is to propose NEW edges that make it easier to generate high-quality\n",
    "    multiple-choice questions (MCQs) that produce\n",
    "    high-quality distractors and har-to-guess answers. \n",
    "\n",
    "    ### Inputs\n",
    "    1. Knowledge Graph (KG): {kg_json}\n",
    "    2. Allowed Relations: {allowed_relations}\n",
    "\n",
    "    ### Task Focus\n",
    "    - Prioritize the edges that help build strong distractors.\n",
    "    - Typical contrastive relations include: contrastsWith, antonymOf, differentFrom, opposes, distinguishesFrom, versus/vs.\n",
    "    - Only use relation types that are in the Allowed Relations list.\n",
    "\n",
    "    ### Evidence Constraints\n",
    "    - Base your proposals STRICTLY on domain knowledge already encoded in the KG\n",
    "      (node labels/definitions, existing relations, hierarchies, examples, contrasts).\n",
    "    - Do NOT invent facts beyond what the KG implies.\n",
    "\n",
    "    ### Quality Bar (MCQ Utility)\n",
    "    For each proposed edge:\n",
    "    - It should enable question writers to craft MCQs where wrong options \n",
    "    are **plausible** yet **incorrect**.\n",
    "    - Prefer pairs that are commonly confused or that share overlapping \n",
    "      properties but differ on key dimensions.\n",
    "    - Avoid trivial or redundant contrasts.\n",
    "    \n",
    "    ### Output\n",
    "    - A list of new edges in the following structure:\n",
    "    e = {{\n",
    "        \"id\": \"<unique_id>\",\n",
    "        \"source\": \"<src_node_id>\",\n",
    "        \"target\": \"<dst_node_id>\",\n",
    "        \"relation\": \"<relation_type>\",\n",
    "        \"definition\": \"<edge_definition>\",\n",
    "        \"provenance\": [],\n",
    "        \"confidence\": <number between 0 and 1>,\n",
    "        \"rationale\": \"<explanation of why this relation was added>\"\n",
    "    }}\n",
    "\n",
    "    Do not repeat existing edges. Only include new, refined edges.\n",
    "    Ensure src_node_id and dst_node_id are existing nodes in the knowledge graph.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Chain-of-Thought Refinement Instructions\n",
    "\n",
    "    1. **Deconstruct the KG**\n",
    "       - Read all nodes and existing edges.\n",
    "       - Note the domain knowledge explicitly encoded in the graph \n",
    "         (hierarchies, prerequisites, usage patterns, examples, contrasts).\n",
    "\n",
    "    2. **Diagnose Gaps**\n",
    "       - Identify pairs of nodes that appear semantically, hierarchically, or logically related but lack an explicit relation.\n",
    "       - Use only domain knowledge present in the KG (structure, relations, node labels) to justify candidate edges.\n",
    "\n",
    "    3. **Develop Candidate Edges**\n",
    "       - For each missing link:\n",
    "         - Choose the most appropriate relation type from the allowed set.\n",
    "         - Ensure the edge does not already exist.\n",
    "\n",
    "    4. **Reason Explicitly**\n",
    "       - For each new edge, provide a rationale:\n",
    "         - Why the two nodes are connected.\n",
    "         - Why this relation type is correct.\n",
    "         - Why the confidence score (0–1) is appropriate.\n",
    "\n",
    "       Example reasoning:\n",
    "       Nodes: \"Linear Regression\" → \"Gradient Descent\"\n",
    "       Evidence: In the KG, Gradient Descent is used to optimize models; Linear Regression requires optimization.\n",
    "       Relation: uses\n",
    "       Confidence: 0.9\n",
    "       Rationale: Domain knowledge in the KG indicates optimization is part of regression training.\n",
    "\n",
    "    5. **Deliver Final List**\n",
    "       - Output only the list of new edge objects in the specified structure.\n",
    "       - Each edge must include id, source_id, target_id, relation, provenance, confidence, rationale.\n",
    "       - Ensure the source_id and target_id are in the knowledge graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the placeholders\n",
    "    prompt = prompt_template.format(\n",
    "        kg_json=json.dumps(kg, indent=2),\n",
    "        allowed_relations=allowed_relations\n",
    "    )\n",
    "\n",
    "    # ---- Call your LLM here ----\n",
    "    # Replace this with your actual LLM call (OpenAI, Anthropic, etc.)\n",
    "    # For example, if using OpenAI:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    # )\n",
    "    # raw_output = response.choices[0].message[\"content\"]\n",
    "\n",
    "    #raw_output = llm_call(prompt)  # <-- Replace with your LLM wrapper\n",
    "\n",
    "    raw_output = llm_call_gemini(prompt, max_output_tokens=50000)\n",
    "\n",
    "    # Try parsing JSON safely\n",
    "    json_str = _extract_json_list(raw_output)\n",
    "    try:\n",
    "        edges: List[KGEdge] = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"LLM output could not be parsed as JSON:\\n\" + json_str)\n",
    "        return None\n",
    "\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf143c-4ad9-4d3f-97a0-4ff9214c5c02",
   "metadata": {},
   "source": [
    "## Add the Edges Refined by Domain Knowledge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74308fab-12d4-4055-832a-e435ae72fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_refined_domain_edges(kg: Dict[str, Any], edges: List[KGEdge]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - Ensure each edge's source/target node exists by id; if not, create a new node.\n",
    "    - New nodes follow your requested schema and use the edge's rationale.\n",
    "    - Append normalized edges to kg['edges'] (avoid duplicates).\n",
    "    \"\"\"\n",
    "\n",
    "    added_node_counter = 0\n",
    "    added_edge_counter = 0\n",
    "    \n",
    "    kg.setdefault(\"nodes\", [])\n",
    "    kg.setdefault(\"edges\", [])\n",
    "\n",
    "    def node_exists(node_id: str) -> bool:\n",
    "        nid = node_id.lower()\n",
    "        return any(str(n.get(\"id\", \"\")).lower() == nid for n in kg[\"nodes\"])\n",
    "\n",
    "    def add_node_from_edge(node_id: str, edge: KGEdge) -> None:\n",
    "        kg[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": node_id,  # per requirement: use id for label\n",
    "            \"type\": \"Concept\",\n",
    "            \"definition\": edge.get(\"rationale\", \"\") or \"\",\n",
    "            \"aliases\": [],  # you didn't request aliases here\n",
    "            \"provenance\": [{\"text\": edge.get(\"rationale\", \"\") or \"\"}],\n",
    "            \"attributes\": {},\n",
    "            \"confidence\": float(edge.get(\"confidence\", 0.5) or 0.5),\n",
    "            \"rationale\": \"Add unmatched node by domain knowledge from LLM\"\n",
    "        })\n",
    "\n",
    "    def edge_exists(src_id: str, rel: str, dst_id: str) -> bool:\n",
    "        s, r, t = src_id.lower(), rel, dst_id.lower()\n",
    "        return any(\n",
    "            str(e.get(\"source\", \"\")).lower() == s and\n",
    "            str(e.get(\"relation\", \"\")) == r and\n",
    "            str(e.get(\"target\", \"\")).lower() == t\n",
    "            for e in kg[\"edges\"]\n",
    "        )\n",
    "\n",
    "    for e in edges:\n",
    "        rel = str(e.get(\"relation\", \"\")).strip()\n",
    "        if not rel:\n",
    "            continue\n",
    "\n",
    "        src_id = str(e.get(\"source\", \"\")).strip()\n",
    "        dst_id = str(e.get(\"target\", \"\")).strip()\n",
    "        if not src_id or not dst_id:\n",
    "            continue\n",
    "\n",
    "        e_def = str(e.get(\"definition\", \"\")).strip()\n",
    "        if not e_def:\n",
    "            continue\n",
    "\n",
    "        if not node_exists(src_id):\n",
    "            add_node_from_edge(src_id, e)\n",
    "            added_node_counter += 1\n",
    "        if not node_exists(dst_id):\n",
    "            add_node_from_edge(dst_id, e)\n",
    "            added_node_counter += 1\n",
    "\n",
    "        # Normalize provenance to list[dict] with \"text\" if needed\n",
    "        norm_prov = []\n",
    "        norm_prov.append({\"text_excerpt\": e.get(\"rationale\", \"\")})\n",
    "\n",
    "        if not edge_exists(src_id, rel, dst_id):\n",
    "            kg[\"edges\"].append({\n",
    "                \"id\": str(e.get(\"id\", \"\")) or f\"e_{abs(hash(src_id + '::' + rel + '::' + dst_id))}\",\n",
    "                \"source\": src_id,\n",
    "                \"target\": dst_id,\n",
    "                \"relation\": rel,\n",
    "                \"definition\": e_def,\n",
    "                \"provenance\": norm_prov,\n",
    "                \"confidence\": float(e.get(\"confidence\", 0.5) or 0.5),\n",
    "                \"rationale\": str(e.get(\"rationale\", \"\")).strip(),\n",
    "            })\n",
    "\n",
    "            added_edge_counter += 1\n",
    "\n",
    "    return added_node_counter, added_edge_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dacc1-f8d7-4a74-96aa-a1ac4ad2daf4",
   "metadata": {
    "id": "a11dacc1-f8d7-4a74-96aa-a1ac4ad2daf4"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11821f4-7e9b-4450-bf9e-2dfce2fd03d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1758492492893,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "d11821f4-7e9b-4450-bf9e-2dfce2fd03d3",
    "outputId": "035ca342-e805-4a37-8b5f-0349acf62e19"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "lecture_json_path = \"./data/lecture_notes_8.json\" #type=str, required=True, help=\"Path to lecture notes JSON\")\n",
    "kg_json_path = \"./data/kg8.json\" #type=str, required=True, help=\"Path to initial KG JSON\")\n",
    "out_dir = \"./data/output\" #type=str, required=True, help=\"Output directory\")\n",
    "#beta = 20.0 #type=float, default=20.0)\n",
    "beta = 100.0\n",
    "alpha = \"0.2,0.3,0.5\" #help=\"alpha weights for lecture distance (chron,logic,semantic)\"\n",
    "gamma = \"0.4,0.6\" #help=\"gamma weights for KG distance (struct,semantic)\")\n",
    "lambda_feat = 0.6 #help=\"FGW feature balance (alpha in POT)\")\n",
    "max_iterations = 10\n",
    "convergence_threshold = 0.01\n",
    "#theta_add = 0.06\n",
    "theta_add = 0.02\n",
    "theta_split = 0.35\n",
    "theta_merge = 0.12\n",
    "theta_relate = 0.25\n",
    "sinkhorn_eps = 0.05\n",
    "sinkhorn_iter = 300\n",
    "disable_domain_rules = True\n",
    "no_pot =  False #help=\"Disable POT even if installed (use proxy)\")\n",
    "seed = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    a_chron, a_logic, a_sem = [float(x) for x in alpha.split(\",\")]\n",
    "    g_struct, g_sem = [float(x) for x in gamma.split(\",\")]\n",
    "except Exception:\n",
    "    raise SystemExit(\"Please provide --alpha like '0.2,0.3,0.5' and --gamma like '0.4,0.6'\")\n",
    "\n",
    "hp = HyperParams(\n",
    "    alpha_chron=a_chron,\n",
    "    alpha_logic=a_logic,\n",
    "    alpha_sem=a_sem,\n",
    "    gamma_struct=g_struct,\n",
    "    gamma_sem=g_sem,\n",
    "    lambda_feat=lambda_feat,\n",
    "    beta=beta,\n",
    "    theta_add=theta_add,\n",
    "    theta_split=theta_split,\n",
    "    theta_merge=theta_merge,\n",
    "    theta_relate=theta_relate,\n",
    "    max_iterations=max_iterations,\n",
    "    convergence_threshold=convergence_threshold,\n",
    "    sinkhorn_eps=sinkhorn_eps,\n",
    "    sinkhorn_iter=sinkhorn_iter\n",
    ")\n",
    "\n",
    "use_pot = (not no_pot) and _HAS_POT\n",
    "if use_pot:\n",
    "    print(\"Using POT for exact FGW: %s\", use_pot)\n",
    "else:\n",
    "    print(\"Use POT proxy.\")\n",
    "\n",
    "ensure_dir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ade94-15f0-46a8-be2a-08549972c746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1758492495401,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "389ade94-15f0-46a8-be2a-08549972c746",
    "outputId": "a18c4d7e-c980-4b3e-aac0-ad14ffa11a82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elements = parse_lecture_elements(lecture_json_path)\n",
    "#elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dd8a3-6543-4098-8173-5e466fcc263b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1758492502140,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "151dd8a3-6543-4098-8173-5e466fcc263b",
    "outputId": "d201c277-eda5-4cda-e10e-038693c54243"
   },
   "outputs": [],
   "source": [
    "# Build embeddings (fit once on lecture + KG text to align spaces for TF-IDF fallback)\n",
    "embedder = Embedder()\n",
    "lecture_texts = [e.text for e in elements]\n",
    "E_L = embedder.fit_transform(lecture_texts)\n",
    "D_L, D_chron, D_logic, D_semL = build_lecture_distance(\n",
    "    elements, E_L, hp.alpha_chron, hp.alpha_logic, hp.alpha_sem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31395fee-df6e-4212-8a22-4dc1e27f83a1",
   "metadata": {},
   "source": [
    "## Refinement Starts Here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d976e-6fa2-4e20-a316-f57aa7c078da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_history = {} # store the individual knowledge graph at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255877a1-b14c-4eba-897f-0b1f712dc917",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758492509499,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "255877a1-b14c-4eba-897f-0b1f712dc917",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load inputs\n",
    "#kg = read_json(kg_json_path)\n",
    "kg = read_kg(kg_json_path)\n",
    "\n",
    "if \"allowed_relations\" not in kg.get(\"meta\", {}):\n",
    "    if \"meta\" not in kg:\n",
    "        kg[\"meta\"] = {}\n",
    "    kg[\"meta\"][\"allowed_relations\"] = _ALLOWED_RELATIONS_DEFAULT.copy()\n",
    "\n",
    "allowed_relations = _ALLOWED_RELATIONS_DEFAULT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0317a2-0e1b-4fe9-a884-80222c81036f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1758492511086,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9a0317a2-0e1b-4fe9-a884-80222c81036f",
    "outputId": "220427ff-18e9-4da6-ddfc-ffece399d6bc"
   },
   "outputs": [],
   "source": [
    "len(kg[\"nodes\"]), len(kg['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55575bf0-21c3-4853-8c78-1389a6388b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "kg_history[0] = deepcopy(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef79fd-b315-4f95-b9d1-cbb165edfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kg_history[0]['nodes']), len(kg_history[0]['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288091d6-a4a7-4480-a3bc-21ddeecb1c8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1758492512874,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "288091d6-a4a7-4480-a3bc-21ddeecb1c8e",
    "outputId": "9dec460d-c363-4089-b807-45fd5f98e6da"
   },
   "outputs": [],
   "source": [
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "\n",
    "# Measures\n",
    "mu = normalized_measure(len(elements))\n",
    "nu = degree_centrality_measure(kg, node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e34a1f-264b-4c51-b36f-15d647301d9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758492513915,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "11e34a1f-264b-4c51-b36f-15d647301d9e",
    "outputId": "bd7ca965-b67b-4335-843e-2db694589ba0"
   },
   "outputs": [],
   "source": [
    "# Initial coupling and distance\n",
    "P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbfe1f-2053-444c-a6d9-d2388ed881f6",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1758492515233,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "b7fbfe1f-2053-444c-a6d9-d2388ed881f6"
   },
   "outputs": [],
   "source": [
    "best_KG = json.loads(json.dumps(kg))\n",
    "best_P = P.copy()\n",
    "best_fgw = fgw_total\n",
    "best_L = compute_objective(kg, hp.beta, best_fgw)\n",
    "\n",
    "history = [{\n",
    "    \"iter\": 0,\n",
    "    \"rate\": rate_complexity(kg),\n",
    "    \"fgw_total\": best_fgw,\n",
    "    \"fgw_structure\": struct_term,\n",
    "    \"fgw_feature\": feat_term,\n",
    "    \"objective\": best_L,\n",
    "    \"ops\": {\"add\":0,\"split\":0,\"merge\":0,\"edge_add\":0,\"edge_remove\":0}\n",
    "}]\n",
    "\n",
    "ops_counters = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db80d4e-02ff-489f-afa4-6b2d8fdd555a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1758492518778,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5db80d4e-02ff-489f-afa4-6b2d8fdd555a",
    "outputId": "75db6b07-cc0f-4081-d5a5-bb98c4de40c1"
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701a1ea-9cef-4ddf-8975-d372956cc3a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241408,
     "status": "ok",
     "timestamp": 1758492761466,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5701a1ea-9cef-4ddf-8975-d372956cc3a9",
    "outputId": "bb094cf6-1a81-4944-8601-ad4813ed75e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterative refinement\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "for it in tqdm(range(1, hp.max_iterations + 1)):\n",
    "    improved = False\n",
    "    local_ops = collections.Counter()\n",
    "\n",
    "    # --- Operation A: Add new concepts for under-represented content\n",
    "    # mass over lecture elements\n",
    "    row_mass = P.sum(axis=1)  # size N (lecture)\n",
    "    add_candidates = np.where(row_mass < hp.theta_add)[0].tolist()\n",
    "    random.shuffle(add_candidates)\n",
    "    added_in_this_iter = 0\n",
    "    added_edge_in_this_iter = 0\n",
    "    for i in add_candidates[:5]:  # cap additions per iter\n",
    "        # connect to nearest existing concept by semantic similarity\n",
    "        v = E_L[i]\n",
    "        sims = E_K @ v\n",
    "        j = int(np.argmax(sims))\n",
    "        connect_to = node_ids[j]\n",
    "        added_node, added_edges = add_concept_from_element(\n",
    "            kg, elements[i], embedder, allowed_relations,\n",
    "            connect_to=connect_to,\n",
    "            relation=\"relatedTo\"\n",
    "        )\n",
    "\n",
    "        if added_node:\n",
    "            added_in_this_iter += 1\n",
    "        if added_edges:\n",
    "            added_edge_in_this_iter += len(added_edges)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    local_ops[\"add\"] += added_in_this_iter\n",
    "    local_ops[\"edge_add\"] += added_edge_in_this_iter\n",
    "\n",
    "    # Rebuild KG side after additions (embeddings, distances, measures)\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "    # --- Operation B: Split concepts with high coupling entropy\n",
    "    # Recompute coupling to assess splits\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    cols_entropy = [coupling_entropy(P[:, j]) for j in range(P.shape[1])]\n",
    "    split_targets = [j for j, H in enumerate(cols_entropy) if H > hp.theta_split]\n",
    "    random.shuffle(split_targets)\n",
    "    splits_done = 0\n",
    "    for j in split_targets[:2]:  # cap splits per iter\n",
    "        A, B, a_idx, b_idx = kmeans_split_concept(P[:, j], E_L, elements, kg, j)\n",
    "        if A is None:\n",
    "            continue\n",
    "        # Commit split: remove old node j and add A,B\n",
    "        old_id = kg[\"nodes\"][j][\"id\"]\n",
    "        # Add A, B\n",
    "        kg[\"nodes\"].append(dataclasses.asdict(A))\n",
    "        kg[\"nodes\"].append(dataclasses.asdict(B))\n",
    "        # Rewire edges connected to old_id: duplicate to A and B\n",
    "        new_edges = []\n",
    "        for e in kg[\"edges\"]:\n",
    "            if e[\"source\"] == old_id:\n",
    "                for nid in [A.id, B.id]:\n",
    "                    new_e = dict(e); new_e[\"source\"] = nid; new_edges.append(new_e)\n",
    "            elif e[\"target\"] == old_id:\n",
    "                for nid in [A.id, B.id]:\n",
    "                    new_e = dict(e); new_e[\"target\"] = nid; new_edges.append(new_e)\n",
    "            else:\n",
    "                new_edges.append(e)\n",
    "        kg[\"edges\"] = new_edges\n",
    "        # Remove old node\n",
    "        del kg[\"nodes\"][j]\n",
    "        splits_done += 1\n",
    "    local_ops[\"split\"] += splits_done\n",
    "\n",
    "    # --- Operation C: Merge redundant concept pairs\n",
    "    # Rebuild KG matrices after splits\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    to_merge = []\n",
    "    m = len(node_ids)\n",
    "    for i in range(m):\n",
    "        for j in range(i+1, m):\n",
    "            if merge_if_redundant(i, j, P, E_K, sim_thresh=0.92, kl_thresh=hp.theta_merge):\n",
    "                to_merge.append((i, j))\n",
    "    # Commit merges (greedy, no transitive handling beyond sequential)\n",
    "    merges_done = 0\n",
    "    merged_ids = set()\n",
    "    for (i, j) in to_merge[:2]:  # cap merges per iter\n",
    "        if i in merged_ids or j in merged_ids:\n",
    "            continue\n",
    "        id_i = kg[\"nodes\"][i][\"id\"]; id_j = kg[\"nodes\"][j][\"id\"]\n",
    "        # Merge j into i: keep i's label/def; absorb aliases\n",
    "        ni = kg[\"nodes\"][i]; nj = kg[\"nodes\"][j]\n",
    "        ali = set(ni.get(\"aliases\", []) + [nj.get(\"label\", \"\")] + nj.get(\"aliases\", []))\n",
    "        ni[\"aliases\"] = sorted([a for a in ali if a])\n",
    "        # Rewire edges from j to i\n",
    "        for e in kg[\"edges\"]:\n",
    "            if e[\"source\"] == id_j: e[\"source\"] = id_i\n",
    "            if e[\"target\"] == id_j: e[\"target\"] = id_i\n",
    "        # Remove node j\n",
    "        del kg[\"nodes\"][j]\n",
    "        merges_done += 1\n",
    "        merged_ids.add(i); merged_ids.add(j)\n",
    "    local_ops[\"merge\"] += merges_done\n",
    "\n",
    "    # --- Operation D: Relationship updates (add/rewire/remove)\n",
    "    # Add edges based on coupling + D_L proximity\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "\n",
    "    # Add relations where coupled lecture elements are near\n",
    "    edge_adds = 0\n",
    "    N, M = P.shape\n",
    "    # For each concept j, find top lecture elements and propose relations with \n",
    "    # other concepts k\n",
    "    for j in range(M):\n",
    "        top_i = np.argsort(-P[:, j])[:5]\n",
    "        for k in range(M):\n",
    "            if j == k:\n",
    "                continue\n",
    "            # If top elements for j are close to top elements for k in lecture space, \n",
    "            # relate\n",
    "            top_k = np.argsort(-P[:, k])[:5]\n",
    "            # Compute average cross-distance\n",
    "            pairs = list(itertools.product(top_i, top_k))\n",
    "            if not pairs:\n",
    "                continue\n",
    "            avg_d = float(np.mean([D_L[i1, i2] for i1, i2 in pairs]))\n",
    "            if avg_d < hp.theta_relate:\n",
    "                a = kg[\"nodes\"][j][\"id\"]; b = kg[\"nodes\"][k][\"id\"]\n",
    "                sig = (a, b, \"relatedTo\")\n",
    "                if not any((e[\"source\"], e[\"target\"], e[\"relation\"]) == sig for e in kg[\"edges\"]):\n",
    "                    # get edge definition\n",
    "                    e_def = get_edge_definition(kg, a, b, \"relatedTo\")\n",
    "\n",
    "                    e_rationale = get_edge_rationale(kg, a, b, \"relatedTo\")\n",
    "                    \n",
    "                    kg[\"edges\"].append({\n",
    "                        \"id\": f\"rel_{a}_{b}\",\n",
    "                        \"source\": a,\n",
    "                        \"target\": b,\n",
    "                        \"relation\": \"relatedTo\",\n",
    "                        \"definition\": e_def,\n",
    "                        \"provenance\": [],\n",
    "                        \"confidence\": 0.6,\n",
    "                        \"rationale\": e_rationale\n",
    "                    })\n",
    "                    edge_adds += 1\n",
    "    local_ops[\"edge_add\"] += edge_adds\n",
    "\n",
    "    # Remove weak edges not supported by coupling\n",
    "    edge_removes = 0\n",
    "    keep_edges = []\n",
    "    for e in kg[\"edges\"]:\n",
    "        try:\n",
    "            a = node_ids.index(e[\"source\"])\n",
    "            b = node_ids.index(e[\"target\"])\n",
    "        except ValueError:\n",
    "            # Node removed; drop edge\n",
    "            edge_removes += 1\n",
    "            continue\n",
    "        # Support measure ~ product of marginals around a,b\n",
    "        supp = float(P[:, a].sum() * P[:, b].sum())\n",
    "        if supp < 1e-4:\n",
    "            edge_removes += 1\n",
    "            continue\n",
    "        keep_edges.append(e)\n",
    "    kg[\"edges\"] = keep_edges\n",
    "    local_ops[\"edge_remove\"] += edge_removes\n",
    "\n",
    "    # Domain rules (optional)\n",
    "    #if not disable_domain_rules:\n",
    "    #    added = apply_domain_rules(kg, allowed_relations)\n",
    "    #    local_ops[\"edge_add\"] += len(added)\n",
    "\n",
    "    # Evaluate objective\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "    P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "    r = rate_complexity(kg)\n",
    "    L = r + hp.beta * fgw_total\n",
    "\n",
    "    history.append({\n",
    "        \"iter\": it,\n",
    "        \"rate\": r,\n",
    "        \"fgw_total\": fgw_total,\n",
    "        \"fgw_structure\": struct_term,\n",
    "        \"fgw_feature\": feat_term,\n",
    "        \"objective\": L,\n",
    "        \"ops\": dict(local_ops)\n",
    "    })\n",
    "\n",
    "    kg_history[it] = deepcopy(kg)\n",
    "    print(\"The KG sizes at iteration {}: nodes:{}, edges:{}\".format(it, \n",
    "                                                                    len(kg_history[it]['nodes']),\n",
    "                                                                    len(kg_history[it]['edges'])))\n",
    "\n",
    "    if L + 1e-9 < best_L:\n",
    "        print(\"Reassign best values.\")\n",
    "        best_L = L\n",
    "        best_KG = json.loads(json.dumps(kg))\n",
    "        best_P = P.copy()\n",
    "        best_fgw = fgw_total\n",
    "        improved = True\n",
    "        for k, v in local_ops.items():\n",
    "            ops_counters[k] += v\n",
    "    else:\n",
    "        print(\"best values remain.\")\n",
    "        for k, v in local_ops.items():\n",
    "            ops_counters[k] += v\n",
    "\n",
    "    if not improved:\n",
    "        # If no improvement, consider stopping early\n",
    "        if abs(history[-1][\"objective\"] - history[-2][\"objective\"]) < hp.convergence_threshold:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009af3b-2b95-4b5e-a6ce-e856e3ea4854",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758492765344,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "5009af3b-2b95-4b5e-a6ce-e856e3ea4854",
    "outputId": "260c49d7-4f9a-460f-e8c5-9faa2bc76bdc"
   },
   "outputs": [],
   "source": [
    "ops_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53099753-390d-4f8c-afcc-8b4603626e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kg['nodes']), len(kg['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf130615-76b3-4fea-b26c-105eedefc047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++\n",
    "# Domain knowledge graph refinement\n",
    "#\n",
    "#+++++++++++++++++\n",
    "added_edges_domain_refinement = apply_domain_knowledge_LLM(kg, allowed_relations)\n",
    "\n",
    "added_node_counter, added_edge_counter = add_refined_domain_edges(kg, added_edges_domain_refinement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90716d5e-179e-4dc2-a7ee-0375751d4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_node_counter, added_edge_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b96d-009e-4376-849d-d1a1022cbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++\n",
    "# Domain knowledge graph refinement\n",
    "# Update history\n",
    "#+++++++++++++++++\n",
    "\n",
    "it = it + 1\n",
    "\n",
    "local_ops = collections.Counter()\n",
    "local_ops['add'] += added_node_counter\n",
    "local_ops['edge_add'] += added_edge_counter\n",
    "\n",
    "# Evaluate objective\n",
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "P, fgw_total, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")\n",
    "\n",
    "r = rate_complexity(kg)\n",
    "L = r + hp.beta * fgw_total\n",
    "\n",
    "history.append({\n",
    "    \"iter\": it,\n",
    "    \"rate\": r,\n",
    "    \"fgw_total\": fgw_total,\n",
    "    \"fgw_structure\": struct_term,\n",
    "    \"fgw_feature\": feat_term,\n",
    "    \"objective\": L,\n",
    "    \"ops\": dict(local_ops)\n",
    "})\n",
    "\n",
    "for k, v in local_ops.items():\n",
    "    ops_counters[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86b406-6cee-404f-b8f1-c18ca0c65fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5f0d1-c464-415e-b8a7-3926f9fadac1",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758492771936,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "6cb5f0d1-c464-415e-b8a7-3926f9fadac1"
   },
   "outputs": [],
   "source": [
    "# Build outcome\n",
    "outcome = RefinementOutcome(\n",
    "    iterations=len(history)-1,\n",
    "    final_objective=float(best_L),\n",
    "    rate=float(rate_complexity(best_KG)),\n",
    "    distortion=float(best_fgw),   # report main FGW term as 'distortion'\n",
    "    fgw_distance=float(best_fgw),\n",
    "    history=history,\n",
    "    operations={\n",
    "        \"concepts_added\": int(ops_counters.get(\"add\", 0)),\n",
    "        \"merged\": int(ops_counters.get(\"merge\", 0)),\n",
    "        \"concepts_split\": int(ops_counters.get(\"split\", 0)),\n",
    "        \"relationships_added\": int(ops_counters.get(\"edge_add\", 0)),\n",
    "        \"relationships_removed\": int(ops_counters.get(\"edge_remove\", 0)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5399316-c8cb-41e3-ad62-409c96ec9a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1758492774376,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "c5399316-c8cb-41e3-ad62-409c96ec9a23",
    "outputId": "7cb5226c-139d-4ae3-bbc9-1a5c1ded1441"
   },
   "outputs": [],
   "source": [
    "outcome.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57903a4e-06ec-4389-ae12-45e907c7a111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1758492776808,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "57903a4e-06ec-4389-ae12-45e907c7a111",
    "outputId": "97c9a7fc-bbc8-4a6c-9c50-f45edc044912"
   },
   "outputs": [],
   "source": [
    "# Coupling analysis (top-k)\n",
    "# For interpretability, recompute matrices for best_KG\n",
    "D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(best_KG, embedder, hp.gamma_struct, hp.gamma_sem)\n",
    "nu = degree_centrality_measure(best_KG, node_ids)\n",
    "P, best_fgw, struct_term, feat_term = compute_coupling_and_distance(\n",
    "    D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    ")\n",
    "coupling_report = []\n",
    "\n",
    "topk_align = 12\n",
    "\n",
    "N, M = P.shape\n",
    "for i in range(N):\n",
    "    topj = np.argsort(-P[i, :])[:min(topk_align, M)]\n",
    "    row = []\n",
    "    for j in topj:\n",
    "        row.append({\n",
    "            \"lecture_idx\": int(i),\n",
    "            \"lecture_text\": elements[i].text[:240],\n",
    "            \"concept_id\": node_ids[j],\n",
    "            \"concept_label\": next((n[\"label\"] for n in best_KG[\"nodes\"] if n[\"id\"] == node_ids[j]), node_ids[j]),\n",
    "            \"weight\": float(P[i, j])\n",
    "        })\n",
    "    coupling_report.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a45df6-e769-428e-a61f-b04abfc75f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "file_name = \"objective_rate_distortion_curve.png\"\n",
    "rd_curve_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{file_name}\"))\n",
    "rd_curve_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974bd86-691c-44fd-96ee-2231d2e018ca",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1758492778791,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "9974bd86-691c-44fd-96ee-2231d2e018ca"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    # One chart, no styles/colors\n",
    "    xs = [h[\"iter\"] for h in history]\n",
    "    rates = [h[\"rate\"] for h in history]\n",
    "    dists = [h[\"fgw_total\"] for h in history]\n",
    "    objs = [h[\"objective\"] for h in history]\n",
    "    plt.figure()\n",
    "    plt.plot(xs, objs, label=\"Objective L\")\n",
    "    plt.plot(xs, rates, label=\"Rate r\")\n",
    "    plt.plot(xs, dists, label=\"Distortion d (FGW)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(rd_curve_path, dpi=160)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    logging.warning(\"Could not generate RD curve plot: %s\", e)\n",
    "    rd_curve_path = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bebd2b-ec3c-4e35-912d-fbb8d8246d09",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492781063,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "24bebd2b-ec3c-4e35-912d-fbb8d8246d09"
   },
   "outputs": [],
   "source": [
    "# Package refined KG meta\n",
    "if \"meta\" not in best_KG:\n",
    "    best_KG[\"meta\"] = {}\n",
    "best_KG[\"meta\"].setdefault(\"refinement\", {})\n",
    "best_KG[\"meta\"][\"refinement\"].update({\n",
    "    \"iterations\": outcome.iterations,\n",
    "    \"final_objective\": outcome.final_objective,\n",
    "    \"rate\": outcome.rate,\n",
    "    \"distortion\": outcome.distortion,\n",
    "    \"fgw_distance\": outcome.fgw_distance,\n",
    "    \"used_pot_fgw\": bool(use_pot and _HAS_POT),\n",
    "    \"alpha\": {\n",
    "        \"chronological\": hp.alpha_chron,\n",
    "        \"logical\": hp.alpha_logic,\n",
    "        \"semantic\": hp.alpha_sem\n",
    "    },\n",
    "    \"gamma\": {\n",
    "        \"structural\": hp.gamma_struct,\n",
    "        \"semantic\": hp.gamma_sem\n",
    "    },\n",
    "    \"lambda_feature_balance\": hp.lambda_feat,\n",
    "    \"beta_rate_distortion\": hp.beta,\n",
    "    \"thresholds\": {\n",
    "        \"theta_add\": hp.theta_add,\n",
    "        \"theta_split\": hp.theta_split,\n",
    "        \"theta_merge\": hp.theta_merge,\n",
    "        \"theta_relate\": hp.theta_relate\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db84d6b-ae2d-4425-b72c-b336fc1e37d5",
   "metadata": {
    "id": "1db84d6b-ae2d-4425-b72c-b336fc1e37d5"
   },
   "source": [
    "# Plot\n",
    "- Computes rate R = nodes + 0.5 * edges.\n",
    "- Computes objective L = R + beta * D for a user-chosen beta (default 20).\n",
    "- Finds the geometric knee (max perpendicular distance to the line between the\n",
    "  first and last points in (D, R) space).\n",
    "- Writes a CSV of the iteration history and saves a PNG plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbefbd45-42eb-4e03-9062-9ea7a6d12465",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1758492917145,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "cbefbd45-42eb-4e03-9062-9ea7a6d12465"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RDHistory:\n",
    "    iters: np.ndarray       # iteration indices\n",
    "    D: np.ndarray           # distortion per iteration\n",
    "    R: np.ndarray\n",
    "\n",
    "    #nodes: np.ndarray       # node count per iteration\n",
    "    #edges: np.ndarray       # edge count per iteration\n",
    "\n",
    "\n",
    "def objective_L(R: np.ndarray, D: np.ndarray, beta: float) -> np.ndarray:\n",
    "    return R + beta * D\n",
    "\n",
    "\n",
    "def knee_index_max_distance(D: np.ndarray, R: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Knee = point of maximum perpendicular distance to the chord from the\n",
    "    first to last point in (D, R) space.\n",
    "    \"\"\"\n",
    "    A = np.array([D[0], R[0]], dtype=float)\n",
    "    B = np.array([D[-1], R[-1]], dtype=float)\n",
    "    AB = B - A\n",
    "    ab_norm = np.linalg.norm(AB)\n",
    "    if ab_norm == 0:\n",
    "        return 0\n",
    "\n",
    "    def dist_to_line(p: np.ndarray) -> float:\n",
    "        # In 2D, || (B - A) x (P - A) || / ||B - A||\n",
    "        return np.abs(np.cross(AB, p - A)) / ab_norm\n",
    "\n",
    "    distances = np.array([dist_to_line(np.array([D[i], R[i]], dtype=float)) for i in range(len(D))])\n",
    "    return int(np.argmax(distances))\n",
    "\n",
    "\n",
    "def beta_threshold_vs_seed(R: np.ndarray, D: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each iteration i, solve R_i + beta*D_i = R_0 + beta*D_0 for beta.\n",
    "    beta_i = (R0 - Ri) / (Di - D0) = (Ri - R0) / (D0 - Di)\n",
    "    \"\"\"\n",
    "    R0, D0 = R[0], D[0]\n",
    "    betas = []\n",
    "    for i in range(len(D)):\n",
    "        if i == 0:\n",
    "            betas.append(np.nan)\n",
    "            continue\n",
    "        denom = (D0 - D[i])\n",
    "        betas.append((R[i] - R0) / denom if denom != 0 else np.inf)\n",
    "    return np.array(betas, dtype=float)\n",
    "\n",
    "\n",
    "def build_dataframe(hist: RDHistory, beta: float) -> pd.DataFrame:\n",
    "    R = hist.R\n",
    "    L = objective_L(R, hist.D, beta)\n",
    "    beta_tie = beta_threshold_vs_seed(R, hist.D)\n",
    "    df = pd.DataFrame({\n",
    "        \"iter\": hist.iters,\n",
    "        \"distortion_D\": hist.D,\n",
    "        \"rate_R\": R,\n",
    "        #\"nodes\": hist.nodes,\n",
    "        #\"edges\": hist.edges,\n",
    "        f\"objective_L(beta={beta:g})\": L,\n",
    "        \"beta_threshold_vs_seed\": beta_tie\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_rd_curve(\n",
    "    hist: RDHistory,\n",
    "    beta: float,\n",
    "    savepath: str,\n",
    "    show_iso_L: bool = False,\n",
    "    iso_L_count: int = 4\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Plot R vs D, annotate iterations, knee, and min-L point (for given beta).\n",
    "    Optionally overlay iso-L lines.\n",
    "    \"\"\"\n",
    "    D, R = hist.D, hist.R\n",
    "    L = objective_L(R, D, beta)\n",
    "    knee_idx = knee_index_max_distance(D, R)\n",
    "    lopt_idx = int(np.argmin(L))\n",
    "\n",
    "    plt.figure(figsize=(6.0, 5.0))\n",
    "    # R–D curve\n",
    "    plt.plot(D, R, marker='o')  # default styling\n",
    "\n",
    "    # annotate points t0..tn\n",
    "    for i in range(len(D)):\n",
    "        plt.annotate(f\"t{i}\", (D[i], R[i]), xytext=(5, 5),\n",
    "                     textcoords='offset points', fontsize=8)\n",
    "\n",
    "    # highlight knee and L-optimum\n",
    "    plt.scatter([D[knee_idx]], [R[knee_idx]], marker='s', s=90, label=f\"Knee ~ t{knee_idx}\")\n",
    "    plt.scatter([D[lopt_idx]], [R[lopt_idx]], marker='*', s=160, label=f\"Min L (β={beta:g}) ~ t{lopt_idx}\")\n",
    "\n",
    "    # optional: iso-L lines through a few L values\n",
    "    if show_iso_L:\n",
    "        # pick a span of L values centered around median\n",
    "        Lmin, Lmax = float(np.min(L)), float(np.max(L))\n",
    "        Lvals = np.linspace(Lmin, Lmax, num=iso_L_count)\n",
    "        x_span = np.linspace(float(np.min(D)) - 0.02, float(np.max(D)) + 0.02, 100)\n",
    "        for Lv in Lvals:\n",
    "            # R = L - beta * D\n",
    "            y_line = Lv - beta * x_span\n",
    "            plt.plot(x_span, y_line, linestyle='--', linewidth=0.8)\n",
    "\n",
    "    plt.xlabel(\"Distortion D\")\n",
    "    plt.ylabel(\"Rate R\")\n",
    "    plt.title(\"Rate–Distortion Curve (D on x, R on y)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath, dpi=150)\n",
    "    plt.close()\n",
    "    return knee_idx, lopt_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437e891-ede0-458d-8b2f-fb5f993cd252",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758492922180,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "4437e891-ede0-458d-8b2f-fb5f993cd252"
   },
   "outputs": [],
   "source": [
    "plot_file_name = \"rate_distortion_plot_curve.png\" #help=\"Base filename (without extension) for outputs\")\n",
    "iso = True # help=\"Overlay iso-L lines on the plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cb073-8724-4e1e-9ab7-e12fa3f47dc3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758492924108,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "dd0cb073-8724-4e1e-9ab7-e12fa3f47dc3"
   },
   "outputs": [],
   "source": [
    "iters = []\n",
    "D = []\n",
    "R = []\n",
    "for h in history:\n",
    "    iters.append(h['iter'])\n",
    "    R.append(h['rate'])\n",
    "    D.append(h['fgw_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc122fe-e91d-4007-a27a-3145b2982027",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758492926166,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "0cc122fe-e91d-4007-a27a-3145b2982027"
   },
   "outputs": [],
   "source": [
    "hist = RDHistory(np.array(iters), np.array(D), np.array(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e31110-04ee-46a4-8196-7924ec899f4d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758492926889,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "37e31110-04ee-46a4-8196-7924ec899f4d"
   },
   "outputs": [],
   "source": [
    "df = build_dataframe(hist, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ee278-0ffb-4ae0-b45e-fa021440c925",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1758492927853,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "d73ee278-0ffb-4ae0-b45e-fa021440c925",
    "outputId": "26888e3b-d7ca-404a-8e88-3588f6ee4899"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47452a56-f4e1-40ab-916e-49ff1f5586f2",
   "metadata": {},
   "source": [
    "### Save Rate_Distortion History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d1d0e-cd02-429b-ac1e-9ddb1407a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_name = \"rate_distortion_history.csv\"\n",
    "history_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{history_name}\"))\n",
    "history_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b9171-9753-4af9-80fb-d676aa82ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2079772-0914-4cc2-8ea3-0c2182993597",
   "metadata": {},
   "source": [
    "### Save KG History and KG_Knee to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fde175-89d4-4fd6-98d7-2407adbd7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{plot_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a3b97-f565-4e6a-bf35-90981544d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60666231-73e1-4a00-a513-ac7583c9bae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1758492931676,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "60666231-73e1-4a00-a513-ac7583c9bae4",
    "outputId": "eb5483c0-75a8-47a4-95bb-ac8dfe3dca60"
   },
   "outputs": [],
   "source": [
    "knee_idx, lopt_idx = plot_rd_curve(hist, beta, png_path, show_iso_L=iso, iso_L_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934dc10-399c-4138-84bc-abb85459360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_idx, lopt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3c48f-7793-441a-a56b-f08828787af8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1758492934812,
     "user": {
      "displayName": "Yuan An",
      "userId": "12279868416530647161"
     },
     "user_tz": 240
    },
    "id": "2dc3c48f-7793-441a-a56b-f08828787af8",
    "outputId": "b5b72f33-79ed-498c-a170-5a918964df1a"
   },
   "outputs": [],
   "source": [
    "# Console summary\n",
    "R, D = hist.R, hist.D\n",
    "L = objective_L(R, D, beta)\n",
    "print(\"\\n=== Rate–Distortion Summary ===\")\n",
    "print(f\"beta (β): {beta:g}\")\n",
    "print(f\"Knee index (max distance): t{knee_idx}  [D={D[knee_idx]:.3f}, R={R[knee_idx]:.3f}]\")\n",
    "print(f\"Min L index:               t{lopt_idx}  [D={D[lopt_idx]:.3f}, R={R[lopt_idx]:.3f}, L={L[lopt_idx]:.3f}]\")\n",
    "\n",
    "# Optional quick thresholds report\n",
    "beta_tie = beta_threshold_vs_seed(R, D)\n",
    "print(\"\\nβ thresholds to tie seed (t0) in L (higher → favors higher-fidelity iterations):\")\n",
    "for i, b in enumerate(beta_tie):\n",
    "    if i == 0:\n",
    "        print(f\"  t{i}: —\")\n",
    "    else:\n",
    "        if math.isfinite(b):\n",
    "            print(f\"  t{i}: β ≈ {b:.2f}\")\n",
    "        else:\n",
    "            print(f\"  t{i}: β = ∞ (no finite tie)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4902c4e-892b-43ff-a6ce-508ac55adbae",
   "metadata": {
    "id": "f4902c4e-892b-43ff-a6ce-508ac55adbae"
   },
   "outputs": [],
   "source": [
    "# The KG at knee point\n",
    "kg_knee = kg_history[knee_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5b556-326a-4865-9650-6de4aaa7e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kg_knee['nodes']), len(kg_knee['edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3296831-0b35-4841-8461-88ae93e76d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_file_name = \"kg_history.json\"\n",
    "kg_history_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{history_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180658e-8d7e-4007-97e5-7cf1dd10cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_history_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49badfc-6e07-4169-aca6-628a1ace69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(kg_history_path, \"w\") as f:\n",
    "    json.dump(kg_history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456550a1-45eb-43f3-ab6a-978dc6e63c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_file_name = \"kg_knee.json\"\n",
    "kg_knee_path = os.path.join(out_dir, os.path.basename(lecture_json_path).replace(\".json\", f\"_{knee_file_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a4a89-e1d6-42c1-9b41-0be1d5ab55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_knee_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c0c26-e183-4ffd-94d5-0774ab57abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(kg_knee_path, \"w\") as f:\n",
    "    json.dump(kg_knee, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b610e50-a67e-4e79-af51-f32d2bfa5a54",
   "metadata": {},
   "source": [
    "## Checking Coverage of KG vs. Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898303dc-d09f-4a61-9b11-3e3988120f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given lecture elements and KG, evaluate the KG's coverage\n",
    "# within tolerated feature distortion\n",
    "def kg_coverage_with_feature_distortion(elements, kg, embedder, hp, us_pot, thresh):\n",
    "    # build lecture metric-measure space \n",
    "    lecture_texts = [e.text for e in elements]\n",
    "    E_L = embedder.fit_transform(lecture_texts)\n",
    "    D_L, D_chron, D_logic, D_semL = build_lecture_distance(\n",
    "        elements, E_L, hp.alpha_chron, hp.alpha_logic, hp.alpha_sem\n",
    "    )\n",
    "\n",
    "    # build KG metri-measure space\n",
    "    D_K, D_struct, D_semK, E_K, node_ids = build_kg_distance(kg, \n",
    "                                                             embedder, \n",
    "                                                             hp.gamma_struct, hp.gamma_sem)\n",
    "\n",
    "    # build lecture elements and kg nodes distributions\n",
    "    mu = normalized_measure(len(elements))\n",
    "    nu = degree_centrality_measure(kg, node_ids)\n",
    "\n",
    "    # compute coupling\n",
    "    P, fgw_total_tmp, struct_term, feat_term = compute_coupling_and_distance(\n",
    "        D_L, D_K, E_L, E_K, mu, nu, hp.lambda_feat, hp.sinkhorn_eps, hp.sinkhorn_iter, use_pot\n",
    "    )\n",
    "\n",
    "    # compute feature distortion\n",
    "    M_feat = cdist(E_L, E_K, metric=\"sqeuclidean\")\n",
    "\n",
    "    # compute feature distortion tolerance\n",
    "    tau = 0\n",
    "    feat_ravel = np.asarray(M_feat, dtype=float).ravel()\n",
    "    feat_ravel = feat_ravel[np.isfinite(feat_ravel)]\n",
    "    if feat_ravel.size != 0:\n",
    "        tau = float(np.quantile(feat_ravel, thresh))\n",
    "\n",
    "    # compute coverage\n",
    "    good = (M_feat <= tau).astype(float)\n",
    "\n",
    "    per_pair = P * good\n",
    "    return per_pair.sum()  # already in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd4c36-d906-44fd-af50-4c0a6a947846",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_json_path = \"./data/lecture_notes_8.json\"\n",
    "kg_json_path = \"./data/kg8.json\"\n",
    "kg_knee_path = \"./data/output/lecture_notes_8_kg_knee.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358eec85-0b09-4445-a3d5-cc4bc9dededd",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = parse_lecture_elements(lecture_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f9a95-ef24-4639-b4c5-0c0162b7fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = read_kg(kg_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c7963-ca40-4bf1-b3c9-7e73a3b599ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_cov = kg_coverage_with_feature_distortion(elements, kg, embedder, hp, True, 0.3)\n",
    "kg_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fded01e-c934-4a18-af8b-54eb9a6fc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_knee_file = read_json(kg_knee_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff06ab-74d7-48aa-9a34-35d006389652",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_knee_cov = kg_coverage_with_feature_distortion(elements, kg_knee_file, embedder, hp, True, 0.3)\n",
    "kg_knee_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51921395-d42e-4be2-83f9-5d2df6853e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_list = []\n",
    "kg_cov_list  = []\n",
    "kg_knee_cov_list = []\n",
    "tau_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718e5f0-40a1-4f2f-8d76-ef8a40403af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_list.append(\"lecture_notes_8\")\n",
    "kg_cov_list.append(kg_cov)\n",
    "kg_knee_cov_list.append(kg_knee_cov)\n",
    "tau_list.append(\"30-th quantile of feature distortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7642ac8-0d80-43b6-8978-9a2429fd83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_pd = pd.DataFrame({\"lecture\":lecture_list, \"initial_kg_coverage\":kg_cov_list,\n",
    "              \"knee_kg_coverage\":kg_knee_cov_list, \"feature_distortion_tolerance\":tau_list})\n",
    "coverage_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b994564-0f9d-40b6-a410-8e33bd173c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_pd.to_csv(\"./data/output/kg_coverages.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "drtutor",
   "language": "python",
   "name": "drtutor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
